<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <title>Deep Learning with Python</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link href="../Styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../Styles/page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
  <h1 class="part" id="ch03">Chapter 3. <a class="calibre3" id="ch03__title"></a>Getting started with neural networks</h1>

  <p class="noind1">This chapter covers</p>

  <ul class="calibre16">
    <li class="calibre17">Core components of neural networks</li>

    <li class="calibre17">An introduction to Keras</li>

    <li class="calibre17">Setting up a deep-learning workstation</li>

    <li class="calibre17">Using neural networks to solve basic classification and regression problems</li>
  </ul>

  <p class="noind">This chapter is designed to get you started with using neural networks to solve real problems. You’ll consolidate the knowledge you gained from our first practical example in <a href="../Text/02.html#ch02">chapter 2</a>, and you’ll apply what you’ve learned to three new problems covering the three most common use cases of neural networks: binary classification, multiclass classification, and scalar regression.</p>

  <p class="noind">In this chapter, we’ll take a closer look at the core components of neural networks that we introduced in <a href="../Text/02.html#ch02">chapter 2</a>: layers, networks, objective functions, and optimizers. We’ll give you a quick introduction to Keras, the Python deep-learning library that we’ll use throughout the book. You’ll set up a deep-learning workstation, with TensorFlow, Keras, and GPU support. We’ll dive into three introductory examples of how to use neural networks to address real problems:</p>

  <ul class="calibre16">
    <li class="calibre17">Classifying movie reviews as positive or negative (binary classification)</li>

    <li class="calibre17">Classifying news wires by topic (multiclass classification)</li>

    <li class="calibre17">Estimating the price of a house, given real-estate data (regression)</li>
  </ul>

  <p class="noind">By the end of this chapter, you’ll be able to use neural networks to solve simple machine problems such as classification and regression over vector data. You’ll then be ready to start building a more principled, theory-driven understanding of machine learning in <a href="../Text/04.html#ch04">chapter 4</a>.</p>

  <h2 class="head" id="ch03lev1sec1"><a class="calibre3" id="ch03lev1sec1__title"></a>3.1. Anatomy of a neural network</h2>

  <p class="noind"><a id="iddle1303"></a><a id="iddle1427"></a><a id="iddle1507"></a><a id="iddle1529"></a><a id="iddle1535"></a><a id="iddle1665"></a><a id="iddle1666"></a><a id="iddle1751"></a><a id="iddle2068"></a>As you saw in the previous chapters, training a neural network revolves around the following objects:</p>

  <ul class="calibre16">
    <li class="calibre17"><i class="calibre5">Layers</i>, which are combined into a <i class="calibre5">network</i> (or <i class="calibre5">model</i>)</li>

    <li class="calibre17">The <i class="calibre5">input data</i> and corresponding <i class="calibre5">targets</i></li>

    <li class="calibre17">The <i class="calibre5">loss function</i>, which defines the feedback signal used for learning</li>

    <li class="calibre17">The <i class="calibre5">optimizer</i>, which determines how learning proceeds</li>
  </ul>

  <p class="noind">You can visualize their interaction as illustrated in <a href="#ch03fig01">figure 3.1</a>: the network, composed of layers that are chained together, maps the input data to predictions. The loss function then compares these predictions to the targets, producing a loss value: a measure of how well the network’s predictions match what was expected. The optimizer uses this loss value to update the network’s weights.</p>

  <p class="notetitle" id="ch03fig01">Figure 3.1. <a id="ch03fig01__title"></a>Relationship between the network, layers, loss function, and optimizer</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/03fig01.jpg"/></p>

  <p class="noind">Let’s take a closer look at layers, networks, loss functions, and optimizers.</p>

  <h3 class="head1" id="ch03lev2sec1">3.1.1. <a id="ch03lev2sec1__title"></a>Layers: the building blocks of deep learning</h3>

  <p class="noind">The fundamental data structure in neural networks is the <i class="calibre5">layer</i>, to which you were introduced in <a href="../Text/02.html#ch02">chapter 2</a>. A layer is a data-processing module that takes as input one or more tensors and that outputs one or more tensors. Some layers are stateless, but more frequently layers have a state: the layer’s <i class="calibre5">weights</i>, one or several tensors learned with stochastic gradient descent, which together contain the network’s <i class="calibre5">knowledge</i>.</p>

  <p class="noind">Different layers are appropriate for different tensor formats and different types of data processing. For instance, simple vector data, stored in 2D tensors of shape <kbd class="calibre24">(samples,</kbd> <kbd class="calibre24">features)</kbd>, is often processed by <i class="calibre5">densely connected</i> layers, also called <i class="calibre5">fully connected</i> or <i class="calibre5">dense</i> layers (the <kbd class="calibre24">Dense</kbd> class in Keras). Sequence data, stored in 3D tensors of shape <kbd class="calibre24">(samples, timesteps, features)</kbd>, is typically processed by <i class="calibre5">recurrent</i> layers such as an <kbd class="calibre24">LSTM</kbd> layer. Image data, stored in 4D tensors, is usually processed by 2D convolution layers (<kbd class="calibre24">Conv2D</kbd>).</p>

  <p class="noind"><a id="iddle1394"></a><a id="iddle1416"></a><a id="iddle1498"></a><a id="iddle1651"></a><a id="iddle1668"></a><a id="iddle2015"></a>You can think of layers as the LEGO bricks of deep learning, a metaphor that is made explicit by frameworks like Keras. Building deep-learning models in Keras is done by clipping together compatible layers to form useful data-transformation pipelines. The notion of <i class="calibre5">layer compatibility</i> here refers specifically to the fact that every layer will only accept input tensors of a certain shape and will return output tensors of a certain shape. Consider the following example:</p>
  <pre class="calibre4" id="PLd0e6515">from keras import layers

layer = layers.Dense(32, input_shape=(784,))    <span class="cambriamathin">❶</span></pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> A dense layer with 32 output units</p>
  </div>

  <p class="noind">We’re creating a layer that will only accept as input 2D tensors where the first dimension is 784 (axis 0, the batch dimension, is unspecified, and thus any value would be accepted). This layer will return a tensor where the first dimension has been transformed to be 32.</p>

  <p class="noind">Thus this layer can only be connected to a downstream layer that expects 32-dimensional vectors as its input. When using Keras, you don’t have to worry about compatibility, because the layers you add to your models are dynamically built to match the shape of the incoming layer. For instance, suppose you write the following:</p>
  <pre class="calibre4" id="PLd0e6541">from keras import models
from keras import layers

model = models.Sequential()
model.add(layers.Dense(32, input_shape=(784,)))
model.add(layers.Dense(32))</pre>

  <p class="noind">The second layer didn’t receive an input shape argument—instead, it automatically inferred its input shape as being the output shape of the layer that came before.</p>

  <h3 class="head1" id="ch03lev2sec2">3.1.2. <a id="ch03lev2sec2__title"></a>Models: networks of layers</h3>

  <p class="noind">A deep-learning model is a directed, acyclic graph of layers. The most common instance is a linear stack of layers, mapping a single input to a single output.</p>

  <p class="noind">But as you move forward, you’ll be exposed to a much broader variety of network topologies. Some common ones include the following:</p>

  <ul class="calibre16">
    <li class="calibre17">Two-branch networks</li>

    <li class="calibre17">Multihead networks</li>

    <li class="calibre17">Inception blocks</li>
  </ul>

  <p class="noind">The topology of a network defines a <i class="calibre5">hypothesis space</i>. You may remember that in <a href="../Text/01.html#ch01">chapter 1</a>, we defined machine learning as “searching for useful representations of some input data, within a predefined space of possibilities, using guidance from a feedback signal.” By choosing a network topology, you constrain your <i class="calibre5">space of possibilities</i> (hypothesis space) to a specific series of tensor operations, mapping input data to <a id="iddle1071"></a><a id="iddle1095"></a><a id="iddle1530"></a><a id="iddle1667"></a><a id="iddle1669"></a><a id="iddle1735"></a><a id="iddle1753"></a><a id="iddle1834"></a><a id="iddle1881"></a><a id="iddle1892"></a>output data. What you’ll then be searching for is a good set of values for the weight tensors involved in these tensor operations.</p>

  <p class="noind">Picking the right network architecture is more an art than a science; and although there are some best practices and principles you can rely on, only practice can help you become a proper neural-network architect. The next few chapters will both teach you explicit principles for building neural networks and help you develop intuition as to what works or doesn’t work for specific problems.</p>

  <h3 class="head1" id="ch03lev2sec3">3.1.3. <a id="ch03lev2sec3__title"></a>Loss functions and optimizers: keys to configuring the learning process</h3>

  <p class="noind">Once the network architecture is defined, you still have to choose two more things:</p>

  <ul class="calibre16">
    <li class="calibre17"><b class="calibre22">Loss function (objective function)—</b> The quantity that will be minimized during training. It represents a measure of success for the task at hand.</li>

    <li class="calibre17"><b class="calibre22">Optimizer—</b> Determines how the network will be updated based on the loss function. It implements a specific variant of stochastic gradient descent (SGD).</li>
  </ul>

  <p class="noind">A neural network that has multiple outputs may have multiple loss functions (one per output). But the gradient-descent process must be based on a <i class="calibre5">single</i> scalar loss value; so, for multiloss networks, all losses are combined (via averaging) into a single scalar quantity.</p>

  <p class="noind">Choosing the right objective function for the right problem is extremely important: your network will take any shortcut it can, to minimize the loss; so if the objective doesn’t fully correlate with success for the task at hand, your network will end up doing things you may not have wanted. Imagine a stupid, omnipotent AI trained via SGD, with this poorly chosen objective function: “maximizing the average well-being of all humans alive.” To make its job easier, this AI might choose to kill all humans except a few and focus on the well-being of the remaining ones—because average well-being isn’t affected by how many humans are left. That might not be what you intended! Just remember that all neural networks you build will be just as ruthless in lowering their loss function—so choose the objective wisely, or you’ll have to face unintended side effects.</p>

  <p class="noind">Fortunately, when it comes to common problems such as classification, regression, and sequence prediction, there are simple guidelines you can follow to choose the correct loss. For instance, you’ll use binary crossentropy for a two-class classification problem, categorical crossentropy for a many-class classification problem, mean-squared error for a regression problem, connectionist temporal classification (CTC) for a sequence-learning problem, and so on. Only when you’re working on truly new research problems will you have to develop your own objective functions. In the next few chapters, we’ll detail explicitly which loss functions to choose for a wide range of common tasks.</p>

  <h2 class="head" id="ch03lev1sec2"><a class="calibre3" id="ch03lev1sec2__title"></a>3.2. Introduction to Keras</h2>

  <p class="noind"><a id="iddle1472"></a><a id="iddle1696"></a>Throughout this book, the code examples use Keras (<a href="https://keras.io">https://keras.io</a>). Keras is a deep-learning framework for Python that provides a convenient way to define and train almost any kind of deep-learning model. Keras was initially developed for researchers, with the aim of enabling fast experimentation.</p>

  <p class="noind">Keras has the following key features:</p>

  <ul class="calibre16">
    <li class="calibre17">It allows the same code to run seamlessly on CPU or GPU.</li>

    <li class="calibre17">It has a user-friendly API that makes it easy to quickly prototype deep-learning models.</li>

    <li class="calibre17">It has built-in support for convolutional networks (for computer vision), recurrent networks (for sequence processing), and any combination of both.</li>

    <li class="calibre17">It supports arbitrary network architectures: multi-input or multi-output models, layer sharing, model sharing, and so on. This means Keras is appropriate for building essentially any deep-learning model, from a generative adversarial network to a neural Turing machine.</li>
  </ul>

  <p class="noind">Keras is distributed under the permissive MIT license, which means it can be freely used in commercial projects. It’s compatible with any version of Python from 2.7 to 3.6 (as of mid-2017).</p>

  <p class="noind">Keras has well over 200,000 users, ranging from academic researchers and engineers at both startups and large companies to graduate students and hobbyists. Keras is used at Google, Netflix, Uber, CERN, Yelp, Square, and hundreds of startups working on a wide range of problems. Keras is also a popular framework on Kaggle, the machine-learning competition website, where almost every recent deep-learning competition has been won using Keras models.</p>

  <p class="notetitle" id="ch03fig02">Figure 3.2. <a id="ch03fig02__title"></a>Google web search interest for different deep-learning frameworks over time</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/03fig02_alt.jpg"/></p>

  <h3 class="head1" id="ch03lev2sec4">3.2.1. <a id="ch03lev2sec4__title"></a>Keras, TensorFlow, Theano, and CNTK</h3>

  <p class="noind"><a id="iddle1052"></a><a id="iddle1099"></a><a id="iddle1143"></a><a id="iddle1257"></a><a id="iddle1473"></a><a id="iddle1474"></a><a id="iddle1476"></a><a id="iddle1477"></a><a id="iddle1603"></a><a id="iddle1697"></a><a id="iddle1698"></a><a id="iddle1699"></a><a id="iddle1700"></a><a id="iddle1983"></a>Keras is a model-level library, providing high-level building blocks for developing deep-learning models. It doesn’t handle low-level operations such as tensor manipulation and differentiation. Instead, it relies on a specialized, well-optimized tensor library to do so, serving as the <i class="calibre5">backend engine</i> of Keras. Rather than choosing a single tensor library and tying the implementation of Keras to that library, Keras handles the problem in a modular way (see <a href="#ch03fig03">figure 3.3</a>); thus several different backend engines can be plugged seamlessly into Keras. Currently, the three existing backend implementations are the TensorFlow backend, the Theano backend, and the Microsoft Cognitive Toolkit (CNTK) backend. In the future, it’s likely that Keras will be extended to work with even more deep-learning execution engines.</p>

  <p class="notetitle" id="ch03fig03">Figure 3.3. <a id="ch03fig03__title"></a>The deep-learning software and hardware stack</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/03fig03.jpg"/></p>

  <p class="noind">TensorFlow, CNTK, and Theano are some of the primary platforms for deep learning today. Theano (<a href="http://deeplearning.net/software/theano">http://deeplearning.net/software/theano</a>) is developed by the MILA lab at <i class="calibre5">Université de Montréal</i>, TensorFlow (<a href="http://www.tensorflow.org">www.tensorflow.org</a>) is developed by Google, and CNTK (<a href="https://github.com/Microsoft/CNTK">https://github.com/Microsoft/CNTK</a>) is developed by Microsoft. Any piece of code that you write with Keras can be run with any of these backends without having to change anything in the code: you can seamlessly switch between the two during development, which often proves useful—for instance, if one of these backends proves to be faster for a specific task. We recommend using the TensorFlow backend as the default for most of your deep-learning needs, because it’s the most widely adopted, scalable, and production ready.</p>

  <p class="noind">Via TensorFlow (or Theano, or CNTK), Keras is able to run seamlessly on both CPUs and GPUs. When running on CPU, TensorFlow is itself wrapping a low-level library for tensor operations called Eigen (<a href="http://eigen.tuxfamily.org">http://eigen.tuxfamily.org</a>). On GPU, Tensor-Flow wraps a library of well-optimized deep-learning operations called the NVIDIA CUDA Deep Neural Network library (cuDNN).</p>

  <h3 class="head1" id="ch03lev2sec5">3.2.2. <a id="ch03lev2sec5__title"></a>Developing with Keras: a quick overview</h3>

  <p class="noind">You’ve already seen one example of a Keras model: the MNIST example. The typical Keras workflow looks just like that example:</p>

  <ol class="calibre23">
    <li class="calibre17">Define your training data: input tensors and target tensors.</li>

    <li class="calibre17">Define a network of layers (or <i class="calibre5">model</i>) that maps your inputs to your targets.</li>

    <li class="calibre17"><a id="iddle1870"></a><a id="iddle1887"></a>Configure the learning process by choosing a loss function, an optimizer, and some metrics to monitor.</li>

    <li class="calibre17">Iterate on your training data by calling the <kbd class="calibre24">fit()</kbd> method of your model.</li>
  </ol>

  <p class="noind">There are two ways to define a model: using the <kbd class="calibre24">Sequential</kbd> class (only for linear stacks of layers, which is the most common network architecture by far) or the <i class="calibre5">functional API</i> (for directed acyclic graphs of layers, which lets you build completely arbitrary architectures).</p>

  <p class="noind">As a refresher, here’s a two-layer model defined using the <kbd class="calibre24">Sequential</kbd> class (note that we’re passing the expected shape of the input data to the first layer):</p>
  <pre class="calibre4" id="PLd0e6998">from keras import models
from keras import layers

model = models.Sequential()
model.add(layers.Dense(32, activation='relu', input_shape=(784,)))
model.add(layers.Dense(10, activation='softmax'))</pre>

  <p class="noind">And here’s the same model defined using the functional API:</p>
  <pre class="calibre4" id="PLd0e7007">input_tensor = layers.Input(shape=(784,))
x = layers.Dense(32, activation='relu')(input_tensor)
output_tensor = layers.Dense(10, activation='softmax')(x)

model = models.Model(inputs=input_tensor, outputs=output_tensor)</pre>

  <p class="noind">With the functional API, you’re manipulating the data tensors that the model processes and applying layers to this tensor as if they were functions.</p>
  <hr class="calibre25"/>

  <p class="notetitle" id="ch03note01">Note</p>

  <p class="noindclose">A detailed guide to what you can do with the functional API can be found in <a href="../Text/07.html#ch07" class="calibre13">chapter 7</a>. Until <a href="../Text/07.html#ch07" class="calibre13">chapter 7</a>, we’ll only be using the <kbd class="calibre27">Sequential</kbd> class in our code examples.</p>
  <hr class="calibre25"/>

  <p class="noind">Once your model architecture is defined, it doesn’t matter whether you used a <kbd class="calibre24">Sequential</kbd> model or the functional API. All of the following steps are the same.</p>

  <p class="noind">The learning process is configured in the compilation step, where you specify the optimizer and loss function(s) that the model should use, as well as the metrics you want to monitor during training. Here’s an example with a single loss function, which is by far the most common case:</p>
  <pre class="calibre4" id="PLd0e7042">from keras import optimizers

model.compile(optimizer=optimizers.RMSprop(lr=0.001),
              loss='mse',
              metrics=['accuracy'])</pre>

  <p class="noind">Finally, the learning process consists of passing Numpy arrays of input data (and the corresponding target data) to the model via the <kbd class="calibre24">fit()</kbd> method, similar to what you would do in Scikit-Learn and several other machine-learning libraries:</p>
  <pre class="calibre4" id="PLd0e7054">model.fit(input_tensor, target_tensor, batch_size=128, epochs=10)</pre>

  <p class="noind">Over the next few chapters, you’ll build a solid intuition about what type of network architectures work for different kinds of problems, how to pick the right learning configuration, and how to tweak a model until it gives the results you want to see. We’ll look at three basic examples in <a href="#ch03lev1sec4">sections 3.4</a>, <a href="#ch03lev1sec5">3.5</a>, and <a href="#ch03lev1sec6">3.6</a>: a two-class classification example, a many-class classification example, and a regression example.</p>

  <h2 class="head" id="ch03lev1sec3"><a class="calibre3" id="ch03lev1sec3__title"></a>3.3. Setting up a deep-learning workstation</h2>

  <p class="noind"><a id="iddle1445"></a><a id="iddle1703"></a><a id="iddle1705"></a><a id="iddle1804"></a><a id="iddle2027"></a><a id="iddle2098"></a><a id="iddle2099"></a>Before you can get started developing deep-learning applications, you need to set up your workstation. It’s highly recommended, although not strictly necessary, that you run deep-learning code on a modern NVIDIA GPU. Some applications—in particular, image processing with convolutional networks and sequence processing with recurrent neural networks—will be excruciatingly slow on CPU, even a fast multicore CPU. And even for applications that can realistically be run on CPU, you’ll generally see speed increase by a factor or 5 or 10 by using a modern GPU. If you don’t want to install a GPU on your machine, you can alternatively consider running your experiments on an AWS EC2 GPU instance or on Google Cloud Platform. But note that cloud GPU instances can become expensive over time.</p>

  <p class="noind">Whether you’re running locally or in the cloud, it’s better to be using a Unix workstation. Although it’s technically possible to use Keras on Windows (all three Keras backends support Windows), We don’t recommend it. In the installation instructions in <a href="../Text/A.html#app01">appendix A</a>, we’ll consider an Ubuntu machine. If you’re a Windows user, the simplest solution to get everything running is to set up an Ubuntu dual boot on your machine. It may seem like a hassle, but using Ubuntu will save you a lot of time and trouble in the long run.</p>

  <p class="noind">Note that in order to use Keras, you need to install TensorFlow <i class="calibre5">or</i> CNTK <i class="calibre5">or</i> Theano (or all of them, if you want to be able to switch back and forth among the three backends). In this book, we’ll focus on TensorFlow, with some light instructions relative to Theano. We won’t cover CNTK.</p>

  <h3 class="head1" id="ch03lev2sec6">3.3.1. <a id="ch03lev2sec6__title"></a>Jupyter notebooks: the preferred way to run deep-learning experiments</h3>

  <p class="noind">Jupyter notebooks are a great way to run deep-learning experiments—in particular, the many code examples in this book. They’re widely used in the data-science and machine-learning communities. A <i class="calibre5">notebook</i> is a file generated by the Jupyter Notebook app (<a href="https://jupyter.org">https://jupyter.org</a>), which you can edit in your browser. It mixes the ability to execute Python code with rich text-editing capabilities for annotating what you’re doing. A notebook also allows you to break up long experiments into smaller pieces that can be executed independently, which makes development interactive and means you don’t have to rerun all of your previous code if something goes wrong late in an experiment.</p>

  <p class="noind">We recommend using Jupyter notebooks to get started with Keras, although that isn’t a requirement: you can also run standalone Python scripts or run code from within an IDE such as PyCharm. All the code examples in this book are available as open source notebooks; you can download them from the book’s website at <a href="http://www.manning.com/books/deep-learning-with-python">www.manning.com/books/deep-learning-with-python</a>.</p>

  <h3 class="head1" id="ch03lev2sec7">3.3.2. <a id="ch03lev2sec7__title"></a>Getting Keras running: two options</h3>

  <p class="noind"><a id="iddle1096"></a><a id="iddle1219"></a><a id="iddle1350"></a><a id="iddle1475"></a><a id="iddle1704"></a><a id="iddle1706"></a><a id="iddle1707"></a><a id="iddle1732"></a><a id="iddle2100"></a><a id="iddle2101"></a><a id="iddle2102"></a>To get started in practice, we recommend one of the following two options:</p>

  <ul class="calibre16">
    <li class="calibre17">Use the official EC2 Deep Learning AMI (<a href="https://aws.amazon.com/amazon-ai/amis">https://aws.amazon.com/amazon-ai/amis</a>), and run Keras experiments as Jupyter notebooks on EC2. Do this if you don’t already have a GPU on your local machine. <a href="../Text/B.html#app02">Appendix B</a> provides a step-by-step guide.</li>

    <li class="calibre17">Install everything from scratch on a local Unix workstation. You can then run either local Jupyter notebooks or a regular Python codebase. Do this if you already have a high-end NVIDIA GPU. <a href="../Text/A.html#app01">Appendix A</a> provides an Ubuntu-specific, step-by-step guide.</li>
  </ul>

  <p class="noind">Let’s take a closer look at some of the compromises involved in picking one option over the other.</p>

  <h3 class="head1" id="ch03lev2sec8">3.3.3. <a id="ch03lev2sec8__title"></a>Running deep-learning jobs in the cloud: pros and cons</h3>

  <p class="noind">If you don’t already have a GPU that you can use for deep learning (a recent, high-end NVIDIA GPU), then running deep-learning experiments in the cloud is a simple, low-cost way for you to get started without having to buy any additional hardware. If you’re using Jupyter notebooks, the experience of running in the cloud is no different from running locally. As of mid-2017, the cloud offering that makes it easiest to get started with deep learning is definitely AWS EC2. <a href="../Text/B.html#app02">Appendix B</a> provides a step-by-step guide to running Jupyter notebooks on a EC2 GPU instance.</p>

  <p class="noind">But if you’re a heavy user of deep learning, this setup isn’t sustainable in the long term—or even for more than a few weeks. EC2 instances are expensive: the instance type recommended in <a href="../Text/B.html#app02">appendix B</a> (the <kbd class="calibre24">p2.xlarge</kbd> instance, which won’t provide you with much power) costs $0.90 per hour as of mid-2017. Meanwhile, a solid consumer-class GPU will cost you somewhere between $1,000 and $1,500—a price that has been fairly stable over time, even as the specs of these GPUs keep improving. If you’re serious about deep learning, you should set up a local workstation with one or more GPUs.</p>

  <p class="noind">In short, EC2 is a great way to get started. You could follow the code examples in this book entirely on an EC2 GPU instance. But if you’re going to be a power user of deep learning, get your own GPUs.</p>

  <h3 class="head1" id="ch03lev2sec9">3.3.4. <a id="ch03lev2sec9__title"></a>What is the best GPU for deep learning?</h3>

  <p class="noind">If you’re going to buy a GPU, which one should you choose? The first thing to note is that it must be an NVIDIA GPU. NVIDIA is the only graphics computing company that has invested heavily in deep learning so far, and modern deep-learning frameworks can only run on NVIDIA cards.</p>

  <p class="noind">As of mid-2017, we recommend the NVIDIA TITAN Xp as the best card on the market for deep learning. For lower budgets, you may want to consider the GTX 1060. If you’re reading these pages in 2018 or later, take the time to look online for fresher recommendations, because new models come out every year.</p>

  <p class="noind">From this section onward, we’ll assume that you have access to a machine with Keras and its dependencies installed—preferably with GPU support. Make sure you finish this step before you proceed. Go through the step-by-step guides in the appendixes, and look online if you need further help. There is no shortage of tutorials on how to install Keras and common deep-learning dependencies.</p>

  <p class="noind">We can now dive into practical Keras examples.</p>

  <h2 class="head" id="ch03lev1sec4"><a class="calibre3" id="ch03lev1sec4__title"></a>3.4. Classifying movie reviews: a binary classification example</h2>

  <p class="noind"><a id="iddle1069"></a><a id="iddle1609"></a><a id="iddle1670"></a><a id="iddle2009"></a>Two-class classification, or binary classification, may be the most widely applied kind of machine-learning problem. In this example, you’ll learn to classify movie reviews as positive or negative, based on the text content of the reviews.</p>

  <h3 class="head1" id="ch03lev2sec10">3.4.1. <a id="ch03lev2sec10__title"></a>The IMDB dataset</h3>

  <p class="noind">You’ll work with the IMDB dataset: a set of 50,000 highly polarized reviews from the Internet Movie Database. They’re split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting of 50% negative and 50% positive reviews.</p>

  <p class="noind">Why use separate training and test sets? Because you should never test a machine-learning model on the same data that you used to train it! Just because a model performs well on its training data doesn’t mean it will perform well on data it has never seen; and what you care about is your model’s performance on new data (because you already know the labels of your training data—obviously you don’t need your model to predict those). For instance, it’s possible that your model could end up merely <i class="calibre5">memorizing</i> a mapping between your training samples and their targets, which would be useless for the task of predicting targets for data the model has never seen before. We’ll go over this point in much more detail in the next chapter.</p>

  <p class="noind">Just like the MNIST dataset, the IMDB dataset comes packaged with Keras. It has already been preprocessed: the reviews (sequences of words) have been turned into sequences of integers, where each integer stands for a specific word in a dictionary.</p>

  <p class="noind">The following code will load the dataset (when you run it the first time, about 80 MB of data will be downloaded to your machine).</p>

  <p class="notetitle" id="ch03ex01">Listing 3.1. <a id="ch03ex01__title"></a>Loading the IMDB dataset</p>
  <pre class="calibre4" id="PLd0e7392">from keras.datasets import imdb

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(
    num_words=10000)</pre>

  <p class="noind">The argument <kbd class="calibre24">num_words=10000</kbd> means you’ll only keep the top 10,000 most frequently occurring words in the training data. Rare words will be discarded. This allows you to work with vector data of manageable size.</p>

  <p class="noind">The variables <kbd class="calibre24">train_data</kbd> and <kbd class="calibre24">test_data</kbd> are lists of reviews; each review is a list of word indices (encoding a sequence of words). <kbd class="calibre24">train_labels</kbd> and <kbd class="calibre24">test_labels</kbd> are lists of 0s and 1s, where 0 stands for <i class="calibre5">negative</i> and 1 stands for <i class="calibre5">positive</i>:</p>
  <pre class="calibre4" id="PLd0e7425">&gt;&gt;&gt; train_data[0]
[1, 14, 22, 16, ... 178, 32]

&gt;&gt;&gt; train_labels[0]
1</pre>

  <p class="noind"><a id="iddle1228"></a><a id="iddle2050"></a><a id="iddle2086"></a>Because you’re restricting yourself to the top 10,000 most frequent words, no word index will exceed 10,000:</p>
  <pre class="calibre4" id="PLd0e7450">&gt;&gt;&gt; max([max(sequence) for sequence in train_data])
9999</pre>

  <p class="noind">For kicks, here’s how you can quickly decode one of these reviews back to English words:</p>
  <pre class="calibre4" id="PLd0e7459">word_index = imdb.get_word_index()                                    <span class="cambriamathin">❶</span>
reverse_word_index = dict(
    [(value, key) for (key, value) in word_index.items()])            <span class="cambriamathin">❷</span>
decoded_review = ' '.join(
    [reverse_word_index.get(i - 3, '?') for i in train_data[0]])      <span class="cambriamathin">❸</span></pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> word_index is a dictionary mapping words to an integer index.</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> Reverses it, mapping integer indices to words</p>

    <p class="codeannotation"><span class="cambriamathin1">❸</span> Decodes the review. Note that the indices are offset by 3 because 0, 1, and 2 are reserved indices for “padding,” “start of sequence,” and “unknown.”</p>
  </div>

  <h3 class="head1" id="ch03lev2sec11">3.4.2. <a id="ch03lev2sec11__title"></a>Preparing the data</h3>

  <p class="noind">You can’t feed lists of integers into a neural network. You have to turn your lists into tensors. There are two ways to do that:</p>

  <ul class="calibre16">
    <li class="calibre17">Pad your lists so that they all have the same length, turn them into an integer tensor of shape <kbd class="calibre24">(samples, word_indices)</kbd>, and then use as the first layer in your network a layer capable of handling such integer tensors (the <kbd class="calibre24">Embedding</kbd> layer, which we’ll cover in detail later in the book).</li>

    <li class="calibre17">One-hot encode your lists to turn them into vectors of 0s and 1s. This would mean, for instance, turning the sequence <kbd class="calibre24">[3, 5]</kbd> into a 10,000-dimensional vector that would be all 0s except for indices 3 and 5, which would be 1s. Then you could use as the first layer in your network a <kbd class="calibre24">Dense</kbd> layer, capable of handling floating-point vector data.</li>
  </ul>

  <p class="noind">Let’s go with the latter solution to vectorize the data, which you’ll do manually for maximum clarity.</p>

  <p class="notetitle" id="ch03ex02">Listing 3.2. <a id="ch03ex02__title"></a>Encoding the integer sequences into a binary matrix</p>
  <pre class="calibre4" id="PLd0e7543">import numpy as np

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))        <span class="cambriamathin">❶</span>
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.                          <span class="cambriamathin">❷</span>
    return results

x_train = vectorize_sequences(train_data)                  <span class="cambriamathin">❸</span>
x_test = vectorize_sequences(test_data)                    <span class="cambriamathin">❹</span></pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> Creates an all-zero matrix of shape (len(sequences), dimension)</p>

    <p class="codeannotation"><a id="iddle1379"></a><span class="cambriamathin1">❷</span> Sets specific indices of results[i] to 1s</p>

    <p class="codeannotation"><span class="cambriamathin1">❸</span> Vectorized training data</p>

    <p class="codeannotation"><span class="cambriamathin1">❹</span> Vectorized test data</p>
  </div>

  <p class="noind">Here’s what the samples look like now:</p>
  <pre class="calibre4" id="PLd0e7608">&gt;&gt;&gt; x_train[0]
array([ 0.,  1.,  1., ...,  0.,  0.,  0.])</pre>

  <p class="noind">You should also vectorize your labels, which is straightforward:</p>
  <pre class="calibre4" id="PLd0e7617">y_train = np.asarray(train_labels).astype('float32')
y_test = np.asarray(test_labels).astype('float32')</pre>

  <p class="noind">Now the data is ready to be fed into a neural network.</p>

  <h3 class="head1" id="ch03lev2sec12">3.4.3. <a id="ch03lev2sec12__title"></a>Building your network</h3>

  <p class="noind">The input data is vectors, and the labels are scalars (1s and 0s): this is the easiest setup you’ll ever encounter. A type of network that performs well on such a problem is a simple stack of fully connected (<kbd class="calibre24">Dense</kbd>) layers with <kbd class="calibre24">relu</kbd> activations: <kbd class="calibre24">Dense(16,</kbd> <kbd class="calibre24">activation='relu')</kbd>.</p>

  <p class="noind">The argument being passed to each <kbd class="calibre24">Dense</kbd> layer (16) is the number of hidden units of the layer. A <i class="calibre5">hidden unit</i> is a dimension in the representation space of the layer. You may remember from <a href="../Text/02.html#ch02">chapter 2</a> that each such <kbd class="calibre24">Dense</kbd> layer with a <kbd class="calibre24">relu</kbd> activation implements the following chain of tensor operations:</p>
  <pre class="calibre4" id="PLd0e7665">output = relu(dot(W, input) + b)</pre>

  <p class="noind">Having 16 hidden units means the weight matrix <kbd class="calibre24">W</kbd> will have shape <kbd class="calibre24">(input_dimension, 16)</kbd>: the dot product with <kbd class="calibre24">W</kbd> will project the input data onto a 16-dimensional representation space (and then you’ll add the bias vector <kbd class="calibre24">b</kbd> and apply the <kbd class="calibre24">relu</kbd> operation). You can intuitively understand the dimensionality of your representation space as “how much freedom you’re allowing the network to have when learning internal representations.” Having more hidden units (a higher-dimensional representation space) allows your network to learn more-complex representations, but it makes the network more computationally expensive and may lead to learning unwanted patterns (patterns that will improve performance on the training data but not on the test data).</p>

  <p class="noind">There are two key architecture decisions to be made about such a stack of <kbd class="calibre24">Dense</kbd> layers:</p>

  <ul class="calibre16">
    <li class="calibre17">How many layers to use</li>

    <li class="calibre17">How many hidden units to choose for each layer</li>
  </ul>

  <p class="noind">In <a href="../Text/04.html#ch04">chapter 4</a>, you’ll learn formal principles to guide you in making these choices. For the time being, you’ll have to trust me with the following architecture choice:</p>

  <ul class="calibre16">
    <li class="calibre17">Two intermediate layers with 16 hidden units each</li>

    <li class="calibre17"><a id="iddle1840"></a><a id="iddle1899"></a>A third layer that will output the scalar prediction regarding the sentiment of the current review</li>
  </ul>

  <p class="noind">The intermediate layers will use <kbd class="calibre24">relu</kbd> as their activation function, and the final layer will use a sigmoid activation so as to output a probability (a score between 0 and 1, indicating how likely the sample is to have the target “1”: how likely the review is to be positive). A <kbd class="calibre24">relu</kbd> (rectified linear unit) is a function meant to zero out negative values (see <a href="#ch03fig04">figure 3.4</a>), whereas a sigmoid “squashes” arbitrary values into the <kbd class="calibre24">[0, 1]</kbd> interval (see <a href="#ch03fig05">figure 3.5</a>), outputting something that can be interpreted as a probability.</p>

  <p class="notetitle" id="ch03fig04">Figure 3.4. <a id="ch03fig04__title"></a>The rectified linear unit function</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/03fig04_alt.jpg"/></p>

  <p class="notetitle" id="ch03fig05">Figure 3.5. <a id="ch03fig05__title"></a>The sigmoid function</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/03fig05_alt.jpg"/></p>

  <p class="noind"><a id="iddle1016"></a><a id="iddle1072"></a><a id="iddle1395"></a><a id="iddle1519"></a><a id="iddle1721"></a><a href="#ch03fig06">Figure 3.6</a> shows what the network looks like. And here’s the Keras implementation, similar to the MNIST example you saw previously.</p>

  <p class="notetitle" id="ch03fig06">Figure 3.6. <a id="ch03fig06__title"></a>The three-layer network</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/03fig06.jpg"/></p>

  <p class="notetitle" id="ch03ex03">Listing 3.3. <a id="ch03ex03__title"></a>The model definition</p>
  <pre class="calibre4" id="PLd0e7825">from keras import models
from keras import layers

model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))</pre>
  <hr class="calibre25"/>

  <div class="calibre14">
    <b class="calibre26" id="ch03sb01">What are activation functions, and why are they necessary?</b>

    <p class="noind">Without an activation function like <kbd class="calibre24">relu</kbd> (also called a <i class="calibre5">non-linearity</i>), the <kbd class="calibre24">Dense</kbd> layer would consist of two linear operations—a dot product and an addition:</p>
    <pre class="calibre4" id="PLd0e7848">output = dot(W, input) + b</pre>

    <p class="noind">So the layer could only learn <i class="calibre5">linear transformations</i> (affine transformations) of the input data: the <i class="calibre5">hypothesis space</i> of the layer would be the set of all possible linear transformations of the input data into a 16-dimensional space. Such a hypothesis space is too restricted and wouldn’t benefit from multiple layers of representations, because a deep stack of linear layers would still implement a linear operation: adding more layers wouldn’t extend the hypothesis space.</p>

    <p class="noind">In order to get access to a much richer hypothesis space that would benefit from deep representations, you need a non-linearity, or activation function. <kbd class="calibre24">relu</kbd> is the most popular activation function in deep learning, but there are many other candidates, which all come with similarly strange names: <kbd class="calibre24">prelu</kbd>, <kbd class="calibre24">elu</kbd>, and so on.</p>
  </div>
  <hr class="calibre25"/>

  <p class="noind">Finally, you need to choose a loss function and an optimizer. Because you’re facing a binary classification problem and the output of your network is a probability (you end your network with a single-unit layer with a sigmoid activation), it’s best to use the <a id="iddle1139"></a><a id="iddle1599"></a><a id="iddle1752"></a><a id="iddle1851"></a><a id="iddle2038"></a><kbd class="calibre24">binary_crossentropy</kbd> loss. It isn’t the only viable choice: you could use, for instance, <kbd class="calibre24">mean_squared_error</kbd>. But crossentropy is usually the best choice when you’re dealing with models that output probabilities. <i class="calibre5">Crossentropy</i> is a quantity from the field of Information Theory that measures the distance between probability distributions or, in this case, between the ground-truth distribution and your predictions.</p>

  <p class="noind">Here’s the step where you configure the model with the <kbd class="calibre24">rmsprop</kbd> optimizer and the <kbd class="calibre24">binary_crossentropy</kbd> loss function. Note that you’ll also monitor accuracy during training.</p>

  <p class="notetitle" id="ch03ex04">Listing 3.4. <a id="ch03ex04__title"></a>Compiling the model</p>
  <pre class="calibre4" id="PLd0e7930">model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy'])</pre>

  <p class="noind">You’re passing your optimizer, loss function, and metrics as strings, which is possible because <kbd class="calibre24">rmsprop</kbd>, <kbd class="calibre24">binary_crossentropy</kbd>, and <kbd class="calibre24">accuracy</kbd> are packaged as part of Keras. Sometimes you may want to configure the parameters of your optimizer or pass a custom loss function or metric function. The former can be done by passing an optimizer class instance as the <kbd class="calibre24">optimizer</kbd> argument, as shown in <a href="#ch03ex05">listing 3.5</a>; the latter can be done by passing function objects as the <kbd class="calibre24">loss</kbd> and/or <kbd class="calibre24">metrics</kbd> arguments, as shown in <a href="#ch03ex06">listing 3.6</a>.</p>

  <p class="notetitle" id="ch03ex05">Listing 3.5. <a id="ch03ex05__title"></a>Configuring the optimizer</p>
  <pre class="calibre4" id="PLd0e7966">from keras import optimizers

model.compile(optimizer=optimizers.RMSprop(lr=0.001),
              loss='binary_crossentropy',
              metrics=['accuracy'])</pre>

  <p class="notetitle" id="ch03ex06">Listing 3.6. <a id="ch03ex06__title"></a>Using custom losses and metrics</p>
  <pre class="calibre4" id="PLd0e7975">from keras import losses
from keras import metrics

model.compile(optimizer=optimizers.RMSprop(lr=0.001),
              loss=losses.binary_crossentropy,
              metrics=[metrics.binary_accuracy])</pre>

  <h3 class="head1" id="ch03lev2sec13">3.4.4. <a id="ch03lev2sec13__title"></a>Validating your approach</h3>

  <p class="noind">In order to monitor during training the accuracy of the model on data it has never seen before, you’ll create a validation set by setting apart 10,000 samples from the original training data.</p>

  <p class="notetitle" id="ch03ex07">Listing 3.7. <a id="ch03ex07__title"></a>Setting aside a validation set</p>
  <pre class="calibre4" id="PLd0e7993">x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]</pre>

  <p class="noind"><a id="iddle1263"></a><a id="iddle1589"></a><a id="iddle2039"></a>You’ll now train the model for 20 epochs (20 iterations over all samples in the <kbd class="calibre24">x_train</kbd> and <kbd class="calibre24">y_train</kbd> tensors), in mini-batches of 512 samples. At the same time, you’ll monitor loss and accuracy on the 10,000 samples that you set apart. You do so by passing the validation data as the <kbd class="calibre24">validation_data</kbd> argument.</p>

  <p class="notetitle" id="ch03ex08">Listing 3.8. <a id="ch03ex08__title"></a>Training your model</p>
  <pre class="calibre4" id="PLd0e8031">model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['acc'])

history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))</pre>

  <p class="noind">On CPU, this will take less than 2 seconds per epoch—training is over in 20 seconds. At the end of every epoch, there is a slight pause as the model computes its loss and accuracy on the 10,000 samples of the validation data.</p>

  <p class="noind">Note that the call to <kbd class="calibre24">model.fit()</kbd> returns a <kbd class="calibre24">History</kbd> object. This object has a member <kbd class="calibre24">history</kbd>, which is a dictionary containing data about everything that happened during training. Let’s look at it:</p>
  <pre class="calibre4" id="PLd0e8052">&gt;&gt;&gt; history_dict = history.history
&gt;&gt;&gt; history_dict.keys()
[u'acc', u'loss', u'val_acc', u'val_loss']</pre>

  <p class="noind">The dictionary contains four entries: one per metric that was being monitored during training and during validation. In the following two listing, let’s use Matplotlib to plot the training and validation loss side by side (see <a href="#ch03fig07">figure 3.7</a>), as well as the training and validation accuracy (see <a href="#ch03fig08">figure 3.8</a>). Note that your own results may vary slightly due to a different random initialization of your network.</p>

  <p class="notetitle" id="ch03ex09">Listing 3.9 Plotting the training and validation loss</p>
  <pre class="calibre4" id="PLd0e8101">import matplotlib.pyplot as plt

history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']

epochs = range(1, len(loss_values) + 1)

plt.plot(epochs, loss_values, 'bo', label='Training loss')           <span class="cambriamathin">❶</span>
plt.plot(epochs, val_loss_values, 'b', label='Validation loss')      <span class="cambriamathin">❷</span>
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()</pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> “bo” is for “blue dot.”</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> “b” is for “solid blue line.”</p>
  </div>

  <p class="notetitle" id="ch03fig07">Figure 3.7.&#160;<a id="ch03fig07__title"></a>Training and validation loss</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/03fig07b.jpg"/></p>

  <p class="notetitle" id="ch03ex10">Listing 3.10. <a id="ch03ex10__title"></a>Plotting the training and validation accuracy</p>
  <pre class="calibre4" id="PLd0e8137">plt.clf()                                      <span class="cambriamathin">❶</span>
acc = history_dict['acc']
val_acc = history_dict['val_acc']

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()</pre>

  <div class="annotations">
    <p class="codeannotation"><a id="iddle1264"></a><a id="iddle1774"></a><span class="cambriamathin1">❶</span> Clears the figure</p>
  </div>

  <p class="noind"></p>

  <p class="center2">Figure 3.8.&#160;<a id="ch03fig08" class="calibre13"></a>Training and validation accuracy</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/03fig08b.jpg"/></p>

  <p class="notetitle">Listing 3.9.&#160;<a id="ch03ex09__title"></a>Plotting the training and validation loss</p>

  <p class="noind">As you can see, the training loss decreases with every epoch, and the training accuracy increases with every epoch. That’s what you would expect when running gradient-descent optimization—the quantity you’re trying to minimize should be less with every iteration. But that isn’t the case for the validation loss and accuracy: they seem to peak at the fourth epoch. This is an example of what we warned against earlier: a model that performs better on the training data isn’t necessarily a model that will do better on data it has never seen before. In precise terms, what you’re seeing is <i class="calibre5">overfitting</i>: after the second epoch, you’re overoptimizing on the training data, and you end up learning representations that are specific to the training data and don’t generalize to data outside of the training set.</p>

  <p class="noind">In this case, to prevent overfitting, you could stop training after three epochs. In general, you can use a range of techniques to mitigate overfitting, which we’ll cover in <a href="../Text/04.html#ch04">chapter 4</a>.</p>

  <p class="noind">Let’s train a new network from scratch for four epochs and then evaluate it on the test data.</p>

  <p class="notetitle" id="ch03ex11">Listing 3.11. <a id="ch03ex11__title"></a>Retraining a model from scratch</p>
  <pre class="calibre4" id="PLd0e8188">model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=4, batch_size=512)
results = model.evaluate(x_test, y_test)</pre>

  <p class="noind">The final results are as follows:</p>
  <pre class="calibre4" id="PLd0e8197">&gt;&gt;&gt; results
[0.2929924130630493, 0.88327999999999995]</pre>

  <p class="noind">This fairly naive approach achieves an accuracy of 88%. With state-of-the-art approaches, you should be able to get close to 95%.</p>

  <h3 class="head1" id="ch03lev2sec14">3.4.5. <a id="ch03lev2sec14__title"></a>Using a trained network to generate predictions on new data</h3>

  <p class="noind">After having trained a network, you’ll want to use it in a practical setting. You can generate the likelihood of reviews being positive by using the <kbd class="calibre24">predict</kbd> method:</p>
  <pre class="calibre4" id="PLd0e8220">&gt;&gt;&gt; model.predict(x_test)
array([[ 0.98006207]
       [ 0.99758697]
       [ 0.99975556]
       ...,
       [ 0.82167041]
       [ 0.02885115]
       [ 0.65371346]], dtype=float32)</pre>

  <p class="noind"><a id="iddle1378"></a><a id="iddle1645"></a><a id="iddle1754"></a><a id="iddle1852"></a><a id="iddle1939"></a>As you can see, the network is confident for some samples (0.99 or more, or 0.01 or less) but less confident for others (0.6, 0.4).</p>

  <h3 class="head1" id="ch03lev2sec15">3.4.6. <a id="ch03lev2sec15__title"></a>Further experiments</h3>

  <p class="noind">The following experiments will help convince you that the architecture choices you’ve made are all fairly reasonable, although there’s still room for improvement:</p>

  <ul class="calibre16">
    <li class="calibre17">You used two hidden layers. Try using one or three hidden layers, and see how doing so affects validation and test accuracy.</li>

    <li class="calibre17">Try using layers with more hidden units or fewer hidden units: 32 units, 64 units, and so on.</li>

    <li class="calibre17">Try using the <kbd class="calibre24">mse</kbd> loss function instead of <kbd class="calibre24">binary_crossentropy</kbd>.</li>

    <li class="calibre17">Try using the <kbd class="calibre24">tanh</kbd> activation (an activation that was popular in the early days of neural networks) instead of <kbd class="calibre24">relu</kbd>.</li>
  </ul>

  <h3 class="head1" id="ch03lev2sec16">3.4.7. <a id="ch03lev2sec16__title"></a>Wrapping up</h3>

  <p class="noind">Here’s what you should take away from this example:</p>

  <ul class="calibre16">
    <li class="calibre17">You usually need to do quite a bit of preprocessing on your raw data in order to be able to feed it—as tensors—into a neural network. Sequences of words can be encoded as binary vectors, but there are other encoding options, too.</li>

    <li class="calibre17">Stacks of <kbd class="calibre24">Dense</kbd> layers with <kbd class="calibre24">relu</kbd> activations can solve a wide range of problems (including sentiment classification), and you’ll likely use them frequently.</li>

    <li class="calibre17">In a binary classification problem (two output classes), your network should end with a <kbd class="calibre24">Dense</kbd> layer with one unit and a <kbd class="calibre24">sigmoid</kbd> activation: the output of your network should be a scalar between 0 and 1, encoding a probability.</li>

    <li class="calibre17">With such a scalar sigmoid output on a binary classification problem, the loss function you should use is <kbd class="calibre24">binary_crossentropy</kbd>.</li>

    <li class="calibre17">The <kbd class="calibre24">rmsprop</kbd> optimizer is generally a good enough choice, whatever your problem. That’s one less thing for you to worry about.</li>

    <li class="calibre17">As they get better on their training data, neural networks eventually start overfitting and end up obtaining increasingly worse results on data they’ve never seen before. Be sure to always monitor performance on data that is outside of the training set.</li>
  </ul>

  <h2 class="head" id="ch03lev1sec5"><a class="calibre3" id="ch03lev1sec5__title"></a>3.5. Classifying newswires: a multiclass classification example</h2>

  <p class="noind"><a id="iddle1650"></a><a id="iddle1653"></a><a id="iddle1701"></a><a id="iddle1906"></a>In the previous section, you saw how to classify vector inputs into two mutually exclusive classes using a densely connected neural network. But what happens when you have more than two classes?</p>

  <p class="noind">In this section, you’ll build a network to classify Reuters newswires into 46 mutually exclusive topics. Because you have many classes, this problem is an instance of <i class="calibre5">multiclass classification</i>; and because each data point should be classified into only one category, the problem is more specifically an instance of <i class="calibre5">single-label, multiclass classification</i>. If each data point could belong to multiple categories (in this case, topics), you’d be facing a <i class="calibre5">multilabel, multiclass classification</i> problem.</p>

  <h3 class="head1" id="ch03lev2sec17">3.5.1. <a id="ch03lev2sec17__title"></a>The Reuters dataset</h3>

  <p class="noind">You’ll work with the <i class="calibre5">Reuters dataset</i>, a set of short newswires and their topics, published by Reuters in 1986. It’s a simple, widely used toy dataset for text classification. There are 46 different topics; some topics are more represented than others, but each topic has at least 10 examples in the training set.</p>

  <p class="noind">Like IMDB and MNIST, the Reuters dataset comes packaged as part of Keras. Let’s take a look.</p>

  <p class="notetitle" id="ch03ex12">Listing 3.12. <a id="ch03ex12__title"></a>Loading the Reuters dataset</p>
  <pre class="calibre4" id="PLd0e8428">from keras.datasets import reuters

(train_data, train_labels), (test_data, test_labels) = reuters.load_data(
    num_words=10000)</pre>

  <p class="noind">As with the IMDB dataset, the argument <kbd class="calibre24">num_words=10000</kbd> restricts the data to the 10,000 most frequently occurring words found in the data.</p>

  <p class="noind">You have 8,982 training examples and 2,246 test examples:</p>
  <pre class="calibre4" id="PLd0e8443">&gt;&gt;&gt; len(train_data)
8982
&gt;&gt;&gt; len(test_data)
2246</pre>

  <p class="noind">As with the IMDB reviews, each example is a list of integers (word indices):</p>
  <pre class="calibre4" id="PLd0e8452">&gt;&gt;&gt; train_data[10]
[1, 245, 273, 207, 156, 53, 74, 160, 26, 14, 46, 296, 26, 39, 74, 2979,
3554, 14, 46, 4689, 4329, 86, 61, 3499, 4795, 14, 61, 451, 4329, 17, 12]</pre>

  <p class="noind">Here’s how you can decode it back to words, in case you’re curious.</p>

  <p class="notetitle" id="ch03ex13">Listing 3.13. <a id="ch03ex13__title"></a>Decoding newswires back to text</p>
  <pre class="calibre4" id="PLd0e8464">word_index = reuters.get_word_index()
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in
    train_data[0]])                                                          <span class="cambriamathin">❶</span></pre>

  <div class="annotations">
    <p class="codeannotation"><a id="iddle1081"></a><a id="iddle1740"></a><span class="cambriamathin1">❶</span> Note that the indices are offset by 3 because 0, 1, and 2 are reserved indices for “padding,” “start of sequence,” and “unknown.”</p>
  </div>

  <p class="noind">The label associated with an example is an integer between 0 and 45—a topic index:</p>
  <pre class="calibre4" id="PLd0e8502">&gt;&gt;&gt; train_labels[10]
3</pre>

  <h3 class="head1" id="ch03lev2sec18">3.5.2. <a id="ch03lev2sec18__title"></a>Preparing the data</h3>

  <p class="noind">You can vectorize the data with the exact same code as in the previous example.</p>

  <p class="notetitle" id="ch03ex14">Listing 3.14. <a id="ch03ex14__title"></a>Encoding the data</p>
  <pre class="calibre4" id="PLd0e8520">import numpy as np

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

x_train = vectorize_sequences(train_data)            <span class="cambriamathin">❶</span>
x_test = vectorize_sequences(test_data)              <span class="cambriamathin">❷</span></pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> Vectorized training data</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> Vectorized test data</p>
  </div>

  <p class="noind">To vectorize the labels, there are two possibilities: you can cast the label list as an integer tensor, or you can use one-hot encoding. One-hot encoding is a widely used format for categorical data, also called <i class="calibre5">categorical encoding</i>. For a more detailed explanation of one-hot encoding, see <a href="../Text/06.html#ch06lev1sec1">section 6.1</a>. In this case, one-hot encoding of the labels consists of embedding each label as an all-zero vector with a 1 in the place of the label index. Here’s an example:</p>
  <pre class="calibre4" id="PLd0e8561">def to_one_hot(labels, dimension=46):
    results = np.zeros((len(labels), dimension))
    for i, label in enumerate(labels):
        results[i, label] = 1.
    return results

one_hot_train_labels = to_one_hot(train_labels)        <span class="cambriamathin">❶</span>
one_hot_test_labels = to_one_hot(test_labels)          <span class="cambriamathin">❷</span></pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> Vectorized training labels</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> Vectorized test labels</p>
  </div>

  <p class="noind">Note that there is a built-in way to do this in Keras, which you’ve already seen in action in the MNIST example:</p>
  <pre class="calibre4" id="PLd0e8596">from keras.utils.np_utils import to_categorical

one_hot_train_labels = to_categorical(train_labels)
one_hot_test_labels = to_categorical(test_labels)</pre>

  <h3 class="head1" id="ch03lev2sec19">3.5.3. <a id="ch03lev2sec19__title"></a>Building your network</h3>

  <p class="noind"><a id="iddle1083"></a><a id="iddle1421"></a><a id="iddle1795"></a><a id="iddle1911"></a>This topic-classification problem looks similar to the previous movie-review classification problem: in both cases, you’re trying to classify short snippets of text. But there is a new constraint here: the number of output classes has gone from 2 to 46. The dimensionality of the output space is much larger.</p>

  <p class="noind">In a stack of <kbd class="calibre24">Dense</kbd> layers like that you’ve been using, each layer can only access information present in the output of the previous layer. If one layer drops some information relevant to the classification problem, this information can never be recovered by later layers: each layer can potentially become an information bottleneck. In the previous example, you used 16-dimensional intermediate layers, but a 16-dimensional space may be too limited to learn to separate 46 different classes: such small layers may act as information bottlenecks, permanently dropping relevant information.</p>

  <p class="noind">For this reason you’ll use larger layers. Let’s go with 64 units.</p>

  <p class="notetitle" id="ch03ex15">Listing 3.15. <a id="ch03ex15__title"></a>Model definition</p>
  <pre class="calibre4" id="PLd0e8645">from keras import models
from keras import layers

model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(46, activation='softmax'))</pre>

  <p class="noind">There are two other things you should note about this architecture:</p>

  <ul class="calibre16">
    <li class="calibre17">You end the network with a <kbd class="calibre24">Dense</kbd> layer of size 46. This means for each input sample, the network will output a 46-dimensional vector. Each entry in this vector (each dimension) will encode a different output class.</li>

    <li class="calibre17">The last layer uses a <kbd class="calibre24">softmax</kbd> activation. You saw this pattern in the MNIST example. It means the network will output a <i class="calibre5">probability distribution</i> over the 46 different output classes—for every input sample, the network will produce a 46-dimensional output vector, where <kbd class="calibre24">output[i]</kbd> is the probability that the sample belongs to class <kbd class="calibre24">i</kbd>. The 46 scores will sum to 1.</li>
  </ul>

  <p class="noind">The best loss function to use in this case is <kbd class="calibre24">categorical_crossentropy</kbd>. It measures the distance between two probability distributions: here, between the probability distribution output by the network and the true distribution of the labels. By minimizing the distance between these two distributions, you train the network to output something as close as possible to the true labels.</p>

  <p class="notetitle" id="ch03ex16">Listing 3.16. <a id="ch03ex16__title"></a>Compiling the model</p>
  <pre class="calibre4" id="PLd0e8691">model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])</pre>

  <h3 class="head1" id="ch03lev2sec20">3.5.4. <a id="ch03lev2sec20__title"></a>Validating your approach</h3>

  <p class="noind">Let’s set apart 1,000 samples in the training data to use as a validation set.</p>

  <p class="notetitle" id="ch03ex17">Listing 3.17. <a id="ch03ex17__title"></a>Setting aside a validation set</p>
  <pre class="calibre4" id="PLd0e8711">x_val = x_train[:1000]
partial_x_train = x_train[1000:]

y_val = one_hot_train_labels[:1000]
partial_y_train = one_hot_train_labels[1000:]</pre>

  <p class="noind">Now, let’s train the network for 20 epochs.</p>

  <p class="notetitle" id="ch03ex18">Listing 3.18. <a id="ch03ex18__title"></a>Training the model</p>
  <pre class="calibre4" id="PLd0e8723">history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))</pre>

  <p class="noind">And finally, let’s display its loss and accuracy curves (see <a href="#ch03fig09">figures 3.9</a> and <a href="#ch03fig10">3.10</a>).</p>

  <p class="notetitle" id="ch03fig09">Figure 3.9. <a id="ch03fig09__title"></a>Training and validation loss</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/03fig09.jpg"/></p>

  <p class="noind"></p>

  <p class="notetitle" id="ch03fig10">Figure 3.10. <a id="ch03fig10__title"></a>Training and validation accuracy</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/03fig10b.jpg"/></p>

  <p class="notetitle" id="ch03ex19">Listing 3.19. <a id="ch03ex19__title"></a>Plotting the training and validation loss</p>
  <pre class="calibre4" id="PLd0e8769">import matplotlib.pyplot as plt

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(loss) + 1)

plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()</pre>

  <p class="notetitle" id="ch03ex20">Listing 3.20. <a id="ch03ex20__title"></a>Plotting the training and validation accuracy</p>
  <pre class="calibre4" id="PLd0e8778">plt.clf()                                                <span class="cambriamathin">❶</span>

acc = history.history['acc']
val_acc = history.history['val_acc']

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()</pre>

  <div class="annotations">
    <p class="codeannotation"><a id="iddle1265"></a><span class="cambriamathin1">❶</span> Clears the figure</p>
  </div>

  <p class="noind">The network begins to overfit after nine epochs. Let’s train a new network from scratch for nine epochs and then evaluate it on the test set.</p>

  <p class="notetitle" id="ch03ex21">Listing 3.21. <a id="ch03ex21__title"></a>Retraining a model from scratch</p>
  <pre class="calibre4" id="PLd0e8810">model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(46, activation='softmax'))

model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
model.fit(partial_x_train,
          partial_y_train,
          epochs=9,
          batch_size=512,
          validation_data=(x_val, y_val))
results = model.evaluate(x_test, one_hot_test_labels)</pre>

  <p class="noind"><a id="iddle1084"></a><a id="iddle1775"></a><a id="iddle1778"></a><a id="iddle1916"></a>Here are the final results:</p>
  <pre class="calibre4" id="PLd0e8840">&gt;&gt;&gt; results
[0.9565213431445807, 0.79697239536954589]</pre>

  <p class="noind">This approach reaches an accuracy of ~80%. With a balanced binary classification problem, the accuracy reached by a purely random classifier would be 50%. But in this case it’s closer to 19%, so the results seem pretty good, at least when compared to a random baseline:</p>
  <pre class="calibre4" id="PLd0e8849">&gt;&gt;&gt; import copy
&gt;&gt;&gt; test_labels_copy = copy.copy(test_labels)
&gt;&gt;&gt; np.random.shuffle(test_labels_copy)
&gt;&gt;&gt; hits_array = np.array(test_labels) == np.array(test_labels_copy)
&gt;&gt;&gt; float(np.sum(hits_array)) / len(test_labels)
0.18655387355298308</pre>

  <h3 class="head1" id="ch03lev2sec21">3.5.5. <a id="ch03lev2sec21__title"></a>Generating predictions on new data</h3>

  <p class="noind">You can verify that the <kbd class="calibre24">predict</kbd> method of the model instance returns a probability distribution over all 46 topics. Let’s generate topic predictions for all of the test data.</p>

  <p class="notetitle" id="ch03ex22">Listing 3.22. <a id="ch03ex22__title"></a>Generating predictions for new data</p>
  <pre class="calibre4" id="PLd0e8870">predictions = model.predict(x_test)</pre>

  <p class="noind">Each entry in <kbd class="calibre24">predictions</kbd> is a vector of length 46:</p>
  <pre class="calibre4" id="PLd0e8882">&gt;&gt;&gt; predictions[0].shape
(46,)</pre>

  <p class="noind">The coefficients in this vector sum to 1:</p>
  <pre class="calibre4" id="PLd0e8891">&gt;&gt;&gt; np.sum(predictions[0])
1.0</pre>

  <p class="noind">The largest entry is the predicted class—the class with the highest probability:</p>
  <pre class="calibre4" id="PLd0e8900">&gt;&gt;&gt; np.argmax(predictions[0])
4</pre>

  <h3 class="head1" id="ch03lev2sec22">3.5.6. <a id="ch03lev2sec22__title"></a>A different way to handle the labels and the loss</h3>

  <p class="noind">We mentioned earlier that another way to encode the labels would be to cast them as an integer tensor, like this:</p>
  <pre class="calibre4" id="PLd0e8915">y_train = np.array(train_labels)
y_test = np.array(test_labels)</pre>

  <p class="noind">The only thing this approach would change is the choice of the loss function. The loss function used in <a href="#ch03ex21">listing 3.21</a>, <kbd class="calibre24">categorical_crossentropy</kbd>, expects the labels to follow a categorical encoding. With integer labels, you should use <kbd class="calibre24">sparse_categorical_crossentropy</kbd>:</p>
  <pre class="calibre4" id="PLd0e8935">model.compile(optimizer='rmsprop',
              loss='sparse_categorical_crossentropy',
              metrics=['acc'])</pre>

  <p class="noind"><a id="iddle1422"></a><a id="iddle1658"></a><a id="iddle1741"></a><a id="iddle1912"></a>This new loss function is still mathematically the same as <kbd class="calibre24">categorical_crossentropy</kbd>; it just has a different interface.</p>

  <h3 class="head1" id="ch03lev2sec23">3.5.7. <a id="ch03lev2sec23__title"></a>The importance of having sufficiently large intermediate layers</h3>

  <p class="noind">We mentioned earlier that because the final outputs are 46-dimensional, you should avoid intermediate layers with many fewer than 46 hidden units. Now let’s see what happens when you introduce an information bottleneck by having intermediate layers that are significantly less than 46-dimensional: for example, 4-dimensional.</p>

  <p class="notetitle" id="ch03ex23">Listing 3.23. <a id="ch03ex23__title"></a>A model with an information bottleneck</p>
  <pre class="calibre4" id="PLd0e8982">model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(4, activation='relu'))
model.add(layers.Dense(46, activation='softmax'))

model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
model.fit(partial_x_train,
          partial_y_train,
          epochs=20,
          batch_size=128,
          validation_data=(x_val, y_val))</pre>

  <p class="noind">The network now peaks at ~71% validation accuracy, an 8% absolute drop. This drop is mostly due to the fact that you’re trying to compress a lot of information (enough information to recover the separation hyperplanes of 46 classes) into an intermediate space that is too low-dimensional. The network is able to cram <i class="calibre5">most</i> of the necessary information into these four-dimensional representations, but not all of it.</p>

  <h3 class="head1" id="ch03lev2sec24">3.5.8. <a id="ch03lev2sec24__title"></a>Further experiments</h3>

  <ul class="calibre16">
    <li class="calibre17">Try using larger or smaller layers: 32 units, 128 units, and so on.</li>

    <li class="calibre17">You used two hidden layers. Now try using a single hidden layer, or three hidden layers.</li>
  </ul>

  <h3 class="head1" id="ch03lev2sec25">3.5.9. <a id="ch03lev2sec25__title"></a>Wrapping up</h3>

  <p class="noind">Here’s what you should take away from this example:</p>

  <ul class="calibre16">
    <li class="calibre17">If you’re trying to classify data points among <i class="calibre5">N</i> classes, your network should end with a <kbd class="calibre24">Dense</kbd> layer of size <i class="calibre5">N</i>.</li>

    <li class="calibre17">In a single-label, multiclass classification problem, your network should end with a <kbd class="calibre24">softmax</kbd> activation so that it will output a probability distribution over the <i class="calibre5">N</i> output classes.</li>

    <li class="calibre17"><a id="iddle1522"></a><a id="iddle1702"></a><a id="iddle1835"></a>Categorical crossentropy is almost always the loss function you should use for such problems. It minimizes the distance between the probability distributions output by the network and the true distribution of the targets.</li>

    <li class="calibre17">There are two ways to handle labels in multiclass classification:

      <ul class="calibre28">
        <li class="calibre29">Encoding the labels via categorical encoding (also known as one-hot encoding) and using <kbd class="calibre24">categorical_crossentropy</kbd> as a loss function</li>

        <li class="calibre29">Encoding the labels as integers and using the <kbd class="calibre24">sparse_categorical_-crossentropy</kbd> loss function</li>
      </ul>
    </li>

    <li class="calibre17">If you need to classify data into a large number of categories, you should avoid creating information bottlenecks in your network due to intermediate layers that are too small.</li>
  </ul>

  <h2 class="head" id="ch03lev1sec6"><a class="calibre3" id="ch03lev1sec6__title"></a>3.6. Predicting house prices: a regression example</h2>

  <p class="noind"><a id="iddle1864"></a><a id="iddle1900"></a>The two previous examples were considered classification problems, where the goal was to predict a single discrete label of an input data point. Another common type of machine-learning problem is <i class="calibre5">regression</i>, which consists of predicting a continuous value instead of a discrete label: for instance, predicting the temperature tomorrow, given meteorological data; or predicting the time that a software project will take to complete, given its specifications.</p>
  <hr class="calibre25"/>

  <p class="notetitle" id="ch03note02">Note</p>

  <p class="noindclose">Don’t confuse <i class="calibre30">regression</i> and the algorithm <i class="calibre30">logistic regression</i>. Confusingly, logistic regression isn’t a regression algorithm—it’s a classification algorithm.</p>
  <hr class="calibre25"/>

  <h3 class="head1" id="ch03lev2sec26">3.6.1. <a id="ch03lev2sec26__title"></a>The Boston Housing Price dataset</h3>

  <p class="noind">You’ll attempt to predict the median price of homes in a given Boston suburb in the mid-1970s, given data points about the suburb at the time, such as the crime rate, the local property tax rate, and so on. The dataset you’ll use has an interesting difference from the two previous examples. It has relatively few data points: only 506, split between 404 training samples and 102 test samples. And each <i class="calibre5">feature</i> in the input data (for example, the crime rate) has a different scale. For instance, some values are proportions, which take values between 0 and 1; others take values between 1 and 12, others between 0 and 100, and so on.</p>

  <p class="notetitle" id="ch03ex24">Listing 3.24. <a id="ch03ex24__title"></a>Loading the Boston housing dataset</p>
  <pre class="calibre4" id="PLd0e9156">from keras.datasets import boston_housing

(train_data, train_targets), (test_data, test_targets) =
<span class="cambriamathin">➥</span>  boston_housing.load_data()</pre>

  <p class="noind">Let’s look at the data:</p>
  <pre class="calibre4" id="PLd0e9169">&gt;&gt;&gt; train_data.shape
(404, 13)
&gt;&gt;&gt; test_data.shape
(102, 13)</pre>

  <p class="noind">As you can see, you have 404 training samples and 102 test samples, each with 13 numerical features, such as per capita crime rate, average number of rooms per dwelling, accessibility to highways, and so on.</p>

  <p class="noind">The targets are the median values of owner-occupied homes, in thousands of dollars:</p>
  <pre class="calibre4" id="PLd0e9181">&gt;&gt;&gt; train_targets
[ 15.2,  42.3,  50. ...  19.4,  19.4,  29.1]</pre>

  <p class="noind">The prices are typically between $10,000 and $50,000. If that sounds cheap, remember that this was the mid-1970s, and these prices aren’t adjusted for inflation.</p>

  <h3 class="head1" id="ch03lev2sec27">3.6.2. <a id="ch03lev2sec27__title"></a>Preparing the data</h3>

  <p class="noind"><a id="iddle1584"></a><a id="iddle1646"></a>It would be problematic to feed into a neural network values that all take wildly different ranges. The network might be able to automatically adapt to such heterogeneous data, but it would definitely make learning more difficult. A widespread best practice to deal with such data is to do feature-wise normalization: for each feature in the input data (a column in the input data matrix), you subtract the mean of the feature and divide by the standard deviation, so that the feature is centered around 0 and has a unit standard deviation. This is easily done in Numpy.</p>

  <p class="notetitle" id="ch03ex25">Listing 3.25. <a id="ch03ex25__title"></a>Normalizing the data</p>
  <pre class="calibre4" id="PLd0e9214">mean = train_data.mean(axis=0)
train_data -= mean
std = train_data.std(axis=0)
train_data /= std

test_data -= mean
test_data /= std</pre>

  <p class="noind">Note that the quantities used for normalizing the test data are computed using the training data. You should never use in your workflow any quantity computed on the test data, even for something as simple as data normalization.</p>

  <h3 class="head1" id="ch03lev2sec28">3.6.3. <a id="ch03lev2sec28__title"></a>Building your network</h3>

  <p class="noind">Because so few samples are available, you’ll use a very small network with two hidden layers, each with 64 units. In general, the less training data you have, the worse overfitting will be, and using a small network is one way to mitigate overfitting.</p>

  <p class="notetitle" id="ch03ex26">Listing 3.26. <a id="ch03ex26__title"></a>Model definition</p>
  <pre class="calibre4" id="PLd0e9235">from keras import models
from keras import layers

def build_model():
    model = models.Sequential()                                  <span class="cambriamathin">❶</span>
    model.add(layers.Dense(64, activation='relu',
                           input_shape=(train_data.shape[1],)))
    model.add(layers.Dense(64, activation='relu'))
    model.add(layers.Dense(1))
    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])
    return model</pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> Because you’ll need to instantiate the same model multiple times, you use a function to construct it.</p>
  </div>

  <p class="noind">The network ends with a single unit and no activation (it will be a linear layer). This is a typical setup for scalar regression (a regression where you’re trying to predict a single continuous value). Applying an activation function would constrain the range the output can take; for instance, if you applied a <kbd class="calibre24">sigmoid</kbd> activation function to the last layer, the network could only learn to predict values between 0 and 1. Here, because the last layer is purely linear, the network is free to learn to predict values in any range.</p>

  <p class="noind">Note that you compile the network with the <kbd class="calibre24">mse</kbd> loss function—<i class="calibre5">mean squared error</i>, the square of the difference between the predictions and the targets. This is a widely used loss function for regression problems.</p>

  <p class="noind">You’re also monitoring a new metric during training: <i class="calibre5">mean absolute error</i> (MAE). It’s the absolute value of the difference between the predictions and the targets. For instance, an MAE of 0.5 on this problem would mean your predictions are off by $500 on average.</p>

  <h3 class="head1" id="ch03lev2sec29">3.6.4. <a id="ch03lev2sec29__title"></a>Validating your approach using K-fold validation</h3>

  <p class="noind">To evaluate your network while you keep adjusting its parameters (such as the number of epochs used for training), you could split the data into a training set and a validation set, as you did in the previous examples. But because you have so few data points, the validation set would end up being very small (for instance, about 100 examples). As a consequence, the validation scores might change a lot depending on which data points you chose to use for validation and which you chose for training: the validation scores might have a high <i class="calibre5">variance</i> with regard to the validation split. This would prevent you from reliably evaluating your model.</p>

  <p class="noind">The best practice in such situations is to use <i class="calibre5">K-fold</i> cross-validation (see <a href="#ch03fig11">figure 3.11</a>). It consists of splitting the available data into <i class="calibre5">K</i> partitions (typically <i class="calibre5">K</i> = 4 or 5), instantiating <i class="calibre5">K</i> identical models, and training each one on <i class="calibre5">K</i> – 1 partitions while evaluating on the remaining partition. The validation score for the model used is then the average of the <i class="calibre5">K</i> validation scores obtained. In terms of code, this is straightforward.</p>

  <p class="notetitle" id="ch03fig11">Figure 3.11. <a id="ch03fig11__title"></a>3-fold cross-validation</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/03fig11_altB.jpg"/></p>

  <p class="notetitle" id="ch03ex27">Listing 3.27. <a id="ch03ex27__title"></a>K-fold validation</p>
  <pre class="calibre4" id="PLd0e9332">import numpy as np

k = 4
num_val_samples = len(train_data) // k
num_epochs = 100
all_scores = []
for i in range(k):
    print('processing fold #', i)
    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]    <span class="cambriamathin">❶</span>
    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]

    partial_train_data = np.concatenate(                                     <span class="cambriamathin">❷</span>
        [train_data[:i * num_val_samples],
         train_data[(i + 1) * num_val_samples:]],
        axis=0)
    partial_train_targets = np.concatenate(
        [train_targets[:i * num_val_samples],
         train_targets[(i + 1) * num_val_samples:]],
        axis=0)

    model = build_model()                                                    <span class="cambriamathin">❸</span>
    model.fit(partial_train_data, partial_train_targets,                     <span class="cambriamathin">❹</span>
              epochs=num_epochs, batch_size=1, verbose=0)
    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)      <span class="cambriamathin">❺</span>
    all_scores.append(val_mae)</pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> Prepares the validation data: data from partition #k</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> Prepares the training data: data from all other partitions</p>

    <p class="codeannotation"><span class="cambriamathin1">❸</span> Builds the Keras model (already compiled)</p>

    <p class="codeannotation"><span class="cambriamathin1">❹</span> Trains the model (in silent mode, verbose = 0)</p>

    <p class="codeannotation"><span class="cambriamathin1">❺</span> Evaluates the model on the validation data</p>
  </div>

  <p class="noind">Running this with <kbd class="calibre24">num_epochs = 100</kbd> yields the following results:</p>
  <pre class="calibre4" id="PLd0e9409">&gt;&gt;&gt; all_scores
[2.588258957792037, 3.1289568449719116, 3.1856116051248984, 3.0763342615401386]
&gt;&gt;&gt; np.mean(all_scores)
2.9947904173572462</pre>

  <p class="noind">The different runs do indeed show rather different validation scores, from 2.6 to 3.2. The average (3.0) is a much more reliable metric than any single score—that’s the entire point of K-fold cross-validation. In this case, you’re off by $3,000 on average, which is significant considering that the prices range from $10,000 to $50,000.</p>

  <p class="noind">Let’s try training the network a bit longer: 500 epochs. To keep a record of how well the model does at each epoch, you’ll modify the training loop to save the per-epoch validation score log.</p>

  <p class="notetitle" id="ch03ex28">Listing 3.28. <a id="ch03ex28__title"></a>Saving the validation logs at each fold</p>
  <pre class="calibre4" id="PLd0e9424">num_epochs = 500
all_mae_histories = []
for i in range(k):
    print('processing fold #', i)
    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]    <span class="cambriamathin">❶</span>
    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]
    partial_train_data = np.concatenate(                                     <span class="cambriamathin">❷</span>
        [train_data[:i * num_val_samples],
         train_data[(i + 1) * num_val_samples:]],
        axis=0)
    partial_train_targets = np.concatenate(
        [train_targets[:i * num_val_samples],
         train_targets[(i + 1) * num_val_samples:]],
        axis=0)

    model = build_model()                                                    <span class="cambriamathin">❸</span>
    history = model.fit(partial_train_data, partial_train_targets,           <span class="cambriamathin">❹</span>
                        validation_data=(val_data, val_targets),
                        epochs=num_epochs, batch_size=1, verbose=0)
    mae_history = history.history['val_mean_absolute_error']
    all_mae_histories.append(mae_history)</pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> Prepares the validation data: data from partition #k</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> Prepares the training data: data from all other partitions</p>

    <p class="codeannotation"><span class="cambriamathin1">❸</span> Builds the Keras model (already compiled)</p>

    <p class="codeannotation"><span class="cambriamathin1">❹</span> Trains the model (in silent mode, verbose=0)</p>
  </div>

  <p class="noind">You can then compute the average of the per-epoch MAE scores for all folds.</p>

  <p class="notetitle" id="ch03ex29">Listing 3.29. <a id="ch03ex29__title"></a>Building the history of successive mean K-fold validation scores</p>
  <pre class="calibre4" id="PLd0e9489">average_mae_history = [
    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]</pre>

  <p class="noind">Let’s plot this; see <a href="#ch03fig12">figure 3.12</a>.</p>

  <p class="notetitle" id="ch03fig12">Figure 3.12. <a id="ch03fig12__title"></a>Validation MAE by epoch</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/03fig12.jpg"/></p>

  <p class="notetitle" id="ch03ex30">Listing 3.30. <a id="ch03ex30__title"></a>Plotting validation scores</p>
  <pre class="calibre4" id="PLd0e9519">import matplotlib.pyplot as plt

plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)
plt.xlabel('Epochs')
plt.ylabel('Validation MAE')
plt.show()</pre>

  <p class="noind"><a id="iddle1585"></a><a id="iddle1647"></a>It may be a little difficult to see the plot, due to scaling issues and relatively high variance. Let’s do the following:</p>

  <ul class="calibre16">
    <li class="calibre17">Omit the first 10 data points, which are on a different scale than the rest of the curve.</li>

    <li class="calibre17">Replace each point with an exponential moving average of the previous points, to obtain a smooth curve.</li>
  </ul>

  <p class="noind">The result is shown in <a href="#ch03fig13">figure 3.13</a>.</p>

  <p class="notetitle" id="ch03fig13">Figure 3.13. <a id="ch03fig13__title"></a>Validation MAE by epoch, excluding the first 10 data points</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/03fig13.jpg"/></p>

  <p class="notetitle" id="ch03ex31">Listing 3.31. <a id="ch03ex31__title"></a>Plotting validation scores, excluding the first 10 data points</p>
  <pre class="calibre4" id="PLd0e9577">def smooth_curve(points, factor=0.9):
  smoothed_points = []
  for point in points:
    if smoothed_points:
      previous = smoothed_points[-1]
      smoothed_points.append(previous * factor + point * (1 - factor))
    else:
      smoothed_points.append(point)
  return smoothed_points

smooth_mae_history = smooth_curve(average_mae_history[10:])

plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)
plt.xlabel('Epochs')
plt.ylabel('Validation MAE')
plt.show()</pre>

  <p class="noind">According to this plot, validation MAE stops improving significantly after 80 epochs. Past that point, you start overfitting.</p>

  <p class="noind">Once you’re finished tuning other parameters of the model (in addition to the number of epochs, you could also adjust the size of the hidden layers), you can train a final production model on all of the training data, with the best parameters, and then look at its performance on the test data.</p>

  <p class="notetitle" id="ch03ex32">Listing 3.32. <a id="ch03ex32__title"></a>Training the final model</p>
  <pre class="calibre4" id="PLd0e9594">model = build_model()                                                     <span class="cambriamathin">❶</span>
model.fit(train_data, train_targets,                                      <span class="cambriamathin">❷</span>
          epochs=80, batch_size=16, verbose=0)
test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)</pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> Gets a fresh, compiled model</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> Trains it on the entirety of the data</p>
  </div>

  <p class="noind">Here’s the final result:</p>
  <pre class="calibre4" id="PLd0e9630">&gt;&gt;&gt; test_mae_score
2.5532484335057877</pre>

  <p class="noind">You’re still off by about $2,550.</p>

  <h3 class="head1" id="ch03lev2sec30">3.6.5. <a id="ch03lev2sec30__title"></a>Wrapping up</h3>

  <p class="noind">Here’s what you should take away from this example:</p>

  <ul class="calibre16">
    <li class="calibre17">Regression is done using different loss functions than what we used for classification. Mean squared error (MSE) is a loss function commonly used for regression.</li>

    <li class="calibre17">Similarly, evaluation metrics to be used for regression differ from those used for classification; naturally, the concept of accuracy doesn’t apply for regression. A common regression metric is mean absolute error (MAE).</li>

    <li class="calibre17">When features in the input data have values in different ranges, each feature should be scaled independently as a preprocessing step.</li>

    <li class="calibre17">When there is little data available, using K-fold validation is a great way to reliably evaluate a model.</li>

    <li class="calibre17">When little training data is available, it’s preferable to use a small network with few hidden layers (typically only one or two), in order to avoid severe overfitting.</li>
  </ul>

  <p class="noind"></p>
  <hr class="calibre25"/>

  <div class="calibre14">
    <b class="calibre26" id="ch03sb02">Chapter summary</b>

    <ul class="calibre16">
      <li class="calibre17">You’re now able to handle the most common kinds of machine-learning tasks on vector data: binary classification, multiclass classification, and scalar regression. The “Wrapping up” sections earlier in the chapter summarize the important points you’ve learned regarding these types of tasks.</li>

      <li class="calibre17">You’ll usually need to preprocess raw data before feeding it into a neural network.</li>

      <li class="calibre17">When your data has features with different ranges, scale each feature independently as part of preprocessing.</li>

      <li class="calibre17">As training progresses, neural networks eventually begin to overfit and obtain worse results on never-before-seen data.</li>

      <li class="calibre17">If you don’t have much training data, use a small network with only one or two hidden layers, to avoid severe overfitting.</li>

      <li class="calibre17">If your data is divided into many categories, you may cause information bottlenecks if you make the intermediate layers too small.</li>

      <li class="calibre17">Regression uses different loss functions and different evaluation metrics than classification.</li>

      <li class="calibre17">When you’re working with little data, K-fold validation can help reliably evaluate your model.</li>
    </ul>
  </div>
  <hr class="calibre25"/>

  <div class="calibre14" id="calibre_pb_18"></div>
</body>
</html>