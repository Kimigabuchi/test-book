<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <title>Deep Learning with Python</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link href="../Styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../Styles/page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
  <h1 class="part" id="ch08">Chapter 8. <a class="calibre3" id="ch08__title"></a>Generative deep learning</h1>

  <p class="noind1">This chapter covers</p>

  <ul class="calibre16">
    <li class="calibre17">Text generation with LSTM</li>

    <li class="calibre17">Implementing DeepDream</li>

    <li class="calibre17">Performing neural style transfer</li>

    <li class="calibre17">Variational autoencoders</li>

    <li class="calibre17">Understanding generative adversarial networks</li>
  </ul>

  <p class="noind">The potential of artificial intelligence to emulate human thought processes goes beyond passive tasks such as object recognition and mostly reactive tasks such as driving a car. It extends well into creative activities. When I first made the claim that in a not-so-distant future, most of the cultural content that we consume will be created with substantial help from AIs, I was met with utter disbelief, even from long-time machine-learning practitioners. That was in 2014. Fast-forward three years, and the disbelief has receded—at an incredible speed. In the summer of 2015, we were entertained by Google’s DeepDream algorithm turning an image into a psychedelic mess of dog eyes and pareidolic artifacts; in 2016, we used the Prisma application to turn photos into paintings of various styles. In the summer of 2016, an experimental short movie, <i class="calibre5">Sunspring</i>, was directed using a script written by a Long Short-Term Memory (LSTM) algorithm—complete with dialogue. Maybe you’ve recently listened to music that was tentatively generated by a neural network.</p>

  <p class="noind"><a id="iddle1034"></a><a id="iddle1040"></a><a id="iddle1497"></a>Granted, the artistic productions we’ve seen from AI so far have been fairly low quality. AI isn’t anywhere close to rivaling human screenwriters, painters, and composers. But replacing humans was always beside the point: artificial intelligence isn’t about replacing our own intelligence with something else, it’s about bringing into our lives and work <i class="calibre5">more</i> intelligence—intelligence of a different kind. In many fields, but especially in creative ones, AI will be used by humans as a tool to augment their own capabilities: more <i class="calibre5">augmented</i> intelligence than <i class="calibre5">artificial</i> intelligence.</p>

  <p class="noind">A large part of artistic creation consists of simple pattern recognition and technical skill. And that’s precisely the part of the process that many find less attractive or even dispensable. That’s where AI comes in. Our perceptual modalities, our language, and our artwork all have statistical structure. Learning this structure is what deep-learning algorithms excel at. Machine-learning models can learn the statistical <i class="calibre5">latent space</i> of images, music, and stories, and they can then <i class="calibre5">sample</i> from this space, creating new artworks with characteristics similar to those the model has seen in its training data. Naturally, such sampling is hardly an act of artistic creation in itself. It’s a mere mathematical operation: the algorithm has no grounding in human life, human emotions, or our experience of the world; instead, it learns from an experience that has little in common with ours. It’s only our interpretation, as human spectators, that will give meaning to what the model generates. But in the hands of a skilled artist, algorithmic generation can be steered to become meaningful—and beautiful. Latent space sampling can become a brush that empowers the artist, augments our creative affordances, and expands the space of what we can imagine. What’s more, it can make artistic creation more accessible by eliminating the need for technical skill and practice—setting up a new medium of pure expression, factoring art apart from craft.</p>

  <p class="noind">Iannis Xenakis, a visionary pioneer of electronic and algorithmic music, beautifully expressed this same idea in the 1960s, in the context of the application of automation technology to music composition:<sup class="calibre19">[<a href="#ch08fn01" class="calibre13">1</a>]</sup></p>

  <blockquote class="smaller">
    <p class="calibre20"><sup class="calibre19"><a id="ch08fn01" class="calibre13">1</a></sup></p>

    <div class="calibre21">
      Iannis Xenakis, “Musiques formelles: nouveaux principes formels de composition musicale,” special issue of <i class="calibre5">La Revue musicale</i>, nos. 253 -254 (1963).
    </div>
  </blockquote>

  <blockquote class="calibre15">
    <p class="noind1">Freed from tedious calculations, the composer is able to devote himself to the general problems that the new musical form poses and to explore the nooks and crannies of this form while modifying the values of the input data. For example, he may test all instrumental combinations from soloists to chamber orchestras, to large orchestras. With the aid of electronic computers the composer becomes a sort of pilot: he presses the buttons, introduces coordinates, and supervises the controls of a cosmic vessel sailing in the space of sound, across sonic constellations and galaxies that he could formerly glimpse only as a distant dream.</p>
  </blockquote>

  <p class="noind">In this chapter, we’ll explore from various angles the potential of deep learning to augment artistic creation. We’ll review sequence data generation (which can be used to generate text or music), DeepDream, and image generation using both variational autoencoders and generative adversarial networks. We’ll get your computer to dream up content never seen before; and maybe we’ll get you to dream, too, about the fantastic possibilities that lie at the intersection of technology and art. Let’s get started.</p>

  <h2 class="head" id="ch08lev1sec1"><a class="calibre3" id="ch08lev1sec1__title"></a>8.1. Text generation with LSTM</h2>

  <p class="noind"><a id="iddle1037"></a><a id="iddle1255"></a><a id="iddle1321"></a><a id="iddle1323"></a><a id="iddle1337"></a><a id="iddle1365"></a><a id="iddle1537"></a><a id="iddle1539"></a><a id="iddle1825"></a><a id="iddle1908"></a><a id="iddle1975"></a><a id="iddle1977"></a>In this section, we’ll explore how recurrent neural networks can be used to generate sequence data. We’ll use text generation as an example, but the exact same techniques can be generalized to any kind of sequence data: you could apply it to sequences of musical notes in order to generate new music, to timeseries of brushstroke data (for example, recorded while an artist paints on an iPad) to generate paintings stroke by stroke, and so on.</p>

  <p class="noind">Sequence data generation is in no way limited to artistic content generation. It has been successfully applied to speech synthesis and to dialogue generation for chatbots. The Smart Reply feature that Google released in 2016, capable of automatically generating a selection of quick replies to emails or text messages, is powered by similar techniques.</p>

  <h3 class="head1" id="ch08lev2sec1">8.1.1. <a id="ch08lev2sec1__title"></a>A brief history of generative recurrent networks</h3>

  <p class="noind">In late 2014, few people had ever seen the initials LSTM, even in the machine-learning community. Successful applications of sequence data generation with recurrent networks only began to appear in the mainstream in 2016. But these techniques have a fairly long history, starting with the development of the LSTM algorithm in 1997.<sup class="calibre19">[<a href="#ch08fn02" class="calibre13">2</a>]</sup> This new algorithm was used early on to generate text character by character.</p>

  <blockquote class="smaller">
    <p class="calibre20"><sup class="calibre19"><a id="ch08fn02" class="calibre13">2</a></sup></p>

    <div class="calibre21">
      Sepp Hochreiter and Jürgen Schmidhuber, “Long Short-Term Memory,” <i class="calibre5">Neural Computation</i> 9, no. 8 (1997).
    </div>
  </blockquote>

  <p class="noind">In 2002, Douglas Eck, then at Schmidhuber’s lab in Switzerland, applied LSTM to music generation for the first time, with promising results. Eck is now a researcher at Google Brain, and in 2016 he started a new research group there, called Magenta, focused on applying modern deep-learning techniques to produce engaging music. Sometimes, good ideas take 15 years to get started.</p>

  <p class="noind">In the late 2000s and early 2010s, Alex Graves did important pioneering work on using recurrent networks for sequence data generation. In particular, his 2013 work on applying recurrent mixture density networks to generate human-like handwriting using timeseries of pen positions is seen by some as a turning point.<sup class="calibre19">[<a href="#ch08fn03" class="calibre13">3</a>]</sup> This specific application of neural networks at that specific moment in time captured for me the notion of <i class="calibre5">machines that dream</i> and was a significant inspiration around the time I started developing Keras. Graves left a similar commented-out remark hidden in a 2013 LaTeX file uploaded to the preprint server arXiv: “generating sequential data is the closest computers get to dreaming.” Several years later, we take a lot of these developments for granted; but at the time, it was difficult to watch Graves’s demonstrations and not walk away awe-inspired by the possibilities.</p>

  <blockquote class="smaller">
    <p class="calibre20"><sup class="calibre19"><a id="ch08fn03" class="calibre13">3</a></sup></p>

    <div class="calibre21">
      Alex Graves, “Generating Sequences With Recurrent Neural Networks,” arXiv (2013), <a href="https://arxiv.org/abs/1308.0850">https://arxiv.org/abs/1308.0850</a>.
    </div>
  </blockquote>

  <p class="noind">Since then, recurrent neural networks have been successfully used for music generation, dialogue generation, image generation, speech synthesis, and molecule design. They were even used to produce a movie script that was then cast with live actors.</p>

  <h3 class="head1" id="ch08lev2sec2">8.1.2. <a id="ch08lev2sec2__title"></a>How do you generate sequence data?</h3>

  <p class="noind"><a id="iddle1089"></a><a id="iddle1102"></a><a id="iddle1151"></a><a id="iddle1322"></a><a id="iddle1325"></a><a id="iddle1366"></a><a id="iddle1538"></a><a id="iddle1541"></a><a id="iddle1813"></a><a id="iddle1862"></a><a id="iddle1878"></a><a id="iddle1924"></a><a id="iddle1925"></a><a id="iddle1976"></a><a id="iddle1979"></a>The universal way to generate sequence data in deep learning is to train a network (usually an RNN or a convnet) to predict the next token or next few tokens in a sequence, using the previous tokens as input. For instance, given the input “the cat is on the ma,” the network is trained to predict the target <i class="calibre5">t</i>, the next character. As usual when working with text data, <i class="calibre5">tokens</i> are typically words or characters, and any network that can model the probability of the next token given the previous ones is called a <i class="calibre5">language model</i>. A language model captures the <i class="calibre5">latent space</i> of language: its statistical structure.</p>

  <p class="noind">Once you have such a trained language model, you can <i class="calibre5">sample</i> from it (generate new sequences): you feed it an initial string of text (called <i class="calibre5">conditioning data</i>), ask it to generate the next character or the next word (you can even generate several tokens at once), add the generated output back to the input data, and repeat the process many times (see <a href="#ch08fig01">figure 8.1</a>). This loop allows you to generate sequences of arbitrary length that reflect the structure of the data on which the model was trained: sequences that look <i class="calibre5">almost</i> like human-written sentences. In the example we present in this section, you’ll take a LSTM layer, feed it strings of <i class="calibre5">N</i> characters extracted from a text corpus, and train it to predict character <i class="calibre5">N</i> + 1. The output of the model will be a softmax over all possible characters: a probability distribution for the next character. This LSTM is called a <i class="calibre5">character-level neural language model</i>.</p>

  <p class="notetitle" id="ch08fig01">Figure 8.1. <a id="ch08fig01__title"></a>The process of character-by-character text generation using a language model</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/08fig01.jpg"/></p>

  <h3 class="head1" id="ch08lev2sec3">8.1.3. <a id="ch08lev2sec3__title"></a>The importance of the sampling strategy</h3>

  <p class="noind">When generating text, the way you choose the next character is crucially important. A naive approach is <i class="calibre5">greedy sampling</i>, consisting of always choosing the most likely next character. But such an approach results in repetitive, predictable strings that don’t look like coherent language. A more interesting approach makes slightly more surprising choices: it introduces randomness in the sampling process, by sampling from the probability distribution for the next character. This is called <i class="calibre5">stochastic sampling</i> (recall that <i class="calibre5">stochasticity</i> is what we call <i class="calibre5">randomness</i> in this field). In such a setup, if <i class="calibre5">e</i> has a probability 0.3 of being the next character, according to the model, you’ll choose it <a id="iddle1913"></a>30% of the time. Note that greedy sampling can be also cast as sampling from a probability distribution: one where a certain character has probability 1 and all others have probability 0.</p>

  <p class="noind">Sampling probabilistically from the softmax output of the model is neat: it allows even unlikely characters to be sampled some of the time, generating more interesting-looking sentences and sometimes showing creativity by coming up with new, realistic-sounding words that didn’t occur in the training data. But there’s one issue with this strategy: it doesn’t offer a way to <i class="calibre5">control the amount of randomness</i> in the sampling process.</p>

  <p class="noind">Why would you want more or less randomness? Consider an extreme case: pure random sampling, where you draw the next character from a uniform probability distribution, and every character is equally likely. This scheme has maximum randomness; in other words, this probability distribution has maximum entropy. Naturally, it won’t produce anything interesting. At the other extreme, greedy sampling doesn’t produce anything interesting, either, and has no randomness: the corresponding probability distribution has minimum entropy. Sampling from the “real” probability distribution—the distribution that is output by the model’s softmax function—constitutes an intermediate point between these two extremes. But there are many other intermediate points of higher or lower entropy that you may want to explore. Less entropy will give the generated sequences a more predictable structure (and thus they will potentially be more realistic looking), whereas more entropy will result in more surprising and creative sequences. When sampling from generative models, it’s always good to explore different amounts of randomness in the generation process. Because we—humans—are the ultimate judges of how interesting the generated data is, interestingness is highly subjective, and there’s no telling in advance where the point of optimal entropy lies.</p>

  <p class="noind">In order to control the amount of stochasticity in the sampling process, we’ll introduce a parameter called the <i class="calibre5">softmax temperature</i> that characterizes the entropy of the probability distribution used for sampling: it characterizes how surprising or predictable the choice of the next character will be. Given a <kbd class="calibre24">temperature</kbd> value, a new probability distribution is computed from the original one (the softmax output of the model) by reweighting it in the following way.</p>

  <p class="notetitle" id="ch08ex01">Listing 8.1. <a id="ch08ex01__title"></a>Reweighting a probability distribution to a different temperature</p>
  <pre class="calibre4" id="PLd0e26354">import numpy as np

def reweight_distribution(original_distribution, temperature=0.5):    <span class="cambriamathin">❶</span>
    distribution = np.log(original_distribution) / temperature
    distribution = np.exp(distribution)
    return distribution / np.sum(distribution)                        <span class="cambriamathin">❷</span></pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> original_distribution is a 1D Numpy array of probability values that must sum to 1. temperature is a factor quantifying the entropy of the output distribution.</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> Returns a reweighted version of the original distribution. The sum of the distribution may no longer be 1, so you divide it by its sum to obtain the new distribution.</p>
  </div>

  <p class="noind"><a id="iddle1157"></a><a id="iddle1324"></a><a id="iddle1540"></a><a id="iddle1978"></a>Higher temperatures result in sampling distributions of higher entropy that will generate more surprising and unstructured generated data, whereas a lower temperature will result in less randomness and much more predictable generated data (see <a href="#ch08fig02">figure 8.2</a>).</p>

  <p class="notetitle" id="ch08fig02">Figure 8.2. <a id="ch08fig02__title"></a>Different reweightings of one probability distribution. Low temperature = more deterministic, high temperature = more random.</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/08fig02_alt.jpg"/></p>

  <h3 class="head1" id="ch08lev2sec4">8.1.4. <a id="ch08lev2sec4__title"></a>Implementing character-level LSTM text generation</h3>

  <p class="noind">Let’s put these ideas into practice in a Keras implementation. The first thing you need is a lot of text data that you can use to learn a language model. You can use any sufficiently large text file or set of text files—Wikipedia, <i class="calibre5">The Lord of the Rings</i>, and so on. In this example, you’ll use some of the writings of Nietzsche, the late-nineteenth century German philosopher (translated into English). The language model you’ll learn will thus be specifically a model of Nietzsche’s writing style and topics of choice, rather than a more generic model of the English language.</p>

  <p class="notetitle" id="ch08lev3sec1"><a id="ch08lev3sec1__title"></a>Preparing the data</p>

  <p class="noind">Let’s start by downloading the corpus and converting it to lowercase.</p>

  <p class="notetitle" id="ch08ex02">Listing 8.2. <a id="ch08ex02__title"></a>Downloading and parsing the initial text file</p>
  <pre class="calibre4" id="PLd0e26468">import keras
import numpy as np

path = keras.utils.get_file(
    'nietzsche.txt',
    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')
text = open(path).read().lower()
print('Corpus length:', len(text))</pre>

  <p class="noind">Next, you’ll extract partially overlapping sequences of length <kbd class="calibre24">maxlen</kbd>, one-hot encode them, and pack them in a 3D Numpy array <kbd class="calibre24">x</kbd> of shape <kbd class="calibre24">(sequences, maxlen, unique_characters)</kbd>. Simultaneously, you’ll prepare an array <kbd class="calibre24">y</kbd> containing the corresponding targets: the one-hot-encoded characters that come after each extracted sequence.</p>

  <p class="notetitle" id="ch08ex03">Listing 8.3. <a id="ch08ex03__title"></a>Vectorizing sequences of characters</p>
  <pre class="calibre4" id="PLd0e26495">maxlen = 60                                                            <span class="cambriamathin">❶</span>
step = 3                                                               <span class="cambriamathin">❷</span>

sentences = []                                                         <span class="cambriamathin">❸</span>

next_chars = []                                                        <span class="cambriamathin">❹</span>

for i in range(0, len(text) - maxlen, step):
    sentences.append(text[i: i + maxlen])
    next_chars.append(text[i + maxlen])

print('Number of sequences:', len(sentences))

chars = sorted(list(set(text)))                                        <span class="cambriamathin">❺</span>
print('Unique characters:', len(chars))
char_indices = dict((char, chars.index(char)) for char in chars)       <span class="cambriamathin">❻</span>

print('Vectorization...')
x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)      <span class="cambriamathin">❼</span>
y = np.zeros((len(sentences), len(chars)), dtype=np.bool)              <span class="cambriamathin">❼</span>
for i, sentence in enumerate(sentences):                               <span class="cambriamathin">❼</span>
    for t, char in enumerate(sentence):                                <span class="cambriamathin">❼</span>
        x[i, t, char_indices[char]] = 1                                <span class="cambriamathin">❼</span>
    y[i, char_indices[next_chars[i]]] = 1                              <span class="cambriamathin">❼</span></pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> You’ll extract sequences of 60 characters.</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> You’ll sample a new sequence every three characters.</p>

    <p class="codeannotation"><span class="cambriamathin1">❸</span> Holds the extracted sequences</p>

    <p class="codeannotation"><span class="cambriamathin1">❹</span> Holds the targets (the follow-up characters)</p>

    <p class="codeannotation"><span class="cambriamathin1">❺</span> List of unique characters in the corpus</p>

    <p class="codeannotation"><span class="cambriamathin1">❻</span> Dictionary that maps unique characters to their index in the list “chars”</p>

    <p class="codeannotation"><span class="cambriamathin1">❼</span> One-hot encodes the characters into binary arrays</p>
  </div>

  <p class="notetitle" id="ch08lev3sec2"><a id="ch08lev3sec2__title"></a>Building the network</p>

  <p class="noind">This network is a single <kbd class="calibre24">LSTM</kbd> layer followed by a <kbd class="calibre24">Dense</kbd> classifier and softmax over all possible characters. But note that recurrent neural networks aren’t the only way to do sequence data generation; 1D convnets also have proven extremely successful at this task in recent times.</p>

  <p class="notetitle" id="ch08ex04">Listing 8.4. <a id="ch08ex04__title"></a>Single-layer LSTM model for next-character prediction</p>
  <pre class="calibre4" id="PLd0e26625">from keras import layers

model = keras.models.Sequential()
model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))
model.add(layers.Dense(len(chars), activation='softmax'))</pre>

  <p class="noind"><a id="iddle1493"></a><a id="iddle1494"></a><a id="iddle1630"></a><a id="iddle1631"></a><a id="iddle1860"></a><a id="iddle2000"></a>Because your targets are one-hot encoded, you’ll use <kbd class="calibre24">categorical_crossentropy</kbd> as the loss to train the model.</p>

  <p class="notetitle" id="ch08ex05">Listing 8.5. <a id="ch08ex05__title"></a>Model compilation configuration</p>
  <pre class="calibre4" id="PLd0e26696">optimizer = keras.optimizers.RMSprop(lr=0.01)
model.compile(loss='categorical_crossentropy', optimizer=optimizer)</pre>

  <p class="notetitle" id="ch08lev3sec3"><a id="ch08lev3sec3__title"></a>Training the language model and sampling from it</p>

  <p class="noind">Given a trained model and a seed text snippet, you can generate new text by doing the following repeatedly:</p>

  <ol class="calibre23">
    <li class="calibre17">Draw from the model a probability distribution for the next character, given the generated text available so far.</li>

    <li class="calibre17">Reweight the distribution to a certain temperature.</li>

    <li class="calibre17">Sample the next character at random according to the reweighted distribution.</li>

    <li class="calibre17">Add the new character at the end of the available text.</li>
  </ol>

  <p class="noind">This is the code you use to reweight the original probability distribution coming out of the model and draw a character index from it (the <i class="calibre5">sampling function</i>).</p>

  <p class="notetitle" id="ch08ex06">Listing 8.6. <a id="ch08ex06__title"></a>Function to sample the next character given the model’s predictions</p>
  <pre class="calibre4" id="PLd0e26743">def sample(preds, temperature=1.0):
    preds = np.asarray(preds).astype('float64')
    preds = np.log(preds) / temperature
    exp_preds = np.exp(preds)
    preds = exp_preds / np.sum(exp_preds)
    probas = np.random.multinomial(1, preds, 1)
    return np.argmax(probas)</pre>

  <p class="noind">Finally, the following loop repeatedly trains and generates text. You begin generating text using a range of different temperatures after every epoch. This allows you to see how the generated text evolves as the model begins to converge, as well as the impact of temperature in the sampling strategy.</p>

  <p class="notetitle" id="ch08ex07">Listing 8.7. <a id="ch08ex07__title"></a>Text-generation loop</p>
  <pre class="calibre4" id="PLd0e26755">import random
import sys

for epoch in range(1, 60):                                          <span class="cambriamathin">❶</span>
    print('epoch', epoch)
    model.fit(x, y, batch_size=128, epochs=1)                       <span class="cambriamathin">❷</span>
    start_index = random.randint(0, len(text) - maxlen - 1)         <span class="cambriamathin">❸</span>
    generated_text = text[start_index: start_index + maxlen]        <span class="cambriamathin">❸</span>
    print('--- Generating with seed: "' + generated_text + '"')     <span class="cambriamathin">❸</span>
    for temperature in [0.2, 0.5, 1.0, 1.2]:                        <span class="cambriamathin">❹</span>
        print('------ temperature:', temperature)
        sys.stdout.write(generated_text)

        for i in range(400):                                        <span class="cambriamathin">❺</span>
            sampled = np.zeros((1, maxlen, len(chars)))             <span class="cambriamathin">❻</span>
            for t, char in enumerate(generated_text):               <span class="cambriamathin">❻</span>
                sampled[0, t, char_indices[char]] = 1.              <span class="cambriamathin">❻</span>

            preds = model.predict(sampled, verbose=0)[0]            <span class="cambriamathin">❼</span>
            next_index = sample(preds, temperature)                 <span class="cambriamathin">❼</span>
            next_char = chars[next_index]                           <span class="cambriamathin">❼</span>

            generated_text += next_char
            generated_text = generated_text[1:]

            sys.stdout.write(next_char)</pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> Trains the model for 60 epochs</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> Fits the model for one iteration on the data</p>

    <p class="codeannotation"><span class="cambriamathin1">❸</span> Selects a text seed at random</p>

    <p class="codeannotation"><span class="cambriamathin1">❹</span> Tries a range of different sampling temperatures</p>

    <p class="codeannotation"><span class="cambriamathin1">❺</span> Generates 400 characters, starting from the seed text</p>

    <p class="codeannotation"><span class="cambriamathin1">❻</span> One-hot encodes the characters generated so far</p>

    <p class="codeannotation"><span class="cambriamathin1">❼</span> Samples the next character</p>
  </div>

  <p class="noind">Here, we used the random seed text “new faculty, and the jubilation reached its climax when kant.” Here’s what you get at epoch 20, long before the model has fully converged, with <kbd class="calibre24">temperature=0.2</kbd>:</p>
  <pre class="calibre4" id="PLd0e26880">new faculty, and the jubilation reached its climax when kant and such a man
in the same time the spirit of the surely and the such the such
as a man is the sunligh and subject the present to the superiority of the
special pain the most man and strange the subjection of the
special conscience the special and nature and such men the subjection of the
special men, the most surely the subjection of the special
intellect of the subjection of the same things and</pre>

  <p class="noind">Here’s the result with <kbd class="calibre24">temperature=0.5</kbd>:</p>
  <pre class="calibre4" id="PLd0e26892">new faculty, and the jubilation reached its climax when kant in the eterned
and such man as it's also become himself the condition of the
experience of off the basis the superiory and the special morty of the
strength, in the langus, as which the same time life and "even who
discless the mankind, with a subject and fact all you have to be the stand
and lave no comes a troveration of the man and surely the
conscience the superiority, and when one must be w</pre>

  <p class="noind">And here’s what you get with <kbd class="calibre24">temperature=1.0</kbd>:</p>
  <pre class="calibre4" id="PLd0e26904">new faculty, and the jubilation reached its climax when kant, as a
periliting of manner to all definites and transpects it it so
hicable and ont him artiar resull
too such as if ever the proping to makes as cnecience. to been juden,
all every could coldiciousnike hother aw passife, the plies like
which might thiod was account, indifferent germin, that everythery
certain destrution, intellect into the deteriorablen origin of moralian,
and a lessority o</pre>

  <p class="noind">At epoch 60, the model has mostly converged, and the text starts to look significantly more coherent. Here’s the result with <kbd class="calibre24">temperature=0.2</kbd>:</p>
  <pre class="calibre4" id="PLd0e26918">cheerfulness, friendliness and kindness of a heart are the sense of the
spirit is a man with the sense of the sense of the world of the
self-end and self-concerning the subjection of the strengthorixes--the
subjection of the subjection of the subjection of the
self-concerning the feelings in the superiority in the subjection of the
subjection of the spirit isn't to be a man of the sense of the
subjection and said to the strength of the sense of the</pre>

  <p class="noind">Here’s <kbd class="calibre24">temperature=0.5</kbd>:</p>
  <pre class="calibre4" id="PLd0e26930">cheerfulness, friendliness and kindness of a heart are the part of the soul
who have been the art of the philosophers, and which the one
won't say, which is it the higher the and with religion of the frences.
the life of the spirit among the most continuess of the
strengther of the sense the conscience of men of precisely before enough
presumption, and can mankind, and something the conceptions, the
subjection of the sense and suffering and the</pre>

  <p class="noind">And here’s <kbd class="calibre24">temperature=1.0</kbd>:</p>
  <pre class="calibre4" id="PLd0e26942">cheerfulness, friendliness and kindness of a heart are spiritual by the
ciuture for the
entalled is, he astraged, or errors to our you idstood--and it needs,
to think by spars to whole the amvives of the newoatly, prefectly
raals! it was
name, for example but voludd atu-especity"--or rank onee, or even all
"solett increessic of the world and
implussional tragedy experience, transf, or insiderar,--must hast
if desires of the strubction is be stronges</pre>

  <p class="noind">As you can see, a low temperature value results in extremely repetitive and predictable text, but local structure is highly realistic: in particular, all words (a <i class="calibre5">word</i> being a local pattern of characters) are real English words. With higher temperatures, the generated text becomes more interesting, surprising, even creative; it sometimes invents completely new words that sound somewhat plausible (such as <i class="calibre5">eterned</i> and <i class="calibre5">troveration</i>). With a high temperature, the local structure starts to break down, and most words look like semi-random strings of characters. Without a doubt, 0.5 is the most interesting temperature for text generation in this specific setup. Always experiment with multiple sampling strategies! A clever balance between learned structure and randomness is what makes generation interesting.</p>

  <p class="noind">Note that by training a bigger model, longer, on more data, you can achieve generated samples that look much more coherent and realistic than this one. But, of course, don’t expect to ever generate any meaningful text, other than by random chance: all you’re doing is sampling data from a statistical model of which characters come after which characters. Language is a communication channel, and there’s a distinction between what communications are about and the statistical structure of the messages in which communications are encoded. To evidence this distinction, here’s a thought experiment: what if human language did a better job of compressing communications, much like computers do with most digital communications? Language would be no less meaningful, but it would lack any intrinsic statistical structure, thus making it impossible to learn a language model as you just did.</p>

  <h3 class="head1" id="ch08lev2sec5">8.1.5. <a id="ch08lev2sec5__title"></a>Wrapping up</h3>

  <ul class="calibre16">
    <li class="calibre17">You can generate discrete sequence data by training a model to predict the next token(s), given previous tokens.</li>

    <li class="calibre17">In the case of text, such a model is called a <i class="calibre5">language model</i>. It can be based on either words or characters.</li>

    <li class="calibre17">Sampling the next token requires balance between adhering to what the model judges likely, and introducing randomness.</li>

    <li class="calibre17">One way to handle this is the notion of softmax temperature. Always experiment with different temperatures to find the right one.</li>
  </ul>

  <h2 class="head" id="ch08lev1sec2"><a class="calibre3" id="ch08lev1sec2__title"></a>8.2. DeepDream</h2>

  <p class="noind"><a id="iddle1220"></a><a id="iddle1221"></a><a id="iddle1336"></a><a id="iddle2016"></a><i class="calibre5">DeepDream</i> is an artistic image-modification technique that uses the representations learned by convolutional neural networks. It was first released by Google in the summer of 2015, as an implementation written using the Caffe deep-learning library (this was several months before the first public release of TensorFlow).<sup class="calibre19">[<a href="#ch08fn04" class="calibre13">4</a>]</sup> It quickly became an internet sensation thanks to the trippy pictures it could generate (see, for example, <a href="#ch08fig03">figure 8.3</a>), full of algorithmic pareidolia artifacts, bird feathers, and dog eyes—a byproduct of the fact that the DeepDream convnet was trained on ImageNet, where dog breeds and bird species are vastly overrepresented.</p>

  <blockquote class="smaller">
    <p class="calibre20"><sup class="calibre19"><a id="ch08fn04" class="calibre13">4</a></sup></p>

    <div class="calibre21">
      Alexander Mordvintsev, Christopher Olah, and Mike Tyka, “DeepDream: A Code Example for Visualizing Neural Networks,” <i class="calibre5">Google Research Blog</i>, July 1, 2015, <a href="http://mng.bz/xXlM">http://mng.bz/xXlM</a>.
    </div>
  </blockquote>

  <p class="notetitle" id="ch08fig03">Figure 8.3. <a id="ch08fig03__title"></a>Example of a DeepDream output image</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/08fig03.jpg"/></p>

  <p class="noind">The DeepDream algorithm is almost identical to the convnet filter-visualization technique introduced in <a href="../Text/05.html#ch05">chapter 5</a>, consisting of running a convnet in reverse: doing gradient ascent on the input to the convnet in order to maximize the activation of a specific filter in an upper layer of the convnet. DeepDream uses this same idea, with a few simple differences:</p>

  <ul class="calibre16">
    <li class="calibre17">With DeepDream, you try to maximize the activation of entire layers rather than that of a specific filter, thus mixing together visualizations of large numbers of features at once.</li>

    <li class="calibre17"><a id="iddle1222"></a><a id="iddle1409"></a><a id="iddle1419"></a><a id="iddle1462"></a><a id="iddle1737"></a>You start not from blank, slightly noisy input, but rather from an existing image—thus the resulting effects latch on to preexisting visual patterns, distorting elements of the image in a somewhat artistic fashion.</li>

    <li class="calibre17">The input images are processed at different scales (called <i class="calibre5">octaves</i>), which improves the quality of the visualizations.</li>
  </ul>

  <p class="noind">Let’s make some DeepDreams.</p>

  <h3 class="head1" id="ch08lev2sec6">8.2.1. <a id="ch08lev2sec6__title"></a>Implementing DeepDream in Keras</h3>

  <p class="noind">You’ll start from a convnet pretrained on ImageNet. In Keras, many such convnets are available: VGG16, VGG19, Xception, ResNet50, and so on. You can implement DeepDream with any of them, but your convnet of choice will naturally affect your visualizations, because different convnet architectures result in different learned features. The convnet used in the original DeepDream release was an Inception model, and in practice Inception is known to produce nice-looking DeepDreams, so you’ll use the Inception V3 model that comes with Keras.</p>

  <p class="notetitle" id="ch08ex08">Listing 8.8. <a id="ch08ex08__title"></a>Loading the pretrained Inception V3 model</p>
  <pre class="calibre4" id="PLd0e27134">from keras.applications import inception_v3
from keras import backend as K

K.set_learning_phase(0)                                 <span class="cambriamathin">❶</span>

model = inception_v3.InceptionV3(weights='imagenet',    <span class="cambriamathin">❷</span>
                                 include_top=False)     <span class="cambriamathin">❷</span></pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> You won’t be training the model, so this command disables all training-specific operations.</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> Builds the Inception V3 network, without its convolutional base. The model will be loaded with pretrained ImageNet weights.</p>
  </div>

  <p class="noind">Next, you’ll compute the <i class="calibre5">loss</i>: the quantity you’ll seek to maximize during the gradient-ascent process. In <a href="../Text/05.html#ch05">chapter 5</a>, for filter visualization, you tried to maximize the value of a specific filter in a specific layer. Here, you’ll simultaneously maximize the activation of all filters in a number of layers. Specifically, you’ll maximize a weighted sum of the L2 norm of the activations of a set of high-level layers. The exact set of layers you choose (as well as their contribution to the final loss) has a major influence on the visuals you’ll be able to produce, so you want to make these parameters easily configurable. Lower layers result in geometric patterns, whereas higher layers result in visuals in which you can recognize some classes from ImageNet (for example, birds or dogs). You’ll start from a somewhat arbitrary configuration involving four layers—but you’ll definitely want to explore many different configurations later.</p>

  <p class="notetitle" id="ch08ex09">Listing 8.9. <a id="ch08ex09__title"></a>Setting up the DeepDream configuration</p>
  <pre class="calibre4" id="PLd0e27182">layer_contributions = {          <span class="cambriamathin">❶</span>
    'mixed2': 0.2,
    'mixed3': 3.,
    'mixed4': 2.,
    'mixed5': 1.5,
}</pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> Dictionary mapping layer names to a coefficient quantifying how much the layer’s activation contributes to the loss you’ll seek to maximize. Note that the layer names are hardcoded in the built-in Inception V3 application. You can list all layer names using model.summary().</p>
  </div>

  <p class="noind">Now, let’s define a tensor that contains the loss: the weighted sum of the L2 norm of the activations of the layers in <a href="#ch08ex09">listing 8.9</a>.</p>

  <p class="notetitle" id="ch08ex10">Listing 8.10. <a id="ch08ex10__title"></a>Defining the loss to be maximized</p>
  <pre class="calibre4" id="PLd0e27214">layer_dict = dict([(layer.name, layer) for layer in model.layers])            <span class="cambriamathin">❶</span>

loss = K.variable(0.)                                                         <span class="cambriamathin">❷</span>
for layer_name in layer_contributions:
    coeff = layer_contributions[layer_name]
    activation = layer_dict[layer_name].output                                <span class="cambriamathin">❸</span>

    scaling = K.prod(K.cast(K.shape(activation), 'float32'))
    loss += coeff * K.sum(K.square(activation[:, 2: -2, 2: -2, :])) / scaling <span class="cambriamathin">❹</span></pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> Creates a dictionary that maps layer names to layer instances</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> You’ll define the loss by adding layer contributions to this scalar variable.</p>

    <p class="codeannotation"><span class="cambriamathin1">❸</span> Retrieves the layer’s output</p>

    <p class="codeannotation"><span class="cambriamathin1">❹</span> Adds the L2 norm of the features of a layer to the loss. You avoid border artifacts by only involving nonborder pixels in the loss.</p>
  </div>

  <p class="noind">Next, you can set up the gradient-ascent process.</p>

  <p class="notetitle" id="ch08ex11">Listing 8.11. <a id="ch08ex11__title"></a>Gradient-ascent process</p>
  <pre class="calibre4" id="PLd0e27276">dream = model.input                                               <span class="cambriamathin">❶</span>

grads = K.gradients(loss, dream)[0]                               <span class="cambriamathin">❷</span>

grads /= K.maximum(K.mean(K.abs(grads)), 1e-7)                    <span class="cambriamathin">❸</span>

outputs = [loss, grads]                                           <span class="cambriamathin">❹</span>
fetch_loss_and_grads = K.function([dream], outputs)               <span class="cambriamathin">❹</span>

def eval_loss_and_grads(x):
    outs = fetch_loss_and_grads([x])
    loss_value = outs[0]
    grad_values = outs[1]
    return loss_value, grad_values

def gradient_ascent(x, iterations, step, max_loss=None):          <span class="cambriamathin">❺</span>
    for i in range(iterations):                                   <span class="cambriamathin">❺</span>
        loss_value, grad_values = eval_loss_and_grads(x)          <span class="cambriamathin">❺</span>
        if max_loss is not None and loss_value &gt; max_loss:        <span class="cambriamathin">❺</span>
            break                                                 <span class="cambriamathin">❺</span>
        print('...Loss value at', i, ':', loss_value)             <span class="cambriamathin">❺</span>
        x += step * grad_values                                   <span class="cambriamathin">❺</span>
    return x                                                      <span class="cambriamathin">❺</span></pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> This tensor holds the generated image: the dream.</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> Computes the gradients of the dream with regard to the loss</p>

    <p class="codeannotation"><span class="cambriamathin1">❸</span> Normalizes the gradients (important trick)</p>

    <p class="codeannotation"><span class="cambriamathin1">❹</span> Sets up a Keras function to retrieve the value of the loss and gradients, given an input image</p>

    <p class="codeannotation"><span class="cambriamathin1">❺</span> This function runs gradient ascent for a number of iterations.</p>
  </div>

  <p class="noind">Finally: the actual DeepDream algorithm. First, you define a list of <i class="calibre5">scales</i> (also called <i class="calibre5">octaves</i>) at which to process the images. Each successive scale is larger than the previous one by a factor of 1.4 (it’s 40% larger): you start by processing a small image and then increasingly scale it up (see <a href="#ch08fig04">figure 8.4</a>).</p>

  <p class="notetitle" id="ch08fig04">Figure 8.4. <a id="ch08fig04__title"></a>The DeepDream process: successive scales of spatial processing (octaves) and detail reinjection upon upscaling</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/08fig04_alt.jpg"/></p>

  <p class="noind">For each successive scale, from the smallest to the largest, you run gradient ascent to maximize the loss you previously defined, at that scale. After each gradient ascent run, you upscale the resulting image by 40%.</p>

  <p class="noind">To avoid losing a lot of image detail after each successive scale-up (resulting in increasingly blurry or pixelated images), you can use a simple trick: after each scale-up, you’ll reinject the lost details back into the image, which is possible because you know what the original image should look like at the larger scale. Given a small image size <i class="calibre5">S</i> and a larger image size <i class="calibre5">L</i>, you can compute the difference between the original image resized to size <i class="calibre5">L</i> and the original resized to size <i class="calibre5">S</i>—this difference quantifies the details lost when going from <i class="calibre5">S</i> to <i class="calibre5">L</i>.</p>

  <p class="notetitle" id="ch08ex12">Listing 8.12. <a id="ch08ex12__title"></a>Running gradient ascent over different successive scales</p>
  <pre class="calibre4" id="PLd0e27429">import numpy as np

step = 0.01                                                              <span class="cambriamathin">❶</span><span class="cambriamathin">❷</span>
num_octave = 3                                                           <span class="cambriamathin">❶</span><span class="cambriamathin">❸</span>
octave_scale = 1.4                                                       <span class="cambriamathin">❶</span><span class="cambriamathin">❹</span>
iterations = 20                                                           <span class="cambriamathin">❺</span>

max_loss = 10.                                                            <span class="cambriamathin">❻</span>

base_image_path = '...'                                                   <span class="cambriamathin">❼</span>

img = preprocess_image(base_image_path)                                   <span class="cambriamathin">❽</span>

original_shape = img.shape[1:3]
successive_shapes = [original_shape]                                      <span class="cambriamathin">❾</span>
for i in range(1, num_octave):                                            <span class="cambriamathin">❾</span>
    shape = tuple([int(dim / (octave_scale ** i))                         <span class="cambriamathin">❾</span>
    for dim in original_shape])                                          <span class="cambriamathin">❾</span>
    successive_shapes.append(shape)                                       <span class="cambriamathin">❾</span>

successive_shapes = successive_shapes[::-1]                               <span class="cambriamathin">❿</span>

original_img = np.copy(img)                                               <span class="cambriamathin">⓫</span>
shrunk_original_img = resize_img(img, successive_shapes[0])               <span class="cambriamathin">⓫</span>

for shape in successive_shapes:
    print('Processing image shape', shape)
    img = resize_img(img, shape)                                          <span class="cambriamathin">⓬</span>
    img = gradient_ascent(img,
                          iterations=iterations,                          <span class="cambriamathin">⓭</span>
                          step=step,                                      <span class="cambriamathin">⓭</span>
                          max_loss=max_loss)                              <span class="cambriamathin">⓭</span>
    upscaled_shrunk_original_img = resize_img(shrunk_original_img, shape) <span class="cambriamathin">⓮</span>
    same_size_original = resize_img(original_img, shape)                  <span class="cambriamathin">⓯</span>
    lost_detail = same_size_original - upscaled_shrunk_original_img       <span class="cambriamathin">⓰</span>

    img += lost_detail                                                    <span class="cambriamathin">⓱</span>
    shrunk_original_img = resize_img(original_img, shape)
    save_img(img, fname='dream_at_scale_' + str(shape) + '.png')

save_img(img, fname='final_dream.png')</pre>

  <div class="annotations">
    <p class="codeannotation"><a id="iddle1871"></a><span class="cambriamathin1">❶</span> Playing with these hyperparameters will let you achieve new effects.</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> Gradient ascent step size</p>

    <p class="codeannotation"><span class="cambriamathin1">❸</span> Number of scales at which to run gradient ascent</p>

    <p class="codeannotation"><span class="cambriamathin1">❹</span> Size ratio between scales</p>

    <p class="codeannotation"><span class="cambriamathin1">❺</span> Number of ascent steps to run at each scale</p>

    <p class="codeannotation"><span class="cambriamathin1">❻</span> If the loss grows larger than 10, you’ll interrupt the gradient-ascent process to avoid ugly artifacts.</p>

    <p class="codeannotation"><span class="cambriamathin1">❼</span> Fill this with the path to the image you want to use.</p>

    <p class="codeannotation"><span class="cambriamathin1">❽</span> Loads the base image into a Numpy array (function is defined in <a href="#ch08ex13">listing 8.13</a>)</p>

    <p class="codeannotation"><span class="cambriamathin1">❾</span> Prepares a list of shape tuples defining the different scales at which to run gradient ascent</p>

    <p class="codeannotation"><span class="cambriamathin1">❿</span> Reverses the list of shapes so they’re in increasing order</p>

    <p class="codeannotation"><span class="cambriamathin1">⓫</span> Resizes the Numpy array of the image to the smallest scale</p>

    <p class="codeannotation"><span class="cambriamathin1">⓬</span> Scales up the dream image</p>

    <p class="codeannotation"><span class="cambriamathin1">⓭</span> Runs gradient ascent, altering the dream</p>

    <p class="codeannotation"><span class="cambriamathin1">⓮</span> Scales up the smaller version of the original image: it will be pixellated.</p>

    <p class="codeannotation"><span class="cambriamathin1">⓯</span> Computes the high-quality version of the original image at this size</p>

    <p class="codeannotation"><span class="cambriamathin1">⓰</span> The difference between the two is the detail that was lost when scaling up.</p>

    <p class="codeannotation"><span class="cambriamathin1">⓱</span> Reinjects lost detail into the dream</p>
  </div>

  <p class="noind">Note that this code uses the following straightforward auxiliary Numpy functions, which all do as their names suggest. They require that you have SciPy installed.</p>

  <p class="notetitle" id="ch08ex13">Listing 8.13. <a id="ch08ex13__title"></a>Auxiliary functions</p>
  <pre class="calibre4" id="PLd0e27692">import scipy
from keras.preprocessing import image

def resize_img(img, size):
    img = np.copy(img)
    factors = (1,
               float(size[0]) / img.shape[1],
               float(size[1]) / img.shape[2],
               1)
    return scipy.ndimage.zoom(img, factors, order=1)

def save_img(img, fname):
    pil_img = deprocess_image(np.copy(img))
    scipy.misc.imsave(fname, pil_img)

def preprocess_image(image_path):                        <span class="cambriamathin">❶</span>
    img = image.load_img(image_path)
    img = image.img_to_array(img)
    img = np.expand_dims(img, axis=0)
    img = inception_v3.preprocess_input(img)
    return img

def deprocess_image(x):                                  <span class="cambriamathin">❷</span>
    if K.image_data_format() == 'channels_first':
        x = x.reshape((3, x.shape[2], x.shape[3]))
        x = x.transpose((1, 2, 0))
    else:
        x = x.reshape((x.shape[1], x.shape[2], 3))       <span class="cambriamathin">❸</span>
    x /= 2.
    x += 0.5
    x *= 255.
    x = np.clip(x, 0, 255).astype('uint8')
    return x</pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> Util function to open, resize, and format pictures into tensors that Inception V3 can process</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> Util function to convert a tensor into a valid image</p>

    <p class="codeannotation"><span class="cambriamathin1">❸</span> Undoes preprocessing that was performed by inception_v3.preprocess_input</p>
  </div>
  <hr class="calibre25"/>

  <p class="notetitle" id="ch08note01">Note</p>

  <p class="noindclose">Because the original Inception V3 network was trained to recognize concepts in images of size 299 × 299, and given that the process involves scaling the images down by a reasonable factor, the DeepDream implementation produces much better results on images that are somewhere between 300 × 300 and 400 × 400. Regardless, you can run the same code on images of any size and any ratio.</p>
  <hr class="calibre25"/>

  <p class="noind">Starting from a photograph taken in the small hills between San Francisco Bay and the Google campus, we obtained the DeepDream shown in <a href="#ch08fig05">figure 8.5</a>.</p>

  <p class="notetitle" id="ch08fig05">Figure 8.5. <a id="ch08fig05__title"></a>Running the DeepDream code on an example image</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/08fig05_alt.jpg"/></p>

  <p class="noind">We strongly suggest that you explore what you can do by adjusting which layers you use in your loss. Layers that are lower in the network contain more-local, less-abstract representations and lead to dream patterns that look more geometric. Layers that are higher up lead to more-recognizable visual patterns based on the most common objects found in ImageNet, such as dog eyes, bird feathers, and so on. You can use random generation of the parameters in the <kbd class="calibre24">layer_contributions</kbd> dictionary to quickly explore many different layer combinations. <a href="#ch08fig06">Figure 8.6</a> shows a range of results obtained using different layer configurations, from an image of a delicious homemade pastry.</p>

  <p class="noind"></p>

  <p class="notetitle" id="ch08fig06">Figure 8.6. <a id="ch08fig06__title"></a>Trying a range of DeepDream configurations on an example image</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/08fig06_alt.jpg"/></p>

  <h3 class="head1" id="ch08lev2sec7">8.2.2. <a id="ch08lev2sec7__title"></a>Wrapping up</h3>

  <ul class="calibre16">
    <li class="calibre17"><a id="iddle1314"></a><a id="iddle1332"></a><a id="iddle1715"></a>DeepDream consists of running a convnet in reverse to generate inputs based on the representations learned by the network.</li>

    <li class="calibre17">The results produced are fun and somewhat similar to the visual artifacts induced in humans by the disruption of the visual cortex via psychedelics.</li>

    <li class="calibre17">Note that the process isn’t specific to image models or even to convnets. It can be done for speech, music, and more.</li>
  </ul>

  <h2 class="head" id="ch08lev1sec3"><a class="calibre3" id="ch08lev1sec3__title"></a>8.3. Neural style transfer</h2>

  <p class="noind"><a id="iddle1105"></a><a id="iddle1247"></a><a id="iddle1333"></a><a id="iddle1335"></a><a id="iddle1361"></a><a id="iddle1716"></a><a id="iddle1718"></a><a id="iddle1929"></a><a id="iddle1930"></a>In addition to DeepDream, another major development in deep-learning-driven image modification is <i class="calibre5">neural style transfer</i>, introduced by Leon Gatys et al. in the summer of 2015.<sup class="calibre19">[<a href="#ch08fn05" class="calibre13">5</a>]</sup> The neural style transfer algorithm has undergone many refinements and spawned many variations since its original introduction, and it has made its way into many smartphone photo apps. For simplicity, this section focuses on the formulation described in the original paper.</p>

  <blockquote class="smaller">
    <p class="calibre20"><sup class="calibre19"><a id="ch08fn05" class="calibre13">5</a></sup></p>

    <div class="calibre21">
      Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge, “A Neural Algorithm of Artistic Style,” arXiv (2015), <a href="https://arxiv.org/abs/1508.06576">https://arxiv.org/abs/1508.06576</a>.
    </div>
  </blockquote>

  <p class="noind">Neural style transfer consists of applying the style of a reference image to a target image while conserving the content of the target image. <a href="#ch08fig07">Figure 8.7</a> shows an example.</p>

  <p class="notetitle" id="ch08fig07">Figure 8.7. <a id="ch08fig07__title"></a>A style transfer example</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/08fig07.jpg"/></p>

  <p class="noind">In this context, <i class="calibre5">style</i> essentially means textures, colors, and visual patterns in the image, at various spatial scales; and the <i class="calibre5">content</i> is the higher-level macrostructure of the image. For instance, blue-and-yellow circular brushstrokes are considered to be the style in <a href="#ch08fig07">figure 8.7</a> (using <i class="calibre5">Starry Night</i> by Vincent Van Gogh), and the buildings in the Tübingen photograph are considered to be the content.</p>

  <p class="noind">The idea of style transfer, which is tightly related to that of texture generation, has had a long history in the image-processing community prior to the development of neural style transfer in 2015. But as it turns out, the deep-learning-based implementations of style transfer offer results unparalleled by what had been previously achieved with classical computer-vision techniques, and they triggered an amazing renaissance in creative applications of computer vision.</p>

  <p class="noind">The key notion behind implementing style transfer is the same idea that’s central to all deep-learning algorithms: you define a loss function to specify what you want to achieve, and you minimize this loss. You know what you want to achieve: conserving the content of the original image while adopting the style of the reference image. If we were able to mathematically define <i class="calibre5">content</i> and <i class="calibre5">style</i>, then an appropriate loss function to minimize would be the following:</p>
  <pre class="calibre4" id="PLd0e27959">loss = distance(style(reference_image) - style(generated_image)) +
       distance(content(original_image) - content(generated_image))</pre>

  <p class="noind"><a id="iddle1334"></a><a id="iddle1469"></a><a id="iddle1717"></a>Here, <kbd class="calibre24">distance</kbd> is a norm function such as the L2 norm, <kbd class="calibre24">content</kbd> is a function that takes an image and computes a representation of its content, and <kbd class="calibre24">style</kbd> is a function that takes an image and computes a representation of its style. Minimizing this loss causes <kbd class="calibre24">style(generated_image)</kbd> to be close to <kbd class="calibre24">style(reference_image)</kbd>, and <kbd class="calibre24">content(-generated_image)</kbd> is close to <kbd class="calibre24">content(generated_image)</kbd>, thus achieving style transfer as we defined it.</p>

  <p class="noind">A fundamental observation made by Gatys et al. was that deep convolutional neural networks offer a way to mathematically define the <kbd class="calibre24">style</kbd> and <kbd class="calibre24">content</kbd> functions. Let’s see how.</p>

  <h3 class="head1" id="ch08lev2sec8">8.3.1. <a id="ch08lev2sec8__title"></a>The content loss</h3>

  <p class="noind">As you already know, activations from earlier layers in a network contain <i class="calibre5">local</i> information about the image, whereas activations from higher layers contain increasingly <i class="calibre5">global</i>, <i class="calibre5">abstract</i> information. Formulated in a different way, the activations of the different layers of a convnet provide a decomposition of the contents of an image over different spatial scales. Therefore, you’d expect the content of an image, which is more global and abstract, to be captured by the representations of the upper layers in a convnet.</p>

  <p class="noind">A good candidate for content loss is thus the L2 norm between the activations of an upper layer in a pretrained convnet, computed over the target image, and the activations of the same layer computed over the generated image. This guarantees that, as seen from the upper layer, the generated image will look similar to the original target image. Assuming that what the upper layers of a convnet see is really the content of their input images, then this works as a way to preserve image content.</p>

  <h3 class="head1" id="ch08lev2sec9">8.3.2. <a id="ch08lev2sec9__title"></a>The style loss</h3>

  <p class="noind">The content loss only uses a single upper layer, but the style loss as defined by Gatys et al. uses multiple layers of a convnet: you try to capture the appearance of the style--reference image at all spatial scales extracted by the convnet, not just a single scale. For the style loss, Gatys et al. use the <i class="calibre5">Gram matrix</i> of a layer’s activations: the inner product of the feature maps of a given layer. This inner product can be understood as representing a map of the correlations between the layer’s features. These feature correlations capture the statistics of the patterns of a particular spatial scale, which empirically correspond to the appearance of the textures found at this scale.</p>

  <p class="noind">Hence, the style loss aims to preserve similar internal correlations within the activations of different layers, across the style-reference image and the generated image. In turn, this guarantees that the textures found at different spatial scales look similar across the style-reference image and the generated image.</p>

  <p class="noind">In short, you can use a pretrained convnet to define a loss that will do the following:</p>

  <ul class="calibre16">
    <li class="calibre17">Preserve content by maintaining similar high-level layer activations between the target content image and the generated image. The convnet should “see” both the target image and the generated image as containing the same things.</li>

    <li class="calibre17">Preserve style by maintaining similar <i class="calibre5">correlations</i> within activations for both low-level layers and high-level layers. Feature correlations capture <i class="calibre5">textures</i>: the generated image and the style-reference image should share the same textures at different spatial scales.</li>
  </ul>

  <p class="noind">Now, let’s look at a Keras implementation of the original 2015 neural style transfer algorithm. As you’ll see, it shares many similarities with the DeepDream implementation developed in the previous section.</p>

  <h3 class="head1" id="ch08lev2sec10">8.3.3. <a id="ch08lev2sec10__title"></a>Neural style transfer in Keras</h3>

  <p class="noind">Neural style transfer can be implemented using any pretrained convnet. Here, you’ll use the VGG19 network used by Gatys et al. VGG19 is a simple variant of the VGG16 network introduced in <a href="../Text/05.html#ch05">chapter 5</a>, with three more convolutional layers.</p>

  <p class="noind">This is the general process:</p>

  <ol class="calibre23">
    <li class="calibre17">Set up a network that computes VGG19 layer activations for the style-reference image, the target image, and the generated image at the same time.</li>

    <li class="calibre17">Use the layer activations computed over these three images to define the loss function described earlier, which you’ll minimize in order to achieve style transfer.</li>

    <li class="calibre17">Set up a gradient-descent process to minimize this loss function.</li>
  </ol>

  <p class="noind">Let’s start by defining the paths to the style-reference image and the target image. To make sure that the processed images are a similar size (widely different sizes make style transfer more difficult), you’ll later resize them all to a shared height of 400 px.</p>

  <p class="notetitle" id="ch08ex14">Listing 8.14. <a id="ch08ex14__title"></a>Defining initial variables</p>
  <pre class="calibre4" id="PLd0e28127">from keras.preprocessing.image import load_img, img_to_array

target_image_path = 'img/portrait.jpg'                               <span class="cambriamathin">❶</span>
style_reference_image_path = 'img/transfer_style_reference.jpg'      <span class="cambriamathin">❷</span>

width, height = load_img(target_image_path).size                     <span class="cambriamathin">❸</span>
img_height = 400                                                     <span class="cambriamathin">❸</span>
img_width = int(width * img_height / height)                         <span class="cambriamathin">❸</span></pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> Path to the image you want to transform</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> Path to the style image</p>

    <p class="codeannotation"><span class="cambriamathin1">❸</span> Dimensions of the generated picture</p>
  </div>

  <p class="noind">You need some auxiliary functions for loading, preprocessing, and postprocessing the images that go in and out of the VGG19 convnet.</p>

  <p class="notetitle" id="ch08ex15">Listing 8.15. <a id="ch08ex15__title"></a>Auxiliary functions</p>
  <pre class="calibre4" id="PLd0e28185">import numpy as np
from keras.applications import vgg19

def preprocess_image(image_path):
    img = load_img(image_path, target_size=(img_height, img_width))
    img = img_to_array(img)
    img = np.expand_dims(img, axis=0)
    img = vgg19.preprocess_input(img)
    return img

def deprocess_image(x):
    x[:, :, 0] += 103.939                        <span class="cambriamathin">❶</span>
    x[:, :, 1] += 116.779                        <span class="cambriamathin">❶</span>
    x[:, :, 2] += 123.68                         <span class="cambriamathin">❶</span>
    x = x[:, :, ::-1]                            <span class="cambriamathin">❷</span>
    x = np.clip(x, 0, 255).astype('uint8')
    return x</pre>

  <div class="annotations">
    <p class="codeannotation"><a id="iddle1990"></a><span class="cambriamathin1">❶</span> Zero-centering by removing the mean pixel value from ImageNet. This reverses a transformation done by vgg19.preprocess_input.</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> Converts images from 'BGR' to 'RGB'. This is also part of the reversal of vgg19.preprocess_input.</p>
  </div>

  <p class="noind">Let’s set up the VGG19 network. It takes as input a batch of three images: the style--reference image, the target image, and a placeholder that will contain the generated image. A placeholder is a symbolic tensor, the values of which are provided externally via Numpy arrays. The style-reference and target image are static and thus defined using <kbd class="calibre24">K.constant</kbd>, whereas the values contained in the placeholder of the generated image will change over time.</p>

  <p class="notetitle" id="ch08ex16">Listing 8.16. <a id="ch08ex16__title"></a>Loading the pretrained VGG19 network and applying it to the three images</p>
  <pre class="calibre4" id="PLd0e28242">from keras import backend as K

target_image = K.constant(preprocess_image(target_image_path))
style_reference_image = K.constant(preprocess_image(style_reference_image_path))
combination_image = K.placeholder((1, img_height, img_width, 3))      <span class="cambriamathin">❶</span>

input_tensor = K.concatenate([target_image,                           <span class="cambriamathin">❷</span>
                              style_reference_image,                  <span class="cambriamathin">❷</span>
                              combination_image], axis=0)             <span class="cambriamathin">❷</span>

model = vgg19.VGG19(input_tensor=input_tensor,                        <span class="cambriamathin">❸</span>
                    weights='imagenet',                               <span class="cambriamathin">❸</span>
                    include_top=False)                                <span class="cambriamathin">❸</span>
print('Model loaded.')                                                <span class="cambriamathin">❸</span></pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> Placeholder that will contain the generated image</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> Combines the three images in a single batch</p>

    <p class="codeannotation"><span class="cambriamathin1">❸</span> Builds the VGG19 network with the batch of three images as input. The model will be loaded with pretrained ImageNet weights.</p>
  </div>

  <p class="noind">Let’s define the content loss, which will make sure the top layer of the VGG19 convnet has a similar view of the target image and the generated image.</p>

  <p class="noind"></p>

  <p class="notetitle" id="ch08ex17">Listing 8.17. <a id="ch08ex17__title"></a>Content loss</p>
  <pre class="calibre4" id="PLd0e28315">def content_loss(base, combination):
    return K.sum(K.square(combination - base))</pre>

  <p class="noind">Next is the style loss. It uses an auxiliary function to compute the Gram matrix of an input matrix: a map of the correlations found in the original feature matrix.</p>

  <p class="notetitle" id="ch08ex18">Listing 8.18. <a id="ch08ex18__title"></a>Style loss</p>
  <pre class="calibre4" id="PLd0e28327">def gram_matrix(x):
    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))
    gram = K.dot(features, K.transpose(features))
    return gram

def style_loss(style, combination):
    S = gram_matrix(style)
    C = gram_matrix(combination)
    channels = 3
    size = img_height * img_width
    return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))</pre>

  <p class="noind">To these two loss components, you add a third: the <i class="calibre5">total variation loss</i>, which operates on the pixels of the generated combination image. It encourages spatial continuity in the generated image, thus avoiding overly pixelated results. You can interpret it as a regularization loss.</p>

  <p class="notetitle" id="ch08ex19">Listing 8.19. <a id="ch08ex19__title"></a>Total variation loss</p>
  <pre class="calibre4" id="PLd0e28342">def total_variation_loss(x):
    a = K.square(
        x[:, :img_height - 1, :img_width - 1, :] -
        x[:, 1:, :img_width - 1, :])
    b = K.square(
        x[:, :img_height - 1, :img_width - 1, :] -
        x[:, :img_height - 1, 1:, :])
    return K.sum(K.pow(a + b, 1.25))</pre>

  <p class="noind">The loss that you minimize is a weighted average of these three losses. To compute the content loss, you use only one upper layer—the <kbd class="calibre24">block5_conv2</kbd> layer—whereas for the style loss, you use a list of layers than spans both low-level and high-level layers. You add the total variation loss at the end.</p>

  <p class="noind">Depending on the style-reference image and content image you’re using, you’ll likely want to tune the <kbd class="calibre24">content_weight</kbd> coefficient (the contribution of the content loss to the total loss). A higher <kbd class="calibre24">content_weight</kbd> means the target content will be more recognizable in the generated image.</p>

  <p class="notetitle" id="ch08ex20">Listing 8.20. <a id="ch08ex20__title"></a>Defining the final loss that you’ll minimize</p>
  <pre class="calibre4" id="PLd0e28366">outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])<span class="cambriamathin">❶</span>
content_layer = 'block5_conv2'                                             <span class="cambriamathin">❷</span>
style_layers = ['block1_conv1',                                            <span class="cambriamathin">❸</span>
                'block2_conv1',                                            <span class="cambriamathin">❸</span>
                'block3_conv1',                                            <span class="cambriamathin">❸</span>
                'block4_conv1',                                            <span class="cambriamathin">❸</span>
                'block5_conv1']                                            <span class="cambriamathin">❸</span>
total_variation_weight = 1e-4                                              <span class="cambriamathin">❹</span>
style_weight = 1.                                                          <span class="cambriamathin">❹</span>
content_weight = 0.025                                                     <span class="cambriamathin">❹</span>

loss = K.variable(0.)                                                     <span class="cambriamathin">❺</span><span class="cambriamathin">❻</span>
layer_features = outputs_dict[content_layer]                               <span class="cambriamathin">❻</span>
target_image_features = layer_features[0, :, :, :]                         <span class="cambriamathin">❻</span>
combination_features = layer_features[2, :, :, :]                          <span class="cambriamathin">❻</span>
loss += content_weight * content_loss(target_image_features,               <span class="cambriamathin">❻</span>
                                      combination_features)                <span class="cambriamathin">❻</span>
for layer_name in style_layers:                                            <span class="cambriamathin">❼</span>
    layer_features = outputs_dict[layer_name]
    style_reference_features = layer_features[1, :, :, :]
    combination_features = layer_features[2, :, :, :]
    sl = style_loss(style_reference_features, combination_features)
    loss += (style_weight / len(style_layers)) * sl

loss += total_variation_weight * total_variation_loss(combination_image)   <span class="cambriamathin">❽</span></pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> Dictionary that maps layer names to activation tensors</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> Layer used for content loss</p>

    <p class="codeannotation"><span class="cambriamathin1">❸</span> Layers used for style loss</p>

    <p class="codeannotation"><span class="cambriamathin1">❹</span> Weights in the weighted average of the loss components</p>

    <p class="codeannotation"><span class="cambriamathin1">❺</span> You’ll define the loss by adding all components to this scalar variable.</p>

    <p class="codeannotation"><span class="cambriamathin1">❻</span> Adds the content loss</p>

    <p class="codeannotation"><span class="cambriamathin1">❼</span> Adds a style loss component for each target layer</p>

    <p class="codeannotation"><span class="cambriamathin1">❽</span> Adds the total variation loss</p>
  </div>

  <p class="noind">Finally, you’ll set up the gradient-descent process. In the original Gatys et al. paper, optimization is performed using the L-BFGS algorithm, so that’s what you’ll use here. This is a key difference from the DeepDream example in <a href="#ch08lev1sec2">section 8.2</a>. The L-BFGS algorithm comes packaged with SciPy, but there are two slight limitations with the SciPy implementation:</p>

  <ul class="calibre16">
    <li class="calibre17">It requires that you pass the value of the loss function and the value of the gradients as two separate functions.</li>

    <li class="calibre17">It can only be applied to flat vectors, whereas you have a 3D image array.</li>
  </ul>

  <p class="noind">It would be inefficient to compute the value of the loss function and the value of the gradients independently, because doing so would lead to a lot of redundant computation between the two; the process would be almost twice as slow as computing them jointly. To bypass this, you’ll set up a Python class named <kbd class="calibre24">Evaluator</kbd> that computes both the loss value and the gradients value at once, returns the loss value when called the first time, and caches the gradients for the next call.</p>

  <p class="noind"></p>

  <p class="notetitle" id="ch08ex21">Listing 8.21. <a id="ch08ex21__title"></a>Setting up the gradient-descent process</p>
  <pre class="calibre4" id="PLd0e28546">grads = K.gradients(loss, combination_image)[0]                            <span class="cambriamathin">❶</span>

fetch_loss_and_grads = K.function([combination_image], [loss, grads])      <span class="cambriamathin">❷</span>

class Evaluator(object):                                                   <span class="cambriamathin">❸</span>

    def __init__(self):
        self.loss_value = None
        self.grads_values = None

    def loss(self, x):
        assert self.loss_value is None
        x = x.reshape((1, img_height, img_width, 3))
        outs = fetch_loss_and_grads([x])
        loss_value = outs[0]
        grad_values = outs[1].flatten().astype('float64')
        self.loss_value = loss_value
        self.grad_values = grad_values
        return self.loss_value

    def grads(self, x):
        assert self.loss_value is not None
        grad_values = np.copy(self.grad_values)
        self.loss_value = None
        self.grad_values = None
        return grad_values

evaluator = Evaluator()</pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> Gets the gradients of the generated image with regard to the loss</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> Function to fetch the values of the current loss and the current gradients</p>

    <p class="codeannotation"><span class="cambriamathin1">❸</span> This class wraps fetch_loss_and_grads in a way that lets you retrieve the losses and gradients via two separate method calls, which is required by the SciPy optimizer you'll use.</p>
  </div>

  <p class="noind">Finally, you can run the gradient-ascent process using SciPy’s L-BFGS algorithm, saving the current generated image at each iteration of the algorithm (here, a single iteration represents 20 steps of gradient ascent).</p>

  <p class="notetitle" id="ch08ex22">Listing 8.22. <a id="ch08ex22__title"></a>Style-transfer loop</p>
  <pre class="calibre4" id="PLd0e28597">from scipy.optimize import fmin_l_bfgs_b
from scipy.misc import imsave
import time

result_prefix = 'my_result'
iterations = 20

x = preprocess_image(target_image_path)                            <span class="cambriamathin">❶</span>
x = x.flatten()                                                    <span class="cambriamathin">❷</span>
for i in range(iterations):
    print('Start of iteration', i)
    start_time = time.time()
    x, min_val, info = fmin_l_bfgs_b(evaluator.loss,               <span class="cambriamathin">❸</span>
                                     x,                            <span class="cambriamathin">❸</span>
                                     fprime=evaluator.grads,       <span class="cambriamathin">❸</span>
                                     maxfun=20)                    <span class="cambriamathin">❸</span>
    print('Current loss value:', min_val)
    img = x.copy().reshape((img_height, img_width, 3))             <span class="cambriamathin">❹</span>
    img = deprocess_image(img)                                     <span class="cambriamathin">❹</span>
    fname = result_prefix + '_at_iteration_%d.png' % i             <span class="cambriamathin">❹</span>
    imsave(fname, img)                                             <span class="cambriamathin">❹</span>
    print('Image saved as', fname)                                 <span class="cambriamathin">❹</span>
    end_time = time.time()                                         <span class="cambriamathin">❹</span>
    print('Iteration %d completed in %ds' % (i, end_time - start_time))</pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> This is the initial state: the target image.</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> You flatten the image because scipy.optimize.fmin_l_bfgs_b can only process flat vectors.</p>

    <p class="codeannotation"><span class="cambriamathin1">❸</span> Runs L-BFGS optimization over the pixels of the generated image to minimize the neural style loss. Note that you have to pass the function that computes the loss and the function that computes the gradients as two separate arguments.</p>

    <p class="codeannotation"><span class="cambriamathin1">❹</span> Saves the current generated image.</p>
  </div>

  <p class="noind"><a href="#ch08fig08">Figure 8.8</a> shows what you get. Keep in mind that what this technique achieves is merely a form of image retexturing, or texture transfer. It works best with style--reference images that are strongly textured and highly self-similar, and with content targets that don’t require high levels of detail in order to be recognizable. It typically can’t achieve fairly abstract feats such as transferring the style of one portrait to another. The algorithm is closer to classical signal processing than to AI, so don’t expect it to work like magic!</p>

  <p class="noind"></p>

  <p class="notetitle" id="ch08fig08">Figure 8.8. <a id="ch08fig08__title"></a><a id="iddle1044"></a><a id="iddle1311"></a><a id="iddle1318"></a><a id="iddle1320"></a><a id="iddle1412"></a><a id="iddle1414"></a><a id="iddle1415"></a><a id="iddle1496"></a><a id="iddle1861"></a><a id="iddle2029"></a><a id="iddle2031"></a>Some example results</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/08fig08.jpg"/></p>

  <p class="noind">Additionally, note that running this style-transfer algorithm is slow. But the transformation operated by the setup is simple enough that it can be learned by a small, fast feedforward convnet as well—as long as you have appropriate training data available. Fast style transfer can thus be achieved by first spending a lot of compute cycles to generate input-output training examples for a fixed style-reference image, using the method outlined here, and then training a simple convnet to learn this style-specific transformation. Once that’s done, stylizing a given image is instantaneous: it’s just a forward pass of this small convnet.</p>

  <h3 class="head1" id="ch08lev2sec11">8.3.4. <a id="ch08lev2sec11__title"></a>Wrapping up</h3>

  <ul class="calibre16">
    <li class="calibre17"><a id="iddle1101"></a><a id="iddle1256"></a><a id="iddle1319"></a><a id="iddle1410"></a><a id="iddle1413"></a><a id="iddle1909"></a><a id="iddle2030"></a>Style transfer consists of creating a new image that preserves the contents of a target image while also capturing the style of a reference image.</li>

    <li class="calibre17">Content can be captured by the high-level activations of a convnet.</li>

    <li class="calibre17">Style can be captured by the internal correlations of the activations of different layers of a convnet.</li>

    <li class="calibre17">Hence, deep learning allows style transfer to be formulated as an optimization process using a loss defined with a pretrained convnet.</li>

    <li class="calibre17">Starting from this basic idea, many variants and refinements are possible.</li>
  </ul>

  <h2 class="head" id="ch08lev1sec4"><a class="calibre3" id="ch08lev1sec4__title"></a>8.4. Generating images with variational autoencoders</h2>

  <p class="noind"><a id="iddle1486"></a><a id="iddle2071"></a>Sampling from a latent space of images to create entirely new images or edit existing ones is currently the most popular and successful application of creative AI. In this section and the next, we’ll review some high-level concepts pertaining to image generation, alongside implementation details relative to the two main techniques in this domain: <i class="calibre5">variational autoencoders</i> (VAEs) and <i class="calibre5">generative adversarial networks</i> (GANs). The techniques we present here aren’t specific to images—you could develop latent spaces of sound, music, or even text, using GANs and VAEs—but in practice, the most interesting results have been obtained with pictures, and that’s what we focus on here.</p>

  <h3 class="head1" id="ch08lev2sec12">8.4.1. <a id="ch08lev2sec12__title"></a>Sampling from latent spaces of images</h3>

  <p class="noind">The key idea of image generation is to develop a low-dimensional <i class="calibre5">latent space</i> of representations (which naturally is a vector space) where any point can be mapped to a realistic-looking image. The module capable of realizing this mapping, taking as input a latent point and outputting an image (a grid of pixels), is called a <i class="calibre5">generator</i> (in the case of GANs) or a <i class="calibre5">decoder</i> (in the case of VAEs). Once such a latent space has been developed, you can sample points from it, either deliberately or at random, and, by mapping them to image space, generate images that have never been seen before (see <a href="#ch08fig09">figure 8.9</a>).</p>

  <p class="notetitle" id="ch08fig09">Figure 8.9. <a id="ch08fig09__title"></a>Learning a latent vector space of images, and using it to sample new images</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/08fig09.jpg"/></p>

  <p class="noind">GANs and VAEs are two different strategies for learning such latent spaces of image representations, each with its own characteristics. VAEs are great for learning latent spaces that are well structured, where specific directions encode a meaningful axis of variation in the data. GANs generate images that can potentially be highly realistic, but the latent space they come from may not have as much structure and continuity.</p>

  <p class="noind"></p>

  <p class="notetitle" id="ch08fig10">Figure 8.10. <a id="ch08fig10__title"></a>A continuous space of faces generated by Tom White using VAEs</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/08fig10_alt.jpg"/></p>

  <h3 class="head1" id="ch08lev2sec13">8.4.2. <a id="ch08lev2sec13__title"></a>Concept vectors for image editing</h3>

  <p class="noind">We already hinted at the idea of a <i class="calibre5">concept vector</i> when we covered word embeddings in <a href="../Text/06.html#ch06">chapter 6</a>. The idea is still the same: given a latent space of representations, or an embedding space, certain directions in the space may encode interesting axes of variation in the original data. In a latent space of images of faces, for instance, there may be a <i class="calibre5">smile vector</i>, such that if latent point <kbd class="calibre24">z</kbd> is the embedded representation of a certain face, then latent point <kbd class="calibre24">z + s</kbd> is the embedded representation of the same face, smiling. Once you’ve identified such a vector, it then becomes possible to edit images by projecting them into the latent space, moving their representation in a meaningful way, and then decoding them back to image space. There are concept vectors for essentially any independent dimension of variation in image space—in the case of faces, you may discover vectors for adding sunglasses to a face, removing glasses, turning a male face into as female face, and so on. <a href="#ch08fig11">Figure 8.11</a> is an example of a smile vector, a concept vector discovered by Tom White from the Victoria University School of Design in New Zealand, using VAEs trained on a dataset of faces of celebrities (the CelebA dataset).</p>

  <p class="noind"></p>

  <p class="notetitle" id="ch08fig11">Figure 8.11. <a id="ch08fig11__title"></a>The smile vector</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/08fig11_alt.jpg"/></p>

  <h3 class="head1" id="ch08lev2sec14">8.4.3. <a id="ch08lev2sec14__title"></a>Variational autoencoders</h3>

  <p class="noind"><a id="iddle1266"></a><a id="iddle1837"></a>Variational autoencoders, simultaneously discovered by Kingma and Welling in December 2013<sup class="calibre19">[<a href="#ch08fn06" class="calibre13">6</a>]</sup> and Rezende, Mohamed, and Wierstra in January 2014,<sup class="calibre19">[<a href="#ch08fn07" class="calibre13">7</a>]</sup> are a kind of generative model that’s especially appropriate for the task of image editing via concept vectors. They’re a modern take on autoencoders—a type of network that aims to encode an input to a low-dimensional latent space and then decode it back—that mixes ideas from deep learning with Bayesian inference.</p>

  <blockquote class="smaller">
    <p class="calibre20"><sup class="calibre19"><a id="ch08fn06" class="calibre13">6</a></sup></p>

    <div class="calibre21">
      Diederik P. Kingma and Max Welling, “Auto-Encoding Variational Bayes, arXiv (2013), <a href="https://arxiv.org/abs/1312.6114">https://arxiv.org/abs/1312.6114</a>.
    </div>
  </blockquote>

  <blockquote class="smaller">
    <p class="calibre20"><sup class="calibre19"><a id="ch08fn07" class="calibre13">7</a></sup></p>

    <div class="calibre21">
      Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra, “Stochastic Backpropagation and Approximate Inference in Deep Generative Models,” arXiv (2014), <a href="https://arxiv.org/abs/1401.4082">https://arxiv.org/abs/1401.4082</a>.
    </div>
  </blockquote>

  <p class="noind">A classical image autoencoder takes an image, maps it to a latent vector space via an encoder module, and then decodes it back to an output with the same dimensions as the original image, via a decoder module (see <a href="#ch08fig12">figure 8.12</a>). It’s then trained by using as target data the <i class="calibre5">same images</i> as the input images, meaning the autoencoder learns to reconstruct the original inputs. By imposing various constraints on the code (the output of the encoder), you can get the autoencoder to learn more-or-less interesting latent representations of the data. Most commonly, you’ll constrain the code to be low-dimensional and sparse (mostly zeros), in which case the encoder acts as a way to compress the input data into fewer bits of information.</p>

  <p class="noind"></p>

  <p class="notetitle" id="ch08fig12">Figure 8.12. <a id="ch08fig12__title"></a>An autoencoder: mapping an input <kbd class="calibre24">x</kbd> to a compressed representation and then decoding it back as <kbd class="calibre24">x'</kbd></p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/08fig12_alt.jpg"/></p>

  <p class="noind"><a id="iddle1492"></a>In practice, such classical autoencoders don’t lead to particularly useful or nicely structured latent spaces. They’re not much good at compression, either. For these reasons, they have largely fallen out of fashion. VAEs, however, augment autoencoders with a little bit of statistical magic that forces them to learn continuous, highly structured latent spaces. They have turned out to be a powerful tool for image generation.</p>

  <p class="noind">A VAE, instead of compressing its input image into a fixed code in the latent space, turns the image into the parameters of a statistical distribution: a mean and a variance. Essentially, this means you’re assuming the input image has been generated by a statistical process, and that the randomness of this process should be taken into account during encoding and decoding. The VAE then uses the mean and variance parameters to randomly sample one element of the distribution, and decodes that element back to the original input (see <a href="#ch08fig13">figure 8.13</a>). The stochasticity of this process improves robustness and forces the latent space to encode meaningful representations everywhere: every point sampled in the latent space is decoded to a valid output.</p>

  <p class="notetitle" id="ch08fig13">Figure 8.13. <a id="ch08fig13__title"></a>A VAE maps an image to two vectors, <kbd class="calibre24">z_mean</kbd> and <kbd class="calibre24">z_log_sigma</kbd>, which define a probability distribution over the latent space, used to sample a latent point to decode.</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/08fig13_alt.jpg"/></p>

  <p class="noind"><a id="iddle1012"></a>In technical terms, here’s how a VAE works:</p>

  <ol class="calibre23">
    <li class="calibre17">An encoder module turns the input samples <kbd class="calibre24">input_img</kbd> into two parameters in a latent space of representations, <kbd class="calibre24">z_mean</kbd> and <kbd class="calibre24">z</kbd>_<kbd class="calibre24">log_variance</kbd>.</li>

    <li class="calibre17">You randomly sample a point <kbd class="calibre24">z</kbd> from the latent normal distribution that’s assumed to generate the input image, via <kbd class="calibre24">z = z_mean + exp(z_log_variance) * epsilon</kbd>, where <kbd class="calibre24">epsilon</kbd> is a random tensor of small values.</li>

    <li class="calibre17">A decoder module maps this point in the latent space back to the original input image.</li>
  </ol>

  <p class="noind">Because <kbd class="calibre24">epsilon</kbd> is random, the process ensures that every point that’s close to the latent location where you encoded <kbd class="calibre24">input_img</kbd> (<kbd class="calibre24">z-mean</kbd>) can be decoded to something similar to <kbd class="calibre24">input_img</kbd>, thus forcing the latent space to be continuously meaningful. Any two close points in the latent space will decode to highly similar images. Continuity, combined with the low dimensionality of the latent space, forces every direction in the latent space to encode a meaningful axis of variation of the data, making the latent space very structured and thus highly suitable to manipulation via concept vectors.</p>

  <p class="noind">The parameters of a VAE are trained via two loss functions: a <i class="calibre5">reconstruction loss</i> that forces the decoded samples to match the initial inputs, and a <i class="calibre5">regularization loss</i> that helps learn well-formed latent spaces and reduce overfitting to the training data. Let’s quickly go over a Keras implementation of a VAE. Schematically, it looks like this:</p>
  <pre class="calibre4" id="PLd0e29172">z_mean, z_log_variance = encoder(input_img)         <span class="cambriamathin">❶</span>

z = z_mean + exp(z_log_variance) * epsilon          <span class="cambriamathin">❷</span>

reconstructed_img = decoder(z)                      <span class="cambriamathin">❸</span>

model = Model(input_img, reconstructed_img)         <span class="cambriamathin">❹</span></pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> Encodes the input into a mean and variance parameter</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> Draws a latent point using a small random epsilon</p>

    <p class="codeannotation"><span class="cambriamathin1">❸</span> Decodes z back to an image</p>

    <p class="codeannotation"><span class="cambriamathin1">❹</span> Instantiates the autoencoder model, which maps an input image to its reconstruction</p>
  </div>

  <p class="noind">You can then train the model using the reconstruction loss and the regularization loss.</p>

  <p class="noind">The following listing shows the encoder network you’ll use, mapping images to the parameters of a probability distribution over the latent space. It’s a simple convnet that maps the input image <kbd class="calibre24">x</kbd> to two vectors, <kbd class="calibre24">z_mean</kbd> and <kbd class="calibre24">z_log_var</kbd>.</p>

  <p class="notetitle" id="ch08ex23">Listing 8.23. <a id="ch08ex23__title"></a>VAE encoder network</p>
  <pre class="calibre4" id="PLd0e29246">import keras
from keras import layers
from keras import backend as K
from keras.models import Model
import numpy as np

img_shape = (28, 28, 1)
batch_size = 16
latent_dim = 2                                     <span class="cambriamathin">❶</span>

input_img = keras.Input(shape=img_shape)

x = layers.Conv2D(32, 3,
                  padding='same', activation='relu')(input_img)
x = layers.Conv2D(64, 3,
                  padding='same', activation='relu',
                  strides=(2, 2))(x)
x = layers.Conv2D(64, 3,
                  padding='same', activation='relu')(x)
x = layers.Conv2D(64, 3,
                  padding='same', activation='relu')(x)
shape_before_flattening = K.int_shape(x)

x = layers.Flatten()(x)
x = layers.Dense(32, activation='relu')(x)

z_mean = layers.Dense(latent_dim)(x)               <span class="cambriamathin">❷</span>
z_log_var = layers.Dense(latent_dim)(x)            <span class="cambriamathin">❷</span></pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> Dimensionality of the latent space: a 2D plane</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> The input image ends up being encoded into these two parameters.</p>
  </div>

  <p class="noind">Next is the code for using <kbd class="calibre24">z_mean</kbd> and <kbd class="calibre24">z_log_var</kbd>, the parameters of the statistical distribution assumed to have produced <kbd class="calibre24">input_img</kbd>, to generate a latent space point <kbd class="calibre24">z</kbd>. Here, you wrap some arbitrary code (built on top of Keras backend primitives) into a <kbd class="calibre24">Lambda</kbd> layer. In Keras, everything needs to be a layer, so code that isn’t part of a built-in layer should be wrapped in a <kbd class="calibre24">Lambda</kbd> (or in a custom layer).</p>

  <p class="notetitle" id="ch08ex24">Listing 8.24. <a id="ch08ex24__title"></a>Latent-space-sampling function</p>
  <pre class="calibre4" id="PLd0e29308">def sampling(args):
    z_mean, z_log_var = args
    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),
                              mean=0., stddev=1.)
    return z_mean + K.exp(z_log_var) * epsilon

z = layers.Lambda(sampling)([z_mean, z_log_var])</pre>

  <p class="noind">The following listing shows the decoder implementation. You reshape the vector <kbd class="calibre24">z</kbd> to the dimensions of an image and then use a few convolution layers to obtain a final image output that has the same dimensions as the original <kbd class="calibre24">input_img</kbd>.</p>

  <p class="notetitle" id="ch08ex25">Listing 8.25. <a id="ch08ex25__title"></a>VAE decoder network, mapping latent space points to images</p>
  <pre class="calibre4" id="PLd0e29326">decoder_input = layers.Input(K.int_shape(z)[1:])                <span class="cambriamathin">❶</span>

x = layers.Dense(np.prod(shape_before_flattening[1:]),          <span class="cambriamathin">❷</span>
                 activation='relu')(decoder_input)              <span class="cambriamathin">❷</span>

x = layers.Reshape(shape_before_flattening[1:])(x)              <span class="cambriamathin">❸</span>
x = layers.Conv2DTranspose(32, 3,                               <span class="cambriamathin">❹</span>
                           padding='same',                      <span class="cambriamathin">❹</span>
                           activation='relu',                   <span class="cambriamathin">❹</span>
                           strides=(2, 2))(x)                   <span class="cambriamathin">❹</span>
x = layers.Conv2D(1, 3,                                         <span class="cambriamathin">❹</span>
                  padding='same',                               <span class="cambriamathin">❹</span>
                  activation='sigmoid')(x)                      <span class="cambriamathin">❹</span>

decoder = Model(decoder_input, x)                               <span class="cambriamathin">❺</span>

z_decoded = decoder(z)                                          <span class="cambriamathin">❻</span></pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> Input where you’ll feed z</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> Upsamples the input</p>

    <p class="codeannotation"><span class="cambriamathin1">❸</span> Reshapes z into a feature map of the same shape as the feature map just before the last Flatten layer in the encoder model</p>

    <p class="codeannotation"><span class="cambriamathin1">❹</span> Uses a Conv2DTranspose layer and Conv2D layer to decode z into a feature map the same size as the original image input</p>

    <p class="codeannotation"><span class="cambriamathin1">❺</span> Instantiates the decoder model, which turns “decoder_input” into the decoded image</p>

    <p class="codeannotation"><span class="cambriamathin1">❻</span> Applies it to z to recover the decoded z</p>
  </div>

  <p class="noind">The dual loss of a VAE doesn’t fit the traditional expectation of a sample-wise function of the form <kbd class="calibre24">loss(input, target)</kbd>. Thus, you’ll set up the loss by writing a custom layer that internally uses the built-in <kbd class="calibre24">add_loss</kbd> layer method to create an arbitrary loss.</p>

  <p class="notetitle" id="ch08ex26">Listing 8.26. <a id="ch08ex26__title"></a>Custom layer used to compute the VAE loss</p>
  <pre class="calibre4" id="PLd0e29448">class CustomVariationalLayer(keras.layers.Layer):

    def vae_loss(self, x, z_decoded):
        x = K.flatten(x)
        z_decoded = K.flatten(z_decoded)
        xent_loss = keras.metrics.binary_crossentropy(x, z_decoded)
        kl_loss = -5e-4 * K.mean(
            1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)
        return K.mean(xent_loss + kl_loss)

    def call(self, inputs):                               <span class="cambriamathin">❶</span>
        x = inputs[0]
        z_decoded = inputs[1]
        loss = self.vae_loss(x, z_decoded)
        self.add_loss(loss, inputs=inputs)
        return x                                          <span class="cambriamathin">❷</span>

y = CustomVariationalLayer()([input_img, z_decoded])      <span class="cambriamathin">❸</span></pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> You implement custom layers by writing a call method.</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> You don't use this output, but the layer must return something.</p>

    <p class="codeannotation"><span class="cambriamathin1">❸</span> Calls the custom layer on the input and the decoded output to obtain the final model output</p>
  </div>

  <p class="noind">Finally, you’re ready to instantiate and train the model. Because the loss is taken care of in the custom layer, you don’t specify an external loss at compile time (<kbd class="calibre24">loss=None</kbd>), <a id="iddle1015"></a><a id="iddle1245"></a><a id="iddle1312"></a><a id="iddle1326"></a><a id="iddle1346"></a>which in turn means you won’t pass target data during training (as you can see, you only pass <kbd class="calibre24">x_train</kbd> to the model in <kbd class="calibre24">fit</kbd>).</p>

  <p class="notetitle" id="ch08ex27">Listing 8.27. <a id="ch08ex27__title"></a>Training the VAE</p>
  <pre class="calibre4" id="PLd0e29540">from keras.datasets import mnist

vae = Model(input_img, y)
vae.compile(optimizer='rmsprop', loss=None)
vae.summary()

(x_train, _), (x_test, y_test) = mnist.load_data()

x_train = x_train.astype('float32') / 255.
x_train = x_train.reshape(x_train.shape + (1,))
x_test = x_test.astype('float32') / 255.
x_test = x_test.reshape(x_test.shape + (1,))

vae.fit(x=x_train, y=None,
        shuffle=True,
        epochs=10,
        batch_size=batch_size,
        validation_data=(x_test, None))</pre>

  <p class="noind">Once such a model is trained—on MNIST, in this case—you can use the <kbd class="calibre24">decoder</kbd> network to turn arbitrary latent space vectors into images.</p>

  <p class="notetitle" id="ch08ex28">Listing 8.28. <a id="ch08ex28__title"></a>Sampling a grid of points from the 2D latent space and decoding them to images</p>
  <pre class="calibre4" id="PLd0e29555">import matplotlib.pyplot as plt
from scipy.stats import norm

n = 15                                                                    <span class="cambriamathin">❶</span>
digit_size = 28
figure = np.zeros((digit_size * n, digit_size * n))
grid_x = norm.ppf(np.linspace(0.05, 0.95, n))                             <span class="cambriamathin">❷</span>
grid_y = norm.ppf(np.linspace(0.05, 0.95, n))                             <span class="cambriamathin">❷</span>

for i, yi in enumerate(grid_x):
    for j, xi in enumerate(grid_y):
        z_sample = np.array([[xi, yi]])
        z_sample = np.tile(z_sample, batch_size).reshape(batch_size, 2)   <span class="cambriamathin">❸</span>
        x_decoded = decoder.predict(z_sample, batch_size=batch_size)      <span class="cambriamathin">❹</span>
        digit = x_decoded[0].reshape(digit_size, digit_size)              <span class="cambriamathin">❺</span>
        figure[i * digit_size: (i + 1) * digit_size,
               j * digit_size: (j + 1) * digit_size] = digit

plt.figure(figsize=(10, 10))
plt.imshow(figure, cmap='Greys_r')
plt.show()</pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> You’ll display a grid of 15 × 15 digits (255 digits total).</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> Transforms linearly spaced coordinates using the SciPy ppf function to produce values of the latent variable z (because the prior of the latent space is Gaussian)</p>

    <p class="codeannotation"><span class="cambriamathin1">❸</span> Repeats z multiple times to form a complete batch</p>

    <p class="codeannotation"><a id="iddle2017"></a><span class="cambriamathin1">❹</span> Decodes the batch into digit images</p>

    <p class="codeannotation"><span class="cambriamathin1">❺</span> Reshapes the first digit in the batch from 28 × 28 × 1 to 28 × 28</p>
  </div>

  <p class="noind">The grid of sampled digits (see <a href="#ch08fig14">figure 8.14</a>) shows a completely continuous distribution of the different digit classes, with one digit morphing into another as you follow a path through latent space. Specific directions in this space have a meaning: for example, there’s a direction for “four-ness,” “one-ness,” and so on.</p>

  <p class="notetitle" id="ch08fig14">Figure 8.14. <a id="ch08fig14__title"></a>Grid of digits decoded from the latent space</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/08fig14.jpg"/></p>

  <p class="noind">In the next section, we’ll cover in detail the other major tool for generating artificial images: generative adversarial networks (GANs).</p>

  <h3 class="head1" id="ch08lev2sec15">8.4.4. <a id="ch08lev2sec15__title"></a>Wrapping up</h3>

  <ul class="calibre16">
    <li class="calibre17">Image generation with deep learning is done by learning latent spaces that capture statistical information about a dataset of images. By sampling and decoding points from the latent space, you can generate never-before-seen images. There are two major tools to do this: VAEs and GANs.</li>

    <li class="calibre17">VAEs result in highly structured, continuous latent representations. For this reason, they work well for doing all sorts of image editing in latent space: face swapping, turning a frowning face into a smiling face, and so on. They also work nicely for doing latent-space-based animations, such as animating a walk along a cross section of the latent space, showing a starting image slowly morphing into different images in a continuous way.</li>

    <li class="calibre17">GANs enable the generation of realistic single-frame images but may not induce latent spaces with solid structure and high continuity.</li>
  </ul>

  <p class="noind">Most successful practical applications I have seen with images rely on VAEs, but GANs are extremely popular in the world of academic research—at least, circa 2016–2017. You’ll find out how they work and how to implement one in the next section.</p>
  <hr class="calibre25"/>

  <p class="notetitle" id="ch08note02">Tip</p>

  <p class="noindclose">To play further with image generation, I suggest working with the Large-scale Celeb Faces Attributes (CelebA) dataset. It’s a free-to-download image dataset containing more than 200,000 celebrity portraits. It’s great for experimenting with concept vectors in particular—it definitely beats MNIST.</p>
  <hr class="calibre25"/>

  <h2 class="head" id="ch08lev1sec5"><a class="calibre3" id="ch08lev1sec5__title"></a>8.5. Introduction to generative adversarial networks</h2>

  <p class="noind"><a id="iddle1191"></a><a id="iddle1246"></a><a id="iddle1315"></a><a id="iddle1328"></a><a id="iddle1329"></a><a id="iddle1330"></a><a id="iddle1340"></a><a id="iddle1868"></a>Generative adversarial networks (GANs), introduced in 2014 by Goodfellow et al.,<sup class="calibre19">[<a href="#ch08fn08" class="calibre13">8</a>]</sup> are an alternative to VAEs for learning latent spaces of images. They enable the generation of fairly realistic synthetic images by forcing the generated images to be statistically almost indistinguishable from real ones.</p>

  <blockquote class="smaller">
    <p class="calibre20"><sup class="calibre19"><a id="ch08fn08" class="calibre13">8</a></sup></p>

    <div class="calibre21">
      Ian Goodfellow et al., “Generative Adversarial Networks,” arXiv (2014), <a href="https://arxiv.org/abs/1406.2661">https://arxiv.org/abs/1406.2661</a>.
    </div>
  </blockquote>

  <p class="noind">An intuitive way to understand GANs is to imagine a forger trying to create a fake Picasso painting. At first, the forger is pretty bad at the task. He mixes some of his fakes with authentic Picassos and shows them all to an art dealer. The art dealer makes an authenticity assessment for each painting and gives the forger feedback about what makes a Picasso look like a Picasso. The forger goes back to his studio to prepare some new fakes. As times goes on, the forger becomes increasingly competent at imitating the style of Picasso, and the art dealer becomes increasingly expert at spotting fakes. In the end, they have on their hands some excellent fake Picassos.</p>

  <p class="noind">That’s what a GAN is: a forger network and an expert network, each being trained to best the other. As such, a GAN is made of two parts:</p>

  <ul class="calibre16">
    <li class="calibre17"><b class="calibre22">Generator network—</b> Takes as input a random vector (a random point in the latent space), and decodes it into a synthetic image</li>

    <li class="calibre17"><b class="calibre22">Discriminator network (or adversary)—</b> Takes as input an image (real or synthetic), and predicts whether the image came from the training set or was created by the generator network.</li>
  </ul>

  <p class="noind">The generator network is trained to be able to fool the discriminator network, and thus it evolves toward generating increasingly realistic images as training goes on: artificial images that look indistinguishable from real ones, to the extent that it’s impossible for the discriminator network to tell the two apart (see <a href="#ch08fig15">figure 8.15</a>). Meanwhile, the discriminator is constantly adapting to the gradually improving capabilities of the generator, setting a high bar of realism for the generated images. Once training is over, the generator is capable of turning any point in its input space into a believable image. Unlike VAEs, this latent space has fewer explicit guarantees of meaningful structure; in particular, it isn’t continuous.</p>

  <p class="noind"></p>

  <p class="notetitle" id="ch08fig15">Figure 8.15. <a id="ch08fig15__title"></a>A generator transforms random latent vectors into images, and a discriminator seeks to tell real images from generated ones. The generator is trained to fool the discriminator.</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/08fig15_alt.jpg"/></p>

  <p class="noind"><a id="iddle1513"></a><a id="iddle1926"></a>Remarkably, a GAN is a system where the optimization minimum isn’t fixed, unlike in any other training setup you’ve encountered in this book. Normally, gradient descent consists of rolling down hills in a static loss landscape. But with a GAN, every step taken down the hill changes the entire landscape a little. It’s a dynamic system where the optimization process is seeking not a minimum, but an equilibrium between two forces. For this reason, GANs are notoriously difficult to train—getting a GAN to work requires lots of careful tuning of the model architecture and training parameters.</p>

  <p class="notetitle" id="ch08fig16">Figure 8.16. <a id="ch08fig16__title"></a>Latent space dwellers. Images generated by Mike Tyka using a multistaged GAN trained on a dataset of faces (<a href="http://www.miketyka.com">www.miketyka.com</a>).</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/08fig16.jpg"/></p>

  <h3 class="head1" id="ch08lev2sec16">8.5.1. <a id="ch08lev2sec16__title"></a>A schematic GAN implementation</h3>

  <p class="noind">In this section, we’ll explain how to implement a GAN in Keras, in its barest form—because GANs are advanced, diving deeply into the technical details would be out of scope for this book. The specific implementation is a <i class="calibre5">deep convolutional GAN</i> (DCGAN): a GAN where the generator and discriminator are deep convnets. In particular, it uses a <kbd class="calibre24">Conv2DTranspose</kbd> layer for image upsampling in the generator.</p>

  <p class="noind">You’ll train the GAN on images from CIFAR10, a dataset of 50,000 32 × 32 RGB images belonging to 10 classes (5,000 images per class). To make things easier, you’ll only use images belonging to the class “frog.”</p>

  <p class="noind">Schematically, the GAN looks like this:</p>

  <ol class="calibre23">
    <li class="calibre17">A <kbd class="calibre24">generator</kbd> network maps vectors of shape <kbd class="calibre24">(latent_dim,)</kbd> to images of shape <kbd class="calibre24">(32, 32, 3)</kbd>.</li>

    <li class="calibre17">A <kbd class="calibre24">discriminator</kbd> network maps images of shape <kbd class="calibre24">(32, 32, 3)</kbd> to a binary score estimating the probability that the image is real.</li>

    <li class="calibre17">A <kbd class="calibre24">gan</kbd> network chains the generator and the discriminator together: <kbd class="calibre24">gan(x) = discriminator(generator(x))</kbd>. Thus this <kbd class="calibre24">gan</kbd> network maps latent space vectors to the discriminator’s assessment of the realism of these latent vectors as decoded by the generator.</li>

    <li class="calibre17">You train the discriminator using examples of real and fake images along with “real”/“fake” labels, just as you train any regular image-classification model.</li>

    <li class="calibre17">To train the generator, you use the gradients of the generator’s weights with regard to the loss of the <kbd class="calibre24">gan</kbd> model. This means, at every step, you move the weights of the generator in a direction that makes the discriminator more likely to classify as “real” the images decoded by the generator. In other words, you train the generator to fool the discriminator.</li>
  </ol>

  <h3 class="head1" id="ch08lev2sec17">8.5.2. <a id="ch08lev2sec17__title"></a>A bag of tricks</h3>

  <p class="noind">The process of training GANs and tuning GAN implementations is notoriously difficult. There are a number of known tricks you should keep in mind. Like most things in deep learning, it’s more alchemy than science: these tricks are heuristics, not theory-backed guidelines. They’re supported by a level of intuitive understanding of the phenomenon at hand, and they’re known to work well empirically, although not necessarily in every context.</p>

  <p class="noind">Here are a few of the tricks used in the implementation of the GAN generator and discriminator in this section. It isn’t an exhaustive list of GAN-related tips; you’ll find many more across the GAN literature:</p>

  <ul class="calibre16">
    <li class="calibre17">We use <kbd class="calibre24">tanh</kbd> as the last activation in the generator, instead of <kbd class="calibre24">sigmoid</kbd>, which is more commonly found in other types of models.</li>

    <li class="calibre17">We sample points from the latent space using a <i class="calibre5">normal distribution</i> (Gaussian distribution), not a uniform distribution.</li>

    <li class="calibre17"><a id="iddle1014"></a><a id="iddle1192"></a><a id="iddle1327"></a><a id="iddle1331"></a>Stochasticity is good to induce robustness. Because GAN training results in a dynamic equilibrium, GANs are likely to get stuck in all sorts of ways. Introducing randomness during training helps prevent this. We introduce randomness in two ways: by using dropout in the discriminator and by adding random noise to the labels for the discriminator.</li>

    <li class="calibre17">Sparse gradients can hinder GAN training. In deep learning, sparsity is often a desirable property, but not in GANs. Two things can induce gradient sparsity: max pooling operations and <kbd class="calibre24">ReLU</kbd> activations. Instead of max pooling, we recommend using strided convolutions for downsampling, and we recommend using a <kbd class="calibre24">LeakyReLU</kbd> layer instead of a <kbd class="calibre24">ReLU</kbd> activation. It’s similar to <kbd class="calibre24">ReLU</kbd>, but it relaxes sparsity constraints by allowing small negative activation values.</li>

    <li class="calibre17">In generated images, it’s common to see checkerboard artifacts caused by unequal coverage of the pixel space in the generator (see <a href="#ch08fig17">figure 8.17</a>). To fix this, we use a kernel size that’s divisible by the stride size whenever we use a strided <kbd class="calibre24">Conv2DTranpose</kbd> or <kbd class="calibre24">Conv2D</kbd> in both the generator and the discriminator.</li>
  </ul>

  <p class="notetitle" id="ch08fig17">Figure 8.17. <a id="ch08fig17__title"></a>Checkerboard artifacts caused by mismatching strides and kernel sizes, resulting in unequal pixel-space coverage: one of the many gotchas of GANs</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/08fig17.jpg"/></p>

  <h3 class="head1" id="ch08lev2sec18">8.5.3. <a id="ch08lev2sec18__title"></a>The generator</h3>

  <p class="noind">First, let’s develop a <kbd class="calibre24">generator</kbd> model that turns a vector (from the latent space—during training it will be sampled at random) into a candidate image. One of the many issues that commonly arise with GANs is that the generator gets stuck with generated images that look like noise. A possible solution is to use dropout on both the discriminator and the generator.</p>

  <p class="notetitle" id="ch08ex29">Listing 8.29. <a id="ch08ex29__title"></a>GAN generator network</p>
  <pre class="calibre4" id="PLd0e30068">import keras
from keras import layers
import numpy as np

latent_dim = 32
height = 32
width = 32
channels = 3

generator_input = keras.Input(shape=(latent_dim,))

x = layers.Dense(128 * 16 * 16)(generator_input)                     <span class="cambriamathin">❶</span>
x = layers.LeakyReLU()(x)                                            <span class="cambriamathin">❶</span>
x = layers.Reshape((16, 16, 128))(x)                                 <span class="cambriamathin">❶</span>

x = layers.Conv2D(256, 5, padding='same')(x)
x = layers.LeakyReLU()(x)

x = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x)     <span class="cambriamathin">❷</span>
x = layers.LeakyReLU()(x)                                            <span class="cambriamathin">❷</span>

x = layers.Conv2D(256, 5, padding='same')(x)
x = layers.LeakyReLU()(x)
x = layers.Conv2D(256, 5, padding='same')(x)
x = layers.LeakyReLU()(x)

x = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x) <span class="cambriamathin">❸</span>
generator = keras.models.Model(generator_input, x)                   <span class="cambriamathin">❸</span><span class="cambriamathin">❹</span>
generator.summary()                                                  <span class="cambriamathin">❸</span></pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> Transforms the input into a 16 × 16 128-channel feature map</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> Upsamples to 32 × 32</p>

    <p class="codeannotation"><span class="cambriamathin1">❸</span> Produces a 32 × 32 1-channel feature map (shape of a CIFAR10 image)</p>

    <p class="codeannotation"><span class="cambriamathin1">❹</span> Instantiates the generator model, which maps the input of shape (latent_dim,) into an image of shape (32, 32, 3)</p>
  </div>

  <h3 class="head1" id="ch08lev2sec19">8.5.4. <a id="ch08lev2sec19__title"></a>The discriminator</h3>

  <p class="noind">Next, you’ll develop a <kbd class="calibre24">discriminator</kbd> model that takes as input a candidate image (real or synthetic) and classifies it into one of two classes: “generated image” or “real image that comes from the training set.”</p>

  <p class="notetitle" id="ch08ex30">Listing 8.30. <a id="ch08ex30__title"></a>The GAN discriminator network</p>
  <pre class="calibre4" id="PLd0e30160">discriminator_input = layers.Input(shape=(height, width, channels))
x = layers.Conv2D(128, 3)(discriminator_input)
x = layers.LeakyReLU()(x)
x = layers.Conv2D(128, 4, strides=2)(x)
x = layers.LeakyReLU()(x)
x = layers.Conv2D(128, 4, strides=2)(x)
x = layers.LeakyReLU()(x)
x = layers.Conv2D(128, 4, strides=2)(x)
x = layers.LeakyReLU()(x)
x = layers.Flatten()(x)

x = layers.Dropout(0.4)(x)                                      <span class="cambriamathin">❶</span>

x = layers.Dense(1, activation='sigmoid')(x)                    <span class="cambriamathin">❷</span>

discriminator = keras.models.Model(discriminator_input, x)      <span class="cambriamathin">❸</span>
discriminator.summary()

discriminator_optimizer = keras.optimizers.RMSprop(
    lr=0.0008,
    clipvalue=1.0,                                              <span class="cambriamathin">❹</span>
    decay=1e-8)                                                 <span class="cambriamathin">❺</span>
discriminator.compile(optimizer=discriminator_optimizer,
                     loss='binary_crossentropy')</pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> One dropout layer: an important trick!</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> Classification layer</p>

    <p class="codeannotation"><span class="cambriamathin1">❸</span> Instantiates the discrim-inator model, which turns a (32, 32, 3) input into a binary classification decision (fake/real)</p>

    <p class="codeannotation"><span class="cambriamathin1">❹</span> Uses gradient clipping (by value) in the optimizer</p>

    <p class="codeannotation"><span class="cambriamathin1">❺</span> To stabilize training, uses learning-rate decay</p>
  </div>

  <h3 class="head1" id="ch08lev2sec20">8.5.5. <a id="ch08lev2sec20__title"></a>The adversarial network</h3>

  <p class="noind">Finally, you’ll set up the GAN, which chains the generator and the discriminator. When trained, this model will move the generator in a direction that improves its ability to fool the discriminator. This model turns latent-space points into a classification decision—“fake” or “real”—and it’s meant to be trained with labels that are always “these are real images.” So, training <kbd class="calibre24">gan</kbd> will update the weights of <kbd class="calibre24">generator</kbd> in a way that makes <kbd class="calibre24">discriminator</kbd> more likely to predict “real” when looking at fake images. It’s very important to note that you set the discriminator to be frozen during training (non-trainable): its weights won’t be updated when training <kbd class="calibre24">gan</kbd>. If the discriminator weights could be updated during this process, then you’d be training the discriminator to always predict “real,” which isn’t what you want!</p>

  <p class="notetitle" id="ch08ex31">Listing 8.31. <a id="ch08ex31__title"></a>Adversarial network</p>
  <pre class="calibre4" id="PLd0e30255">discriminator.trainable = False                       <span class="cambriamathin">❶</span>

gan_input = keras.Input(shape=(latent_dim,))
gan_output = discriminator(generator(gan_input))
gan = keras.models.Model(gan_input, gan_output)

gan_optimizer = keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8)
gan.compile(optimizer=gan_optimizer, loss='binary_crossentropy')</pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> Sets discriminator weights to non-trainable (this will only apply to the gan model)</p>
  </div>

  <h3 class="head1" id="ch08lev2sec21">8.5.6. <a id="ch08lev2sec21__title"></a>How to train your DCGAN</h3>

  <p class="noind">Now you can begin training. To recapitulate, this is what the training loop looks like schematically. For each epoch, you do the following:</p>

  <ol class="calibre23">
    <li class="calibre17">Draw random points in the latent space (random noise).</li>

    <li class="calibre17">Generate images with <kbd class="calibre24">generator</kbd> using this random noise.</li>

    <li class="calibre17">Mix the generated images with real ones.</li>

    <li class="calibre17">Train <kbd class="calibre24">discriminator</kbd> using these mixed images, with corresponding targets: either “real” (for the real images) or “fake” (for the generated images).</li>

    <li class="calibre17">Draw new random points in the latent space.</li>

    <li class="calibre17">Train <kbd class="calibre24">gan</kbd> using these random vectors, with targets that all say “these are real images.” This updates the weights of the generator (only, because the discriminator is frozen inside <kbd class="calibre24">gan</kbd>) to move them toward getting the discriminator to predict “these are real images” for generated images: this trains the generator to fool the discriminator.</li>
  </ol>

  <p class="noind">Let’s implement it.</p>

  <p class="notetitle" id="ch08ex32">Listing 8.32. <a id="ch08ex32__title"></a>Implementing GAN training</p>
  <pre class="calibre4" id="PLd0e30338">import os
from keras.preprocessing import image

(x_train, y_train), (_, _) = keras.datasets.cifar10.load_data()            <span class="cambriamathin">❶</span>

x_train = x_train[y_train.flatten() == 6]                                  <span class="cambriamathin">❷</span>

x_train = x_train.reshape(
    (x_train.shape[0],) +                                                  <span class="cambriamathin">❸</span>
    (height, width, channels)).astype('float32') / 255.                    <span class="cambriamathin">❸</span>

iterations = 10000
batch_size = 20
save_dir = 'your_dir'                                                      <span class="cambriamathin">❹</span>

start = 0
for step in range(iterations):
    random_latent_vectors = np.random.normal(size=(batch_size,             <span class="cambriamathin">❺</span>
                                            latent_dim))                   <span class="cambriamathin">❺</span>

    generated_images = generator.predict(random_latent_vectors)            <span class="cambriamathin">❻</span>

    stop = start + batch_size                                              <span class="cambriamathin">❼</span>
    real_images = x_train[start: stop]                                     <span class="cambriamathin">❼</span>
    combined_images = np.concatenate([generated_images, real_images])      <span class="cambriamathin">❼</span>

    labels = np.concatenate([np.ones((batch_size, 1)),                     <span class="cambriamathin">❽</span>
                             np.zeros((batch_size, 1))])                   <span class="cambriamathin">❽</span>
    labels += 0.05 * np.random.random(labels.shape)                        <span class="cambriamathin">❾</span>

    d_loss = discriminator.train_on_batch(combined_images, labels)         <span class="cambriamathin">❿</span>

    random_latent_vectors = np.random.normal(size=(batch_size,             <span class="cambriamathin">⓫</span>
                                            latent_dim))                   <span class="cambriamathin">⓫</span>

    misleading_targets = np.zeros((batch_size, 1))                         <span class="cambriamathin">⓬</span>

    a_loss = gan.train_on_batch(random_latent_vectors,                     <span class="cambriamathin">⓭</span>
                                misleading_targets)                        <span class="cambriamathin">⓭</span>

    start += batch_size
    if start &gt; len(x_train) - batch_size:
      start = 0
    if step % 100 == 0:                                                    <span class="cambriamathin">⓮</span>
        gan.save_weights('gan.h5')                                         <span class="cambriamathin">⓯</span>

        print('discriminator loss:', d_loss)                               <span class="cambriamathin">⓰</span>
        print('adversarial loss:', a_loss)                                 <span class="cambriamathin">⓰</span>

        img = image.array_to_img(generated_images[0] * 255., scale=False)  <span class="cambriamathin">⓱</span>
        img.save(os.path.join(save_dir,                                    <span class="cambriamathin">⓱</span>
                      'generated_frog' + str(step) + '.png'))              <span class="cambriamathin">⓱</span>

        img = image.array_to_img(real_images[0] * 255., scale=False)       <span class="cambriamathin">⓲</span>
        img.save(os.path.join(save_dir,                                    <span class="cambriamathin">⓲</span>
                      'real_frog' + str(step) + '.png'))                   <span class="cambriamathin">⓲</span></pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> Loads CIFAR10 data</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> Selects frog images (class 6)</p>

    <p class="codeannotation"><span class="cambriamathin1">❸</span> Normalizes data</p>

    <p class="codeannotation"><span class="cambriamathin1">❹</span> Specifies where you want to save generated images</p>

    <p class="codeannotation"><span class="cambriamathin1">❺</span> Samples random points in the latent space</p>

    <p class="codeannotation"><span class="cambriamathin1">❻</span> Decodes them to fake images</p>

    <p class="codeannotation"><span class="cambriamathin1">❼</span> Combines them with real images</p>

    <p class="codeannotation"><span class="cambriamathin1">❽</span> Assembles labels, discrim-inating real from fake images</p>

    <p class="codeannotation"><span class="cambriamathin1">❾</span> Adds random noise to the labels—an important trick!</p>

    <p class="codeannotation"><span class="cambriamathin1">❿</span> Trains the discriminator</p>

    <p class="codeannotation"><span class="cambriamathin1">⓫</span> Samples random points in the latent space</p>

    <p class="codeannotation"><span class="cambriamathin1">⓬</span> Assembles labels that say “these are all real images” (it’s a lie!)</p>

    <p class="codeannotation"><span class="cambriamathin1">⓭</span> Trains the generator (via the gan model, where the discrim-inator weights are frozen)</p>

    <p class="codeannotation"><span class="cambriamathin1">⓮</span> Occasionally saves and plots (every 100 steps)</p>

    <p class="codeannotation"><span class="cambriamathin1">⓯</span> Saves model weights</p>

    <p class="codeannotation"><span class="cambriamathin1">⓰</span> Prints metrics</p>

    <p class="codeannotation"><span class="cambriamathin1">⓱</span> Saves one generated image</p>

    <p class="codeannotation"><span class="cambriamathin1">⓲</span> Saves one real image for comparison</p>
  </div>

  <p class="noind">When training, you may see the adversarial loss begin to increase considerably, while the discriminative loss tends to zero—the discriminator may end up dominating the generator. If that’s the case, try reducing the discriminator learning rate, and increase the dropout rate of the discriminator.</p>

  <p class="noind"></p>

  <p class="notetitle" id="ch08fig18">Figure 8.18. <a id="ch08fig18__title"></a>Play the discriminator: In each column, two images were dreamed up by the GAN, and one image comes from the training set. Can you tell them apart? (Answers: the real images in each column are middle, top, bottom, middle.)</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/08fig18.jpg"/></p>

  <h3 class="head1" id="ch08lev2sec22">8.5.7. <a id="ch08lev2sec22__title"></a>Wrapping up</h3>

  <ul class="calibre16">
    <li class="calibre17"><a id="iddle1199"></a>A GAN consists of a generator network coupled with a discriminator network. The discriminator is trained to differentiate between the output of the generator and real images from a training dataset, and the generator is trained to fool the discriminator. Remarkably, the generator never sees images from the training set directly; the information it has about the data comes from the discriminator.</li>

    <li class="calibre17">GANs are difficult to train, because training a GAN is a dynamic process rather than a simple gradient descent process with a fixed loss landscape. Getting a GAN to train correctly requires using a number of heuristic tricks, as well as extensive tuning.</li>

    <li class="calibre17">GANs can potentially produce highly realistic images. But unlike VAEs, the latent space they learn doesn’t have a neat continuous structure and thus may not be suited for certain practical applications, such as image editing via latent-space concept vectors.</li>
  </ul>

  <p class="noind"></p>
  <hr class="calibre25"/>

  <div class="calibre14">
    <b class="calibre26" id="ch08sb01">Chapter summary</b>

    <ul class="calibre16">
      <li class="calibre17">
        <a id="iddle1216"></a><a id="iddle1285"></a><a id="iddle1343"></a>With creative applications of deep learning, deep networks go beyond annotating existing content and start generating their own. You learned the following:

        <ul class="calibre28">
          <li class="calibre29">How to generate sequence data, one timestep at a time. This is applicable to text generation and also to note-by-note music generation or any other type of timeseries data.</li>

          <li class="calibre29">How DeepDream works: by maximizing convnet layer activations through gradient ascent in input space.</li>

          <li class="calibre29">How to perform style transfer, where a content image and a style image are combined to produce interesting-looking results.</li>

          <li class="calibre29">What GANs and VAEs are, how they can be used to dream up new images, and how latent-space concept vectors can be used for image editing.</li>
        </ul>
      </li>

      <li class="calibre17">These few techniques cover only the basics of this fast-expanding field.</li>
    </ul>
  </div>
  <hr class="calibre25"/>

  <div class="calibre14" id="calibre_pb_30"></div>
</body>
</html>