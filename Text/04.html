<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <title>Deep Learning with Python</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link href="../Styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../Styles/page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
  <h1 class="part" id="ch04">Chapter 4. <a class="calibre3" id="ch04__title"></a>Fundamentals of machine learning</h1>

  <p class="noind1">This chapter covers</p>

  <ul class="calibre16">
    <li class="calibre17">Forms of machine learning beyond classification and regression</li>

    <li class="calibre17">Formal evaluation procedures for machine-learning models</li>

    <li class="calibre17">Preparing data for deep learning</li>

    <li class="calibre17">Feature engineering</li>

    <li class="calibre17">Tackling overfitting</li>

    <li class="calibre17">The universal workflow for approaching machine-learning problems</li>
  </ul>

  <p class="noind">After the three practical examples in <a href="../Text/03.html#ch03">chapter 3</a>, you should be starting to feel familiar with how to approach classification and regression problems using neural networks, and you’ve witnessed the central problem of machine learning: overfitting. This chapter will formalize some of your new intuition into a solid conceptual framework for attacking and solving deep-learning problems. We’ll consolidate all of these concepts—model evaluation, data preprocessing and feature engineering, and tackling overfitting—into a detailed seven-step workflow for tackling any machine-learning task.</p>

  <h2 class="head" id="ch04lev1sec1"><a class="calibre3" id="ch04lev1sec1__title"></a>4.1. Four branches of machine learning</h2>

  <p class="noind"><a id="iddle1020"></a><a id="iddle1097"></a><a id="iddle1241"></a><a id="iddle1402"></a><a id="iddle1546"></a><a id="iddle1548"></a><a id="iddle1549"></a><a id="iddle1550"></a><a id="iddle1733"></a><a id="iddle1873"></a><a id="iddle1880"></a><a id="iddle1932"></a><a id="iddle1937"></a><a id="iddle2028"></a>In our previous examples, you’ve become familiar with three specific types of machine-learning problems: binary classification, multiclass classification, and scalar regression. All three are instances of <i class="calibre5">supervised learning</i>, where the goal is to learn the relationship between training inputs and training targets.</p>

  <p class="noind">Supervised learning is just the tip of the iceberg—machine learning is a vast field with a complex subfield taxonomy. Machine-learning algorithms generally fall into four broad categories, described in the following sections.</p>

  <h3 class="head1" id="ch04lev2sec1">4.1.1. <a id="ch04lev2sec1__title"></a>Supervised learning</h3>

  <p class="noind">This is by far the most common case. It consists of learning to map input data to known targets (also called <i class="calibre5">annotations</i>), given a set of examples (often annotated by humans). All four examples you’ve encountered in this book so far were canonical examples of supervised learning. Generally, almost all applications of deep learning that are in the spotlight these days belong in this category, such as optical character recognition, speech recognition, image classification, and language translation.</p>

  <p class="noind">Although supervised learning mostly consists of classification and regression, there are more exotic variants as well, including the following (with examples):</p>

  <ul class="calibre16">
    <li class="calibre17"><b class="calibre22">Sequence generation—</b> Given a picture, predict a caption describing it. Sequence generation can sometimes be reformulated as a series of classification problems (such as repeatedly predicting a word or token in a sequence).</li>

    <li class="calibre17"><b class="calibre22">Syntax tree prediction—</b> Given a sentence, predict its decomposition into a syntax tree.</li>

    <li class="calibre17"><b class="calibre22">Object detection—</b> Given a picture, draw a bounding box around certain objects inside the picture. This can also be expressed as a classification problem (given many candidate bounding boxes, classify the contents of each one) or as a joint classification and regression problem, where the bounding-box coordinates are predicted via vector regression.</li>

    <li class="calibre17"><b class="calibre22">Image segmentation—</b> Given a picture, draw a pixel-level mask on a specific object.</li>
  </ul>

  <h3 class="head1" id="ch04lev2sec2">4.1.2. <a id="ch04lev2sec2__title"></a>Unsupervised learning</h3>

  <p class="noind">This branch of machine learning consists of finding interesting transformations of the input data without the help of any targets, for the purposes of data visualization, data compression, or data denoising, or to better understand the correlations present in the data at hand. Unsupervised learning is the bread and butter of data analytics, and it’s often a necessary step in better understanding a dataset before attempting to solve a supervised-learning problem. <i class="calibre5">Dimensionality reduction</i> and <i class="calibre5">clustering</i> are well-known categories of unsupervised learning.</p>

  <h3 class="head1" id="ch04lev2sec3">4.1.3. <a id="ch04lev2sec3__title"></a>Self-supervised learning</h3>

  <p class="noind">This is a specific instance of supervised learning, but it’s different enough that it deserves its own category. Self-supervised learning is supervised learning without <a id="iddle1224"></a><a id="iddle1428"></a><a id="iddle1547"></a><a id="iddle1755"></a><a id="iddle1777"></a><a id="iddle1839"></a><a id="iddle1940"></a><a id="iddle1942"></a>human-annotated labels—you can think of it as supervised learning without any humans in the loop. There are still labels involved (because the learning has to be supervised by something), but they’re generated from the input data, typically using a heuristic algorithm.</p>

  <p class="noind">For instance, <i class="calibre5">autoencoders</i> are a well-known instance of self-supervised learning, where the generated targets are the input, unmodified. In the same way, trying to predict the next frame in a video, given past frames, or the next word in a text, given previous words, are instances of self-supervised learning (<i class="calibre5">temporally supervised learning</i>, in this case: supervision comes from future input data). Note that the distinction between supervised, self-supervised, and unsupervised learning can be blurry sometimes—these categories are more of a continuum without solid borders. Self-supervised learning can be reinterpreted as either supervised or unsupervised learning, depending on whether you pay attention to the learning mechanism or to the context of its application.</p>
  <hr class="calibre25"/>

  <p class="notetitle" id="ch04note01">Note</p>

  <p class="noindclose">In this book, we’ll focus specifically on supervised learning, because it’s by far the dominant form of deep learning today, with a wide range of industry applications. We’ll also take a briefer look at self-supervised learning in later chapters.</p>
  <hr class="calibre25"/>

  <h3 class="head1" id="ch04lev2sec4">4.1.4. <a id="ch04lev2sec4__title"></a>Reinforcement learning</h3>

  <p class="noind">Long overlooked, this branch of machine learning recently started to get a lot of attention after Google DeepMind successfully applied it to learning to play Atari games (and, later, learning to play Go at the highest level). In reinforcement learning, an <i class="calibre5">agent</i> receives information about its environment and learns to choose actions that will maximize some reward. For instance, a neural network that “looks” at a video-game screen and outputs game actions in order to maximize its score can be trained via reinforcement learning.</p>

  <p class="noind">Currently, reinforcement learning is mostly a research area and hasn’t yet had significant practical successes beyond games. In time, however, we expect to see reinforcement learning take over an increasingly large range of real-world applications: self-driving cars, robotics, resource management, education, and so on. It’s an idea whose time has come, or will come soon.</p>
  <hr class="calibre25"/>

  <div class="calibre14">
    <b class="calibre26" id="ch04sb01">Classification and regression glossary</b>

    <p class="noind">Classification and regression involve many specialized terms. You’ve come across some of them in earlier examples, and you’ll see more of them in future chapters. They have precise, machine-learning-specific definitions, and you should be familiar with them:</p>

    <ul class="calibre16">
      <li class="calibre17"><i class="calibre5">Sample</i> or <i class="calibre5">input</i>—One data point that goes into your model.</li>

      <li class="calibre17"><i class="calibre5">Prediction</i> or <i class="calibre5">output</i>—What comes out of your model.</li>

      <li class="calibre17"><b class="calibre22">Target—</b> The truth. What your model should ideally have predicted, according to an external source of data.</li>

      <li class="calibre17"><a id="iddle1021"></a><a id="iddle1070"></a><a id="iddle1094"></a><a id="iddle1367"></a><a id="iddle1491"></a><a id="iddle1533"></a><a id="iddle1605"></a><a id="iddle1654"></a><a id="iddle1865"></a><a id="iddle2048"></a><i class="calibre5">Prediction error</i> or <i class="calibre5">loss value</i>—A measure of the distance between your model’s prediction and the target.</li>

      <li class="calibre17"><b class="calibre22">Classes—</b> A set of possible labels to choose from in a classification problem. For example, when classifying cat and dog pictures, “dog” and “cat” are the two classes.</li>

      <li class="calibre17"><b class="calibre22">Label—</b> A specific instance of a class annotation in a classification problem. For instance, if picture #1234 is annotated as containing the class “dog,” then “dog” is a label of picture #1234.</li>

      <li class="calibre17"><i class="calibre5">Ground-truth</i> or <i class="calibre5">annotations</i>—All targets for a dataset, typically collected by humans.</li>

      <li class="calibre17"><b class="calibre22">Binary classification—</b> A classification task where each input sample should be categorized into two exclusive categories.</li>

      <li class="calibre17"><b class="calibre22">Multiclass classification—</b> A classification task where each input sample should be categorized into more than two categories: for instance, classifying handwritten digits.</li>

      <li class="calibre17"><b class="calibre22">Multilabel classification—</b> A classification task where each input sample can be assigned multiple labels. For instance, a given image may contain both a cat and a dog and should be annotated both with the “cat” label and the “dog” label. The number of labels per image is usually variable.</li>

      <li class="calibre17"><b class="calibre22">Scalar regression—</b> A task where the target is a continuous scalar value. Predicting house prices is a good example: the different target prices form a continuous space.</li>

      <li class="calibre17"><b class="calibre22">Vector regression—</b> A task where the target is a set of continuous values: for example, a continuous vector. If you’re doing regression against multiple values (such as the coordinates of a bounding box in an image), then you’re doing vector regression.</li>

      <li class="calibre17"><i class="calibre5">Mini-batch</i> or <i class="calibre5">batch</i>—A small set of samples (typically between 8 and 128) that are processed simultaneously by the model. The number of samples is often a power of 2, to facilitate memory allocation on GPU. When training, a mini-batch is used to compute a single gradient-descent update applied to the weights of the model.</li>
    </ul>
  </div>
  <hr class="calibre25"/>

  <h2 class="head" id="ch04lev1sec2"><a class="calibre3" id="ch04lev1sec2__title"></a>4.2. Evaluating machine-learning models</h2>

  <p class="noind"><a id="iddle1424"></a><a id="iddle1553"></a><a id="iddle1555"></a><a id="iddle1556"></a><a id="iddle1557"></a><a id="iddle1764"></a><a id="iddle1958"></a><a id="iddle2004"></a><a id="iddle2033"></a><a id="iddle2037"></a>In the three examples presented in <a href="../Text/03.html#ch03">chapter 3</a>, we split the data into a training set, a validation set, and a test set. The reason not to evaluate the models on the same data they were trained on quickly became evident: after just a few epochs, all three models began to <i class="calibre5">overfit</i>. That is, their performance on never-before-seen data started stalling (or worsening) compared to their performance on the training data—which always improves as training progresses.</p>

  <p class="noind">In machine learning, the goal is to achieve models that <i class="calibre5">generalize</i>—that perform well on never-before-seen data—and overfitting is the central obstacle. You can only control that which you can observe, so it’s crucial to be able to reliably measure the generalization power of your model. The following sections look at strategies for mitigating overfitting and maximizing generalization. In this section, we’ll focus on how to measure generalization: how to evaluate machine-learning models.</p>

  <h3 class="head1" id="ch04lev2sec5">4.2.1. <a id="ch04lev2sec5__title"></a>Training, validation, and test sets</h3>

  <p class="noind">Evaluating a model always boils down to splitting the available data into three sets: training, validation, and test. You train on the training data and evaluate your model on the validation data. Once your model is ready for prime time, you test it one final time on the test data.</p>

  <p class="noind">You may ask, why not have two sets: a training set and a test set? You’d train on the training data and evaluate on the test data. Much simpler!</p>

  <p class="noind">The reason is that developing a model always involves tuning its configuration: for example, choosing the number of layers or the size of the layers (called the <i class="calibre5">hyper-parameters</i> of the model, to distinguish them from the <i class="calibre5">parameters</i>, which are the network’s weights). You do this tuning by using as a feedback signal the performance of the model on the validation data. In essence, this tuning is a form of <i class="calibre5">learning</i>: a search for a good configuration in some parameter space. As a result, tuning the configuration of the model based on its performance on the validation set can quickly result in <i class="calibre5">overfitting to the validation set</i>, even though your model is never directly trained on it.</p>

  <p class="noind">Central to this phenomenon is the notion of <i class="calibre5">information leaks</i>. Every time you tune a hyperparameter of your model based on the model’s performance on the validation set, some information about the validation data leaks into the model. If you do this only once, for one parameter, then very few bits of information will leak, and your validation set will remain reliable to evaluate the model. But if you repeat this many times—running one experiment, evaluating on the validation set, and modifying your model as a result—then you’ll leak an increasingly significant amount of information about the validation set into the model.</p>

  <p class="noind">At the end of the day, you’ll end up with a model that performs artificially well on the validation data, because that’s what you optimized it for. You care about performance on completely new data, not the validation data, so you need to use a completely different, never-before-seen dataset to evaluate the model: the test dataset. Your model shouldn’t have had access to <i class="calibre5">any</i> information about the test set, even indirectly. <a id="iddle1177"></a><a id="iddle1179"></a><a id="iddle1384"></a><a id="iddle1391"></a><a id="iddle1959"></a><a id="iddle2005"></a><a id="iddle2034"></a>If anything about the model has been tuned based on test set performance, then your measure of generalization will be flawed.</p>

  <p class="noind">Splitting your data into training, validation, and test sets may seem straightforward, but there are a few advanced ways to do it that can come in handy when little data is available. Let’s review three classic evaluation recipes: simple hold-out validation, K-fold validation, and iterated K-fold validation with shuffling.</p>

  <p class="notetitle" id="ch04lev3sec1"><a id="ch04lev3sec1__title"></a>Simple hold-out validation</p>

  <p class="noind">Set apart some fraction of your data as your test set. Train on the remaining data, and evaluate on the test set. As you saw in the previous sections, in order to prevent information leaks, you shouldn’t tune your model based on the test set, and therefore you should <i class="calibre5">also</i> reserve a validation set.</p>

  <p class="noind">Schematically, hold-out validation looks like <a href="#ch04fig01">figure 4.1</a>. The following listing shows a simple implementation.</p>

  <p class="notetitle" id="ch04fig01">Figure 4.1. <a id="ch04fig01__title"></a>Simple hold-out validation split</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/04fig01.jpg"/></p>

  <p class="notetitle" id="ch04ex01">Listing 4.1. <a id="ch04ex01__title"></a>Hold-out validation</p>
  <pre class="calibre4" id="PLd0e10471">num_validation_samples = 10000

np.random.shuffle(data)                                    <span class="cambriamathin">❶</span>

validation_data = data[:num_validation_samples]            <span class="cambriamathin">❷</span>
data = data[num_validation_samples:]

training_data = data[:]                                    <span class="cambriamathin">❸</span>

model = get_model()                                        <span class="cambriamathin">❹</span>
model.train(training_data)                                 <span class="cambriamathin">❹</span>
validation_score = model.evaluate(validation_data)         <span class="cambriamathin">❹</span>

# At this point you can tune your model,
# retrain it, evaluate it, tune it again...

model = get_model()                                        <span class="cambriamathin">❺</span>
model.train(np.concatenate([training_data,                 <span class="cambriamathin">❺</span>
                            validation_data]))             <span class="cambriamathin">❺</span>
test_score = model.evaluate(test_data)                     <span class="cambriamathin">❺</span></pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> Shuffling the data is usually appropriate.</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> Defines the validation set</p>

    <p class="codeannotation"><a id="iddle1002"></a><a id="iddle1485"></a><a id="iddle1765"></a><a id="iddle1961"></a><a id="iddle2007"></a><a id="iddle2036"></a><span class="cambriamathin1">❸</span> Defines the training set</p>

    <p class="codeannotation"><span class="cambriamathin1">❹</span> Trains a model on the training data, and evaluates it on the validation data</p>

    <p class="codeannotation"><span class="cambriamathin1">❺</span> Once you’ve tuned your hyperparameters, it’s common to train your final model from scratch on all non-test data available.</p>
  </div>

  <p class="noind">This is the simplest evaluation protocol, and it suffers from one flaw: if little data is available, then your validation and test sets may contain too few samples to be statistically representative of the data at hand. This is easy to recognize: if different random shuffling rounds of the data before splitting end up yielding very different measures of model performance, then you’re having this issue. K-fold validation and iterated K-fold validation are two ways to address this, as discussed next.</p>

  <p class="notetitle" id="ch04lev3sec2"><a id="ch04lev3sec2__title"></a>K-fold validation</p>

  <p class="noind">With this approach, you split your data into <i class="calibre5">K</i> partitions of equal size. For each partition <kbd class="calibre24">i</kbd>, train a model on the remaining <i class="calibre5">K</i> – 1 partitions, and evaluate it on partition <kbd class="calibre24">i</kbd>. Your final score is then the averages of the <i class="calibre5">K</i> scores obtained. This method is helpful when the performance of your model shows significant variance based on your train-test split. Like hold-out validation, this method doesn’t exempt you from using a distinct validation set for model calibration.</p>

  <p class="noind">Schematically, K-fold cross-validation looks like <a href="#ch04fig02">figure 4.2</a>. <a href="#ch04ex02">Listing 4.2</a> shows a simple implementation.</p>

  <p class="notetitle" id="ch04fig02">Figure 4.2. <a id="ch04fig02__title"></a>Three-fold validation</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/04fig02_altB.jpg"/></p>

  <p class="notetitle" id="ch04ex02">Listing 4.2. <a id="ch04ex02__title"></a>K-fold cross-validation</p>
  <pre class="calibre4" id="PLd0e10650">k = 4
num_validation_samples = len(data) // k

np.random.shuffle(data)

validation_scores = []
for fold in range(k):
    validation_data = data[num_validation_samples * fold:             <span class="cambriamathin">❶</span>
     num_validation_samples * (fold + 1)]                             <span class="cambriamathin">❶</span>
    training_data = data[:num_validation_samples * fold] +            <span class="cambriamathin">❷</span>
     data[num_validation_samples * (fold + 1):]                       <span class="cambriamathin">❷</span>
    model = get_model()                                               <span class="cambriamathin">❸</span>
    model.train(training_data)
    validation_score = model.evaluate(validation_data)
    validation_scores.append(validation_score)

validation_score = np.average(validation_scores)                      <span class="cambriamathin">❹</span>

model = get_model()                                                   <span class="cambriamathin">❺</span>
model.train(data)                                                     <span class="cambriamathin">❺</span>
test_score = model.evaluate(test_data)                                <span class="cambriamathin">❺</span></pre>

  <div class="annotations">
    <p class="codeannotation"><a id="iddle1032"></a><a id="iddle1161"></a><a id="iddle1178"></a><a id="iddle1188"></a><a id="iddle1268"></a><a id="iddle1554"></a><a id="iddle1812"></a><a id="iddle1897"></a><a id="iddle1941"></a><a id="iddle1960"></a><a id="iddle2006"></a><a id="iddle2032"></a><a id="iddle2035"></a><span class="cambriamathin1">❶</span> Selects the validation-data partition</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> Uses the remainder of the data as training data. Note that the + operator is list concatenation, not summation.</p>

    <p class="codeannotation"><span class="cambriamathin1">❸</span> Creates a brand-new instance of the model (untrained)</p>

    <p class="codeannotation"><span class="cambriamathin1">❹</span> Validation score: average of the validation scores of the k folds</p>

    <p class="codeannotation"><span class="cambriamathin1">❺</span> Trains the final model on all non-test data available</p>
  </div>

  <p class="notetitle" id="ch04lev3sec3"><a id="ch04lev3sec3__title"></a>Iterated K-fold validation with shuffling</p>

  <p class="noind">This one is for situations in which you have relatively little data available and you need to evaluate your model as precisely as possible. I’ve found it to be extremely helpful in Kaggle competitions. It consists of applying K-fold validation multiple times, shuffling the data every time before splitting it <i class="calibre5">K</i> ways. The final score is the average of the scores obtained at each run of K-fold validation. Note that you end up training and evaluating <i class="calibre5">P</i> × <i class="calibre5">K</i> models (where <i class="calibre5">P</i> is the number of iterations you use), which can be very expensive.</p>

  <h3 class="head1" id="ch04lev2sec6">4.2.2. <a id="ch04lev2sec6__title"></a>Things to keep in mind</h3>

  <p class="noind">Keep an eye out for the following when you’re choosing an evaluation protocol:</p>

  <ul class="calibre16">
    <li class="calibre17"><b class="calibre22">Data representativeness—</b> You want both your training set and test set to be representative of the data at hand. For instance, if you’re trying to classify images of digits, and you’re starting from an array of samples where the samples are ordered by their class, taking the first 80% of the array as your training set and the remaining 20% as your test set will result in your training set containing only classes 0–7, whereas your test set contains only classes 8–9. This seems like a ridiculous mistake, but it’s surprisingly common. For this reason, you usually should <i class="calibre5">randomly shuffle</i> your data before splitting it into training and test sets.</li>

    <li class="calibre17"><b class="calibre22">The arrow of time—</b> If you’re trying to predict the future given the past (for example, tomorrow’s weather, stock movements, and so on), you should <i class="calibre5">not</i> randomly shuffle your data before splitting it, because doing so will create a <i class="calibre5">temporal leak</i>: your model will effectively be trained on data from the future. In such situations, you should always make sure all data in your test set is <i class="calibre5">posterior</i> to the data in the training set.</li>

    <li class="calibre17"><b class="calibre22">Redundancy in your data—</b> <a id="iddle1152"></a><a id="iddle1153"></a><a id="iddle1159"></a><a id="iddle1183"></a><a id="iddle1260"></a><a id="iddle1275"></a><a id="iddle1278"></a><a id="iddle1296"></a><a id="iddle1377"></a><a id="iddle1385"></a><a id="iddle1551"></a><a id="iddle1558"></a><a id="iddle1559"></a><a id="iddle1672"></a><a id="iddle1674"></a><a id="iddle1675"></a><a id="iddle1725"></a><a id="iddle1742"></a><a id="iddle1780"></a><a id="iddle1782"></a><a id="iddle2043"></a><a id="iddle2049"></a>If some data points in your data appear twice (fairly common with real-world data), then shuffling the data and splitting it into a training set and a validation set will result in redundancy between the training and validation sets. In effect, you’ll be testing on part of your training data, which is the worst thing you can do! Make sure your training set and validation set are disjoint.</li>
  </ul>

  <h2 class="head" id="ch04lev1sec3"><a class="calibre3" id="ch04lev1sec3__title"></a>4.3. Data preprocessing, feature engineering, and feature learning</h2>

  <p class="noind"><a id="iddle1155"></a><a id="iddle1673"></a><a id="iddle2042"></a>In addition to model evaluation, an important question we must tackle before we dive deeper into model development is the following: how do you prepare the input data and targets before feeding them into a neural network? Many data-preprocessing and feature-engineering techniques are domain specific (for example, specific to text data or image data); we’ll cover those in the following chapters as we encounter them in practical examples. For now, we’ll review the basics that are common to all data domains.</p>

  <h3 class="head1" id="ch04lev2sec7">4.3.1. <a id="ch04lev2sec7__title"></a>Data preprocessing for neural networks</h3>

  <p class="noind">Data preprocessing aims at making the raw data at hand more amenable to neural networks. This includes vectorization, normalization, handling missing values, and feature extraction.</p>

  <p class="notetitle" id="ch04lev3sec4"><a id="ch04lev3sec4__title"></a>Vectorization</p>

  <p class="noind">All inputs and targets in a neural network must be tensors of floating-point data (or, in specific cases, tensors of integers). Whatever data you need to process—sound, images, text—you must first turn into tensors, a step called <i class="calibre5">data vectorization</i>. For instance, in the two previous text-classification examples, we started from text represented as lists of integers (standing for sequences of words), and we used one-hot encoding to turn them into a tensor of <kbd class="calibre24">float32</kbd> data. In the examples of classifying digits and predicting house prices, the data already came in vectorized form, so you were able to skip this step.</p>

  <p class="notetitle" id="ch04lev3sec5"><a id="ch04lev3sec5__title"></a>Value normalization</p>

  <p class="noind">In the digit-classification example, you started from image data encoded as integers in the 0–255 range, encoding grayscale values. Before you fed this data into your network, you had to cast it to <kbd class="calibre24">float32</kbd> and divide by 255 so you’d end up with floating-point values in the 0–1 range. Similarly, when predicting house prices, you started from features that took a variety of ranges—some features had small floating-point values, others had fairly large integer values. Before you fed this data into your network, you had to normalize each feature independently so that it had a standard deviation of 1 and a mean of 0.</p>

  <p class="noind">In general, it isn’t safe to feed into a neural network data that takes relatively large values (for example, multidigit integers, which are much larger than the initial values taken by the weights of a network) or data that is heterogeneous (for example, data where one feature is in the range 0–1 and another is in the range 100–200). Doing so can trigger large gradient updates that will prevent the network from converging. To make learning easier for your network, your data should have the following characteristics:</p>

  <ul class="calibre16">
    <li class="calibre17"><b class="calibre22">Take small values—</b> Typically, most values should be in the 0–1 range.</li>

    <li class="calibre17"><b class="calibre22">Be homogenous—</b> That is, all features should take values in roughly the same range.</li>
  </ul>

  <p class="noind">Additionally, the following stricter normalization practice is common and can help, although it isn’t always necessary (for example, you didn’t do this in the digit-classification example):</p>

  <ul class="calibre16">
    <li class="calibre17">Normalize each feature independently to have a mean of 0.</li>

    <li class="calibre17">Normalize each feature independently to have a standard deviation of 1.</li>
  </ul>

  <p class="noind">This is easy to do with Numpy arrays:</p>
  <pre class="calibre4" id="PLd0e11177">x -= x.mean(axis=0)          <span class="cambriamathin">❶</span>
x /= x.std(axis=0)</pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> Assuming x is a 2D data matrix of shape (samples, features)</p>
  </div>

  <p class="notetitle" id="ch04lev3sec6"><a id="ch04lev3sec6__title"></a>Handling missing values</p>

  <p class="noind">You may sometimes have missing values in your data. For instance, in the house-price example, the first feature (the column of index 0 in the data) was the per capita crime rate. What if this feature wasn’t available for all samples? You’d then have missing values in the training or test data.</p>

  <p class="noind">In general, with neural networks, it’s safe to input missing values as 0, with the condition that 0 isn’t already a meaningful value. The network will learn from exposure to the data that the value 0 means <i class="calibre5">missing data</i> and will start ignoring the value.</p>

  <p class="noind">Note that if you’re expecting missing values in the test data, but the network was trained on data without any missing values, the network won’t have learned to ignore missing values! In this situation, you should artificially generate training samples with missing entries: copy some training samples several times, and drop some of the features that you expect are likely to be missing in the test data.</p>

  <h3 class="head1" id="ch04lev2sec8">4.3.2. <a id="ch04lev2sec8__title"></a>Feature engineering</h3>

  <p class="noind"><i class="calibre5">Feature engineering</i> is the process of using your own knowledge about the data and about the machine-learning algorithm at hand (in this case, a neural network) to make the algorithm work better by applying hardcoded (nonlearned) transformations to the data before it goes into the model. In many cases, it isn’t reasonable to expect a machine-learning model to be able to learn from completely arbitrary data. The data needs to be presented to the model in a way that will make the model’s job easier.</p>

  <p class="noind">Let’s look at an intuitive example. Suppose you’re trying to develop a model that can take as input an image of a clock and can output the time of day (see <a href="#ch04fig03">figure 4.3</a>).</p>

  <p class="notetitle" id="ch04fig03">Figure 4.3. <a id="ch04fig03__title"></a>Feature engineering for reading the time on a clock</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/04fig03.jpg"/></p>

  <p class="noind"><a id="iddle1316"></a><a id="iddle1570"></a><a id="iddle1573"></a><a id="iddle1600"></a><a id="iddle1746"></a><a id="iddle1759"></a><a id="iddle2022"></a><a id="iddle2025"></a>If you choose to use the raw pixels of the image as input data, then you have a difficult machine-learning problem on your hands. You’ll need a convolutional neural network to solve it, and you’ll have to expend quite a bit of computational resources to train the network.</p>

  <p class="noind">But if you already understand the problem at a high level (you understand how humans read time on a clock face), then you can come up with much better input features for a machine-learning algorithm: for instance, it’s easy to write a five-line Python script to follow the black pixels of the clock hands and output the (x, y) coordinates of the tip of each hand. Then a simple machine-learning algorithm can learn to associate these coordinates with the appropriate time of day.</p>

  <p class="noind">You can go even further: do a coordinate change, and express the (x, y) coordinates as polar coordinates with regard to the center of the image. Your input will become the angle <kbd class="calibre24">theta</kbd> of each clock hand. At this point, your features are making the problem so easy that no machine learning is required; a simple rounding operation and dictionary lookup are enough to recover the approximate time of day.</p>

  <p class="noind">That’s the essence of feature engineering: making a problem easier by expressing it in a simpler way. It usually requires understanding the problem in depth.</p>

  <p class="noind">Before deep learning, feature engineering used to be critical, because classical shallow algorithms didn’t have hypothesis spaces rich enough to learn useful features by themselves. The way you presented the data to the algorithm was essential to its success. For instance, before convolutional neural networks became successful on the MNIST digit-classification problem, solutions were typically based on hardcoded features such as the number of loops in a digit image, the height of each digit in an image, a histogram of pixel values, and so on.</p>

  <p class="noind">Fortunately, modern deep learning removes the need for most feature engineering, because neural networks are capable of automatically extracting useful features from raw data. Does this mean you don’t have to worry about feature engineering as long as you’re using deep neural networks? No, for two reasons:</p>

  <ul class="calibre16">
    <li class="calibre17">Good features still allow you to solve problems more elegantly while using fewer resources. For instance, it would be ridiculous to solve the problem of reading a clock face using a convolutional neural network.</li>

    <li class="calibre17">Good features let you solve a problem with far less data. The ability of deep-learning models to learn features on their own relies on having lots of training data available; if you have only a few samples, then the information value in their features becomes critical.</li>
  </ul>

  <h2 class="head" id="ch04lev1sec4"><a class="calibre3" id="ch04lev1sec4__title"></a>4.4. Overfitting and underfitting</h2>

  <p class="noind">In all three examples in the previous chapter—predicting movie reviews, topic classification, and house-price regression—the performance of the model on the held-out validation data always peaked after a few epochs and then began to degrade: the model quickly started to <i class="calibre5">overfit</i> to the training data. Overfitting happens in every machine-learning problem. Learning how to deal with overfitting is essential to mastering machine learning.</p>

  <p class="noind">The fundamental issue in machine learning is the tension between optimization and generalization. <i class="calibre5">Optimization</i> refers to the process of adjusting a model to get the best performance possible on the training data (the <i class="calibre5">learning</i> in <i class="calibre5">machine learning</i>), whereas <i class="calibre5">generalization</i> refers to how well the trained model performs on data it has never seen before. The goal of the game is to get good generalization, of course, but you don’t control generalization; you can only adjust the model based on its training data.</p>

  <p class="noind">At the beginning of training, optimization and generalization are correlated: the lower the loss on training data, the lower the loss on test data. While this is happening, your model is said to be <i class="calibre5">underfit</i>: there is still progress to be made; the network hasn’t yet modeled all relevant patterns in the training data. But after a certain number of iterations on the training data, generalization stops improving, and validation metrics stall and then begin to degrade: the model is starting to overfit. That is, it’s beginning to learn patterns that are specific to the training data but that are misleading or irrelevant when it comes to new data.</p>

  <p class="noind">To prevent a model from learning misleading or irrelevant patterns found in the training data, <i class="calibre5">the best solution is to get more training data</i>. A model trained on more data will naturally generalize better. When that isn’t possible, the next-best solution is to modulate the quantity of information that your model is allowed to store or to add constraints on what information it’s allowed to store. If a network can only afford to memorize a small number of patterns, the optimization process will force it to focus on the most prominent patterns, which have a better chance of generalizing well.</p>

  <p class="noind">The processing of fighting overfitting this way is called <i class="calibre5">regularization</i>. Let’s review some of the most common regularization techniques and apply them in practice to improve the movie-classification model from <a href="../Text/03.html#ch03lev1sec4">section 3.4</a>.</p>

  <h3 class="head1" id="ch04lev2sec9">4.4.1. <a id="ch04lev2sec9__title"></a>Reducing the network’s size</h3>

  <p class="noind">The simplest way to prevent overfitting is to reduce the size of the model: the number of learnable parameters in the model (which is determined by the number of layers and the number of units per layer). In deep learning, the number of learnable parameters in a model is often referred to as the model’s <i class="calibre5">capacity</i>. Intuitively, a model with more parameters has more <i class="calibre5">memorization capacity</i> and therefore can easily learn a perfect dictionary-like mapping between training samples and their targets—a mapping without any generalization power. For instance, a model with 500,000 binary parameters could easily be made to learn the class of every digit in the MNIST training set: we’d need only 10 binary parameters for each of the 50,000 digits. But such a model would be useless for classifying new digit samples. Always keep this in mind: deep-learning models tend to be good at fitting to the training data, but the real challenge is generalization, not fitting.</p>

  <p class="noind">On the other hand, if the network has limited memorization resources, it won’t be able to learn this mapping as easily; thus, in order to minimize its loss, it will have to resort to learning compressed representations that have predictive power regarding the targets—precisely the type of representations we’re interested in. At the same time, keep in mind that you should use models that have enough parameters that they don’t underfit: your model shouldn’t be starved for memorization resources. There is a compromise to be found between <i class="calibre5">too much capacity</i> and <i class="calibre5">not enough capacity</i>.</p>

  <p class="noind">Unfortunately, there is no magical formula to determine the right number of layers or the right size for each layer. You must evaluate an array of different architectures (on your validation set, not on your test set, of course) in order to find the correct model size for your data. The general workflow to find an appropriate model size is to start with relatively few layers and parameters, and increase the size of the layers or add new layers until you see diminishing returns with regard to validation loss.</p>

  <p class="noind">Let’s try this on the movie-review classification network. The original network is shown next.</p>

  <p class="notetitle" id="ch04ex03">Listing 4.3. <a id="ch04ex03__title"></a>Original model</p>
  <pre class="calibre4" id="PLd0e11422">from keras import models
from keras import layers

model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))</pre>

  <p class="noind">Now let’s try to replace it with this smaller network.</p>

  <p class="notetitle" id="ch04ex04">Listing 4.4. <a id="ch04ex04__title"></a>Version of the model with lower capacity</p>
  <pre class="calibre4" id="PLd0e11434">model = models.Sequential()
model.add(layers.Dense(4, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(4, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))</pre>

  <p class="noind"><a href="#ch04fig04">Figure 4.4</a> shows a comparison of the validation losses of the original network and the smaller network. The dots are the validation loss values of the smaller network, and the crosses are the initial network (remember, a lower validation loss signals a better model).</p>

  <p class="noind"></p>

  <p class="notetitle" id="ch04fig04">Figure 4.4. <a id="ch04fig04__title"></a>Effect of model capacity on validation loss: trying a smaller model</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/04fig04.jpg"/></p>

  <p class="noind"><a id="iddle1488"></a><a id="iddle1489"></a><a id="iddle1572"></a><a id="iddle1736"></a><a id="iddle1758"></a><a id="iddle2024"></a><a id="iddle2064"></a><a id="iddle2065"></a>As you can see, the smaller network starts overfitting later than the reference network (after six epochs rather than four), and its performance degrades more slowly once it starts overfitting.</p>

  <p class="noind">Now, for kicks, let’s add to this benchmark a network that has much more capacity—far more than the problem warrants.</p>

  <p class="notetitle" id="ch04ex05">Listing 4.5. <a id="ch04ex05__title"></a>Version of the model with higher capacity</p>
  <pre class="calibre4" id="PLd0e11521">model = models.Sequential()
model.add(layers.Dense(512, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))</pre>

  <p class="noind"><a href="#ch04fig05">Figure 4.5</a> shows how the bigger network fares compared to the reference network. The dots are the validation loss values of the bigger network, and the crosses are the initial network.</p>

  <p class="notetitle" id="ch04fig05">Figure 4.5. <a id="ch04fig05__title"></a>Effect of model capacity on validation loss: trying a bigger model</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/04fig05.jpg"/></p>

  <p class="noind">The bigger network starts overfitting almost immediately, after just one epoch, and it overfits much more severely. Its validation loss is also noisier.</p>

  <p class="noind">Meanwhile, <a href="#ch04fig06">figure 4.6</a> shows the training losses for the two networks. As you can see, the bigger network gets its training loss near zero very quickly. The more capacity the network has, the more quickly it can model the training data (resulting in a low training loss), but the more susceptible it is to overfitting (resulting in a large difference between the training and validation loss).</p>

  <p class="notetitle" id="ch04fig06">Figure 4.6. <a id="ch04fig06__title"></a>Effect of model capacity on training loss: trying a bigger model</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/04fig06.jpg"/></p>

  <h3 class="head1" id="ch04lev2sec10">4.4.2. <a id="ch04lev2sec10__title"></a>Adding weight regularization</h3>

  <p class="noind">You may be familiar with the principle of <i class="calibre5">Occam’s razor</i>: given two explanations for something, the explanation most likely to be correct is the simplest one—the one that makes fewer assumptions. This idea also applies to the models learned by neural networks: given some training data and a network architecture, multiple sets of weight values (multiple <i class="calibre5">models</i>) could explain the data. Simpler models are less likely to overfit than complex ones.</p>

  <p class="noind">A <i class="calibre5">simple model</i> in this context is a model where the distribution of parameter values has less entropy (or a model with fewer parameters, as you saw in the previous section). Thus a common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weights to take only small values, which makes the distribution of weight values more <i class="calibre5">regular</i>. This is called <i class="calibre5">weight regularization</i>, and it’s done by adding to the loss function of the network a <i class="calibre5">cost</i> associated with having large weights. This cost comes in two flavors:</p>

  <ul class="calibre16">
    <li class="calibre17"><b class="calibre22">L1 regularization—</b> The cost added is proportional to the <i class="calibre5">absolute value of the weight coefficients</i> (the <i class="calibre5">L1 norm</i> of the weights).</li>

    <li class="calibre17"><b class="calibre22">L2 regularization—</b> The cost added is proportional to the <i class="calibre5">square of the value of the weight coefficients</i> (the <i class="calibre5">L2 norm</i> of the weights). L2 regularization is also called <i class="calibre5">weight decay</i> in the context of neural networks. Don’t let the different name confuse you: weight decay is mathematically the same as L2 regularization.</li>
  </ul>

  <p class="noind"><a id="iddle1382"></a><a id="iddle1571"></a><a id="iddle1757"></a><a id="iddle2023"></a>In Keras, weight regularization is added by passing <i class="calibre5">weight regularizer instances</i> to layers as keyword arguments. Let’s add L2 weight regularization to the movie-review classification network.</p>

  <p class="notetitle" id="ch04ex06">Listing 4.6. <a id="ch04ex06__title"></a>Adding L2 weight regularization to the model</p>
  <pre class="calibre4" id="PLd0e11676">from keras import regularizers

model = models.Sequential()
model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),
                       activation='relu', input_shape=(10000,)))
model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),
                       activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))</pre>

  <p class="noind"><kbd class="calibre24">l2(0.001)</kbd> means every coefficient in the weight matrix of the layer will add <kbd class="calibre24">0.001 * weight_coefficient_value</kbd> to the total loss of the network. Note that because this penalty is <i class="calibre5">only added at training time</i>, the loss for this network will be much higher at training than at test time.</p>

  <p class="noind"><a href="#ch04fig07">Figure 4.7</a> shows the impact of the L2 regularization penalty. As you can see, the model with L2 regularization (dots) has become much more resistant to overfitting than the reference model (crosses), even though both models have the same number of parameters.</p>

  <p class="notetitle" id="ch04fig07">Figure 4.7. <a id="ch04fig07__title"></a>Effect of L2 weight regularization on validation loss</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/04fig07.jpg"/></p>

  <p class="noind">As an alternative to L2 regularization, you can use one of the following Keras weight regularizers.</p>

  <p class="notetitle" id="ch04ex07">Listing 4.7. <a id="ch04ex07__title"></a>Different weight regularizers available in Keras</p>
  <pre class="calibre4" id="PLd0e11717">from keras import regularizers

regularizers.l1(0.001)                         <span class="cambriamathin">❶</span>

regularizers.l1_l2(l1=0.001, l2=0.001)         <span class="cambriamathin">❷</span></pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> L1 regularization</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> Simultaneous L1 and L2 regularization</p>
  </div>

  <h3 class="head1" id="ch04lev2sec11">4.4.3. <a id="ch04lev2sec11__title"></a>Adding dropout</h3>

  <p class="noind"><i class="calibre5">Dropout</i> is one of the most effective and most commonly used regularization techniques for neural networks, developed by Geoff Hinton and his students at the University of Toronto. Dropout, applied to a layer, consists of randomly <i class="calibre5">dropping out</i> (setting to zero) a number of output features of the layer during training. Let’s say a given layer would normally return a vector [0.2, 0.5, 1.3, 0.8, 1.1] for a given input sample during training. After applying dropout, this vector will have a few zero entries distributed at random: for example, [0, 0.5, 1.3, 0, 1.1]. The <i class="calibre5">dropout rate</i> is the fraction of the features that are zeroed out; it’s usually set between 0.2 and 0.5. At test time, no units are dropped out; instead, the layer’s output values are scaled down by a factor equal to the dropout rate, to balance for the fact that more units are active than at training time.</p>

  <p class="noind">Consider a Numpy matrix containing the output of a layer, <kbd class="calibre24">layer_output</kbd>, of shape <kbd class="calibre24">(batch_size, features)</kbd>. At training time, we zero out at random a fraction of the values in the matrix:</p>
  <pre class="calibre4" id="PLd0e11777">layer_output *= np.random.randint(0, high=2, size=layer_output.shape)    <span class="cambriamathin">❶</span></pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> At training time, drops out 50% of the units in the output</p>
  </div>

  <p class="noind">At test time, we scale down the output by the dropout rate. Here, we scale by 0.5 (because we previously dropped half the units):</p>
  <pre class="calibre4" id="PLd0e11800">layer_output *= 0.5          <span class="cambriamathin">❶</span></pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> At test time</p>
  </div>

  <p class="noind">Note that this process can be implemented by doing both operations at training time and leaving the output unchanged at test time, which is often the way it’s implemented in practice (see <a href="#ch04fig08">figure 4.8</a>):</p>
  <pre class="calibre4" id="PLd0e11826">layer_output *= np.random.randint(0, high=2, size=layer_output.shape)    <span class="cambriamathin">❶</span>
layer_output /= 0.5                                                      <span class="cambriamathin">❷</span></pre>

  <div class="annotations">
    <p class="codeannotation"><span class="cambriamathin1">❶</span> At training time</p>

    <p class="codeannotation"><span class="cambriamathin1">❷</span> Note that we’re scaling up rather scaling down in this case.</p>
  </div>

  <p class="notetitle" id="ch04fig08">Figure 4.8. <a id="ch04fig08__title"></a>Dropout applied to an activation matrix at training time, with rescaling happening during training. At test time, the activation matrix is unchanged.</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/04fig08.jpg"/></p>

  <p class="noind"><a id="iddle1039"></a><a id="iddle1190"></a><a id="iddle1574"></a><a id="iddle1576"></a><a id="iddle1579"></a><a id="iddle1723"></a><a id="iddle1796"></a><a id="iddle2087"></a><a id="iddle2089"></a><a id="iddle2092"></a>This technique may seem strange and arbitrary. Why would this help reduce overfitting? Hinton says he was inspired by, among other things, a fraud-prevention mechanism used by banks. In his own words, “I went to my bank. The tellers kept changing and I asked one of them why. He said he didn’t know but they got moved around a lot. I figured it must be because it would require cooperation between employees to successfully defraud the bank. This made me realize that randomly removing a different subset of neurons on each example would prevent conspiracies and thus reduce overfitting.”<sup class="calibre19">[<a href="#ch04fn01" class="calibre13">1</a>]</sup> The core idea is that introducing noise in the output values of a layer can break up happenstance patterns that aren’t significant (what Hinton refers to as <i class="calibre5">conspiracies</i>), which the network will start memorizing if no noise is present.</p>

  <blockquote class="smaller">
    <p class="calibre20"><sup class="calibre19"><a id="ch04fn01" class="calibre13">1</a></sup></p>

    <div class="calibre21">
      See the Reddit thread “AMA: We are the Google Brain team. We’d love to answer your questions about machine learning,” <a href="http://mng.bz/XrsS">http://mng.bz/XrsS</a>.
    </div>
  </blockquote>

  <p class="noind">In Keras, you can introduce dropout in a network via the <kbd class="calibre24">Dropout</kbd> layer, which is applied to the output of the layer right before it:</p>
  <pre class="calibre4" id="PLd0e11962">model.add(layers.Dropout(0.5))</pre>

  <p class="noind">Let’s add two <kbd class="calibre24">Dropout</kbd> layers in the IMDB network to see how well they do at reducing overfitting.</p>

  <p class="notetitle" id="ch04ex08">Listing 4.8. <a id="ch04ex08__title"></a>Adding dropout to the IMDB network</p>
  <pre class="calibre4" id="PLd0e11977">model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(1, activation='sigmoid'))</pre>

  <p class="noind"><a href="#ch04fig09">Figure 4.9</a> shows a plot of the results. Again, this is a clear improvement over the reference network.</p>

  <p class="notetitle" id="ch04fig09">Figure 4.9. <a id="ch04fig09__title"></a>Effect of dropout on validation loss</p>

  <p class="center2"><img alt="" class="calibre2" src="../Images/04fig09.jpg"/></p>

  <p class="noind"><a id="iddle1156"></a><a id="iddle1269"></a><a id="iddle1577"></a><a id="iddle1578"></a><a id="iddle1581"></a><a id="iddle1779"></a><a id="iddle2090"></a><a id="iddle2091"></a><a id="iddle2094"></a>To recap, these are the most common ways to prevent overfitting in neural networks:</p>

  <ul class="calibre16">
    <li class="calibre17">Get more training data.</li>

    <li class="calibre17">Reduce the capacity of the network.</li>

    <li class="calibre17">Add weight regularization.</li>

    <li class="calibre17">Add dropout.</li>
  </ul>

  <h2 class="head" id="ch04lev1sec5"><a class="calibre3" id="ch04lev1sec5__title"></a>4.5. The universal workflow of machine learning</h2>

  <p class="noind"><a id="iddle1495"></a><a id="iddle1531"></a><a id="iddle1580"></a><a id="iddle1625"></a><a id="iddle1747"></a><a id="iddle1921"></a><a id="iddle2093"></a>In this section, we’ll present a universal blueprint that you can use to attack and solve any machine-learning problem. The blueprint ties together the concepts you’ve learned about in this chapter: problem definition, evaluation, feature engineering, and fighting overfitting.</p>

  <h3 class="head1" id="ch04lev2sec12">4.5.1. <a id="ch04lev2sec12__title"></a>Defining the problem and assembling a dataset</h3>

  <p class="noind">First, you must define the problem at hand:</p>

  <ul class="calibre16">
    <li class="calibre17">What will your input data be? What are you trying to predict? You can only learn to predict something if you have available training data: for example, you can only learn to classify the sentiment of movie reviews if you have both movie reviews and sentiment annotations available. As such, data availability is usually the limiting factor at this stage (unless you have the means to pay people to collect data for you).</li>

    <li class="calibre17">What type of problem are you facing? Is it binary classification? Multiclass classification? Scalar regression? Vector regression? Multiclass, multilabel classification? Something else, like clustering, generation, or reinforcement learning? Identifying the problem type will guide your choice of model architecture, loss function, and so on.</li>
  </ul>

  <p class="noind">You can’t move to the next stage until you know what your inputs and outputs are, and what data you’ll use. Be aware of the hypotheses you make at this stage:</p>

  <ul class="calibre16">
    <li class="calibre17">You hypothesize that your outputs can be predicted given your inputs.</li>

    <li class="calibre17">You hypothesize that your available data is sufficiently informative to learn the relationship between inputs and outputs.</li>
  </ul>

  <p class="noind">Until you have a working model, these are merely hypotheses, waiting to be validated or invalidated. Not all problems can be solved; just because you’ve assembled examples of inputs X and targets Y doesn’t mean X contains enough information to predict Y. For instance, if you’re trying to predict the movements of a stock on the stock market given its recent price history, you’re unlikely to succeed, because price history doesn’t contain much predictive information.</p>

  <p class="noind">One class of unsolvable problems you should be aware of is <i class="calibre5">nonstationary problems</i>. Suppose you’re trying to build a recommendation engine for clothing, you’re training it on one month of data (August), and you want to start generating recommendations in the winter. One big issue is that the kinds of clothes people buy change from season to season: clothes buying is a nonstationary phenomenon over the scale of a few months. What you’re trying to model changes over time. In this case, the right move is to constantly retrain your model on data from the recent past, or gather data at a timescale where the problem is stationary. For a cyclical problem like clothes buying, a few years’ worth of data will suffice to capture seasonal variation—but remember to make the time of the year an input of your model!</p>

  <p class="noind"><a id="iddle1392"></a><a id="iddle1582"></a><a id="iddle1583"></a><a id="iddle1626"></a><a id="iddle1636"></a><a id="iddle1838"></a><a id="iddle2095"></a><a id="iddle2096"></a>Keep in mind that machine learning can only be used to memorize patterns that are present in your training data. You can only recognize what you’ve seen before. Using machine learning trained on past data to predict the future is making the assumption that the future will behave like the past. That often isn’t the case.</p>

  <h3 class="head1" id="ch04lev2sec13">4.5.2. <a id="ch04lev2sec13__title"></a>Choosing a measure of success</h3>

  <p class="noind">To control something, you need to be able to observe it. To achieve success, you must define what you mean by success—accuracy? Precision and recall? Customer-retention rate? Your metric for success will guide the choice of a loss function: what your model will optimize. It should directly align with your higher-level goals, such as the success of your business.</p>

  <p class="noind">For balanced-classification problems, where every class is equally likely, accuracy and <i class="calibre5">area under the receiver operating characteristic curve</i> (ROC AUC) are common metrics. For class-imbalanced problems, you can use precision and recall. For ranking problems or multilabel classification, you can use mean average precision. And it isn’t uncommon to have to define your own custom metric by which to measure success. To get a sense of the diversity of machine-learning success metrics and how they relate to different problem domains, it’s helpful to browse the data science competitions on Kaggle (<a href="https://kaggle.com">https://kaggle.com</a>); they showcase a wide range of problems and evaluation metrics.</p>

  <h3 class="head1" id="ch04lev2sec14">4.5.3. <a id="ch04lev2sec14__title"></a>Deciding on an evaluation protocol</h3>

  <p class="noind">Once you know what you’re aiming for, you must establish how you’ll measure your current progress. We’ve previously reviewed three common evaluation protocols:</p>

  <ul class="calibre16">
    <li class="calibre17"><b class="calibre22">Maintaining a hold-out validation set—</b> The way to go when you have plenty of data</li>

    <li class="calibre17"><b class="calibre22">Doing K-fold cross-validation—</b> The right choice when you have too few samples for hold-out validation to be reliable</li>

    <li class="calibre17"><b class="calibre22">Doing iterated K-fold validation—</b> For performing highly accurate model evaluation when little data is available</li>
  </ul>

  <p class="noind">Just pick one of these. In most cases, the first will work well enough.</p>

  <h3 class="head1" id="ch04lev2sec15">4.5.4. <a id="ch04lev2sec15__title"></a>Preparing your data</h3>

  <p class="noind">Once you know what you’re training on, what you’re optimizing for, and how to evaluate your approach, you’re almost ready to begin training models. But first, you should format your data in a way that can be fed into a machine-learning model—here, we’ll assume a deep neural network:</p>

  <ul class="calibre16">
    <li class="calibre17">As you saw previously, your data should be formatted as tensors.</li>

    <li class="calibre17">The values taken by these tensors should usually be scaled to small values: for example, in the [-1, 1] range or [0, 1] range.</li>

    <li class="calibre17">If different features take values in different ranges (heterogeneous data), then the data should be normalized.</li>

    <li class="calibre17">You may want to do some feature engineering, especially for small-data problems.</li>
  </ul>

  <p class="noind">Once your tensors of input data and target data are ready, you can begin to train models.</p>

  <h3 class="head1" id="ch04lev2sec16">4.5.5. <a id="ch04lev2sec16__title"></a>Developing a model that does better than a baseline</h3>

  <p class="noind">Your goal at this stage is to achieve <i class="calibre5">statistical power</i>: that is, to develop a small model that is capable of beating a dumb baseline. In the MNIST digit-classification example, anything that achieves an accuracy greater than 0.1 can be said to have statistical power; in the IMDB example, it’s anything with an accuracy greater than 0.5.</p>

  <p class="noind">Note that it’s not always possible to achieve statistical power. If you can’t beat a random baseline after trying multiple reasonable architectures, it may be that the answer to the question you’re asking isn’t present in the input data. Remember that you make two hypotheses:</p>

  <ul class="calibre16">
    <li class="calibre17">You hypothesize that your outputs can be predicted given your inputs.</li>

    <li class="calibre17">You hypothesize that the available data is sufficiently informative to learn the relationship between inputs and outputs.</li>
  </ul>

  <p class="noind">It may well be that these hypotheses are false, in which case you must go back to the drawing board.</p>

  <p class="noind">Assuming that things go well, you need to make three key choices to build your first working model:</p>

  <ul class="calibre16">
    <li class="calibre17"><b class="calibre22">Last-layer activation—</b> This establishes useful constraints on the network’s output. For instance, the IMDB classification example used <kbd class="calibre24">sigmoid</kbd> in the last layer; the regression example didn’t use any last-layer activation; and so on.</li>

    <li class="calibre17"><b class="calibre22">Loss function—</b> This should match the type of problem you’re trying to solve. For instance, the IMDB example used <kbd class="calibre24">binary_crossentropy</kbd>, the regression example used <kbd class="calibre24">mse</kbd>, and so on.</li>

    <li class="calibre17"><b class="calibre22">Optimization configuration—</b> What optimizer will you use? What will its learning rate be? In most cases, it’s safe to go with <kbd class="calibre24">rmsprop</kbd> and its default learning rate.</li>
  </ul>

  <p class="noind">Regarding the choice of a loss function, note that it isn’t always possible to directly optimize for the metric that measures success on a problem. Sometimes there is no easy way to turn a metric into a loss function; loss functions, after all, need to be computable given only a mini-batch of data (ideally, a loss function should be computable for as little as a single data point) and must be differentiable (otherwise, you can’t use backpropagation to train your network). For instance, the widely used classification metric ROC AUC can’t be directly optimized. Hence, in classification tasks, it’s common to optimize for a proxy metric of ROC AUC, such as crossentropy. In general, you can hope that the lower the crossentropy gets, the higher the ROC AUC will be.</p>

  <p class="noind"><a href="#ch04table01">Table 4.1</a> can help you choose a last-layer activation and a loss function for a few common problem types.</p>

  <p class="noind"></p>

  <p class="notetitle" id="ch04table01">Table 4.1. <a id="ch04table01__title"></a>Choosing the right last-layer activation and loss function for your model</p>

  <table cellpadding="8" cellspacing="5" class="calibre31" frame="hsides" rules="groups" width="100%">
    <colgroup class="calibre8" span="3">
      <col class="calibre9" width="200"/>
      <col class="calibre9" width="200"/>
      <col class="calibre9" width="200"/>
    </colgroup>

    <thead class="calibre32">
      <tr class="calibre11">
        <th class="doctablecell1" scope="col" valign="middle">
          <p class="noind2">Problem type</p>
        </th>

        <th class="doctablecell1" scope="col" valign="middle">
          <p class="noind2">Last-layer activation</p>
        </th>

        <th class="doctablecell1" scope="col" valign="middle">
          <p class="noind2">Loss function</p>
        </th>
      </tr>
    </thead>

    <tbody class="calibre10">
      <tr class="calibre11">
        <td class="doctablecell2">Binary classification</td>

        <td class="doctablecell2">sigmoid</td>

        <td class="doctablecell2">binary_crossentropy</td>
      </tr>

      <tr class="calibre11">
        <td class="doctablecell2">Multiclass, single-label classification</td>

        <td class="doctablecell2">softmax</td>

        <td class="doctablecell2">categorical_crossentropy</td>
      </tr>

      <tr class="calibre11">
        <td class="doctablecell2">Multiclass, multilabel classification</td>

        <td class="doctablecell2">sigmoid</td>

        <td class="doctablecell2">binary_crossentropy</td>
      </tr>

      <tr class="calibre11">
        <td class="doctablecell2">Regression to arbitrary values</td>

        <td class="doctablecell2">None</td>

        <td class="doctablecell2">mse</td>
      </tr>

      <tr class="calibre11">
        <td class="doctablecell2">Regression to values between 0 and 1</td>

        <td class="doctablecell2">sigmoid</td>

        <td class="doctablecell2">mse or binary_crossentropy</td>
      </tr>
    </tbody>
  </table>

  <h3 class="head1" id="ch04lev2sec17">4.5.6. <a id="ch04lev2sec17__title"></a>Scaling up: developing a model that overfits</h3>

  <p class="noind">Once you’ve obtained a model that has statistical power, the question becomes, is your model sufficiently powerful? Does it have enough layers and parameters to properly model the problem at hand? For instance, a network with a single hidden layer with two units would have statistical power on MNIST but wouldn’t be sufficient to solve the problem well. Remember that the universal tension in machine learning is between optimization and generalization; the ideal model is one that stands right at the border between underfitting and overfitting; between undercapacity and overcapacity. To figure out where this border lies, first you must cross it.</p>

  <p class="noind">To figure out how big a model you’ll need, you must develop a model that overfits. This is fairly easy:</p>

  <ol class="calibre23">
    <li class="calibre17">Add layers.</li>

    <li class="calibre17">Make the layers bigger.</li>

    <li class="calibre17">Train for more epochs.</li>
  </ol>

  <p class="noind">Always monitor the training loss and validation loss, as well as the training and validation values for any metrics you care about. When you see that the model’s performance on the validation data begins to degrade, you’ve achieved overfitting.</p>

  <p class="noind">The next stage is to start regularizing and tuning the model, to get as close as possible to the ideal model that neither underfits nor overfits.</p>

  <h3 class="head1" id="ch04lev2sec18">4.5.7. <a id="ch04lev2sec18__title"></a>Regularizing your model and tuning your hyperparameters</h3>

  <p class="noind">This step will take the most time: you’ll repeatedly modify your model, train it, evaluate on your validation data (not the test data, at this point), modify it again, and repeat, until the model is as good as it can get. These are some things you should try:</p>

  <ul class="calibre16">
    <li class="calibre17">Add dropout.</li>

    <li class="calibre17">Try different architectures: add or remove layers.</li>

    <li class="calibre17">Add L1 and/or L2 regularization.</li>

    <li class="calibre17">Try different hyperparameters (such as the number of units per layer or the learning rate of the optimizer) to find the optimal configuration.</li>

    <li class="calibre17">Optionally, iterate on feature engineering: add new features, or remove features that don’t seem to be informative.</li>
  </ul>

  <p class="noind">Be mindful of the following: every time you use feedback from your validation process to tune your model, you leak information about the validation process into the model. Repeated just a few times, this is innocuous; but done systematically over many iterations, it will eventually cause your model to overfit to the validation process (even though no model is directly trained on any of the validation data). This makes the evaluation process less reliable.</p>

  <p class="noind">Once you’ve developed a satisfactory model configuration, you can train your final production model on all the available data (training and validation) and evaluate it one last time on the test set. If it turns out that performance on the test set is significantly worse than the performance measured on the validation data, this may mean either that your validation procedure wasn’t reliable after all, or that you began overfitting to the validation data while tuning the parameters of the model. In this case, you may want to switch to a more reliable evaluation protocol (such as iterated K-fold validation).</p>

  <p class="noind"></p>
  <hr class="calibre25"/>

  <div class="calibre14">
    <b class="calibre26" id="ch04sb02">Chapter summary</b>

    <ul class="calibre16">
      <li class="calibre17">Define the problem at hand and the data on which you’ll train. Collect this data, or annotate it with labels if need be.</li>

      <li class="calibre17">Choose how you’ll measure success on your problem. Which metrics will you monitor on your validation data?</li>

      <li class="calibre17">Determine your evaluation protocol: hold-out validation? K-fold validation? Which portion of the data should you use for validation?</li>

      <li class="calibre17">Develop a first model that does better than a basic baseline: a model with statistical power.</li>

      <li class="calibre17">Develop a model that overfits.</li>

      <li class="calibre17">Regularize your model and tune its hyperparameters, based on performance on the validation data. A lot of machine-learning research tends to focus only on this step—but keep the big picture in mind.</li>
    </ul>
  </div>
  <hr class="calibre25"/>

  <div class="calibre14" id="calibre_pb_20"></div>
</body>
</html>